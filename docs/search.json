[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Hair Color            0             1 FALSE          5\n  top_counts                  \n1 M: 4, F: 3, O: 2            \n2 BLO: 3, BRU: 3, BLA: 1, O: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate    mean     sd  p0 p25 p50 p75 p100\n1 Height                0             1 166.    16.0   133 156 166 178  183\n2 Weight                0             1  70.1   21.2    45  55  70  80  110\n3 Age                   0             1  29.6    9.51   16  24  29  37   43\n4 age_cat               0             1   0.667  0.866   0   0   0   1    2\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▅▂▇▂▅\n4 ▇▁▃▁▃\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + \n  geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + \n  geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + \n  geom_point() + \n  geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + # this is the model, color= stratify the analysis by gender \n  geom_point() + #make point in the plot \n  geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nMuhammad’s Boxplot:\n\nplot_boxplot &lt;- ggplot(data = mydata, aes(x = Gender, y = Height, fill = Gender)) +\n\n  geom_boxplot() +\n\n  scale_fill_manual(values = c(\"blue\", \"pink\", \"green\"))\n\nplot_boxplot\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"boxplot-gender-height.png\")\n\nggsave(filename = figure_file, plot=plot_boxplot)\n\nSaving 7 x 5 in image\n\n\nMuhammad’s Scatterplot:\n\nplot_scatter &lt;- ggplot(data = mydata, aes(x = Weight, y = Height)) +\n\n  geom_point() +\n\n  labs(\n\n    title = \"Scatter Plot of Weight vs. Height\",\n\n    x = \"Weight\",\n\n    y = \"Height\"\n\n  ) +\n\n  theme_minimal() \n\nplot_scatter\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Scatterplot-Height-Weight.png\")\n\nggsave(filename = figure_file, plot=plot_scatter)\n\nSaving 7 x 5 in image\n\n\nMuhammad’s QQ-Plot: I create QQ-Plot for height Note from Natalie: “d4” was not being recognized here, so I noticed you assigned d4 to “processeddata2”. As you can see “processeddata2” was assigned to “mydata” previously in this document. So “mydata” needs to be used here.\n\np5 &lt;- ggplot(mydata, aes(sample = Height)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot for Height\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"QQ-Plot.jpg\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "I chose to recreate a graph that shows the distribution of movie ratings on the star scale from Fandango, IMDb, IMDb users, Metacritic, Metacritic users, and Rotten Tomatoes, and Rotten Tomatoes users.\nThe graph is depicted below.  Fandango Five Thirty Eight Article"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#load-packages",
    "href": "presentation-exercise/presentation-exercise.html#load-packages",
    "title": "Presentation Exercise",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#import-data",
    "href": "presentation-exercise/presentation-exercise.html#import-data",
    "title": "Presentation Exercise",
    "section": "Import Data",
    "text": "Import Data\n\nhere::here()\n\n[1] \"/Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\"\n\n#getwd()\n#setwd(\"~/Desktop/MADA/nataliecann-MADA-portfolio/presentation-exercise\")\ndf &lt;- read_csv(here::here(\"presentation-exercise\", \"fandango\", \"fandango_score_comparison.csv\"))\n\nRows: 146 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): FILM\ndbl (21): RottenTomatoes, RottenTomatoes_User, Metacritic, Metacritic_User, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-one-creating-initial-graph",
    "href": "presentation-exercise/presentation-exercise.html#version-one-creating-initial-graph",
    "title": "Presentation Exercise",
    "section": "Version One: Creating Initial Graph",
    "text": "Version One: Creating Initial Graph\nI used Microsoft Copilot, and occasionally ChatGPT, for this assignment. First, I sent Microsoft Copilot the data nad the graph that I wanted to recreate. I asked it to provide me with code to do so.\nSent Data to Microsoft Copilot Here is the data fandango_score_comparison.csv\nSend Microsoft Copilot a .pdf of the graph that I was aiming to recreate fandangocomparison.pdf how do i recreate this graph (with the same shape, rather than a histogram with bars) using the data provided?\nAnswer: Install and load necessary packages install.packages(“ggplot2”) library(ggplot2)\nRead the CSV file df &lt;- read.csv(“path/to/your/fandango_score_comparison.csv”)\nNormalize the ratings df\\(Fandango_Stars_norm &lt;- df\\)Fandango_Stars / 5 df\\(Fandango_Ratingvalue_norm &lt;- df\\)Fandango_Ratingvalue / 5 df\\(IMDB_norm &lt;- df\\)IMDB / 10 df\\(Metacritic_norm &lt;- df\\)Metacritic / 100 df\\(Metacritic_User_norm &lt;- df\\)Metacritic_User / 10 df\\(RottenTomatoes_norm &lt;- df\\)RottenTomatoes / 100 df\\(RottenTomatoes_User_norm &lt;- df\\)RottenTomatoes_User / 100\nCreate the density plots ggplot() + geom_density(data = df, aes(x = Fandango_Stars_norm, fill = “Fandango”), alpha = 0.5) + geom_density(data = df, aes(x = Fandango_Ratingvalue_norm, fill = “Fandango Rating Value”), alpha = 0.5) + geom_density(data = df, aes(x = IMDB_norm, fill = “IMDb”), alpha = 0.5) + geom_density(data = df, aes(x = Metacritic_norm, fill = “Metacritic”), alpha = 0.5) + geom_density(data = df, aes(x = Metacritic_User_norm, fill = “Metacritic users”), alpha = 0.5) + geom_density(data = df, aes(x = RottenTomatoes_norm, fill = “Rotten Tomatoes”), alpha = 0.5) + geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = “Rotten Tomatoes users”), alpha = 0.5) + labs(title = “Normalized Ratings Distribution of 146 Films in Theaters in 2015”, x = “Normalized Rating”, y = “Density”) + scale_fill_manual(name = “Source”, values = c(“Fandango” = “blue”, “Fandango Rating Value” = “cyan”, “IMDb” = “red”, “Metacritic” = “green”, “Metacritic users” = “darkgreen”, “Rotten Tomatoes” = “purple”, “Rotten Tomatoes users” = “violet”)) + theme_minimal()\nSave the plot ggsave(“fandangocomparison_replicated.pdf”)\nI ran the code in the code chunk below.\n\n# Normalize the ratings\ndf$Fandango_Stars_norm &lt;- df$Fandango_Stars / 5\ndf$IMDB_norm &lt;- df$IMDB / 10\ndf$Metacritic_norm &lt;- df$Metacritic / 100\ndf$Metacritic_User_norm &lt;- df$Metacritic_User / 10\ndf$RottenTomatoes_norm &lt;- df$RottenTomatoes / 100\ndf$RottenTomatoes_User_norm &lt;- df$RottenTomatoes_User / 100\n\n# Create the density plots\nversion_one &lt;- ggplot() +\n  geom_density(data = df, aes(x = Fandango_Stars_norm, fill = \"Fandango\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = IMDB_norm, fill = \"IMDb\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_norm, fill = \"Metacritic\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), alpha = 0.5) +\n  labs(title = \"Normalized Ratings Distribution of 146 Films in Theaters in 2015\",\n       x = \"Normalized Rating\",\n       y = \"Density\") +\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"blue\", \"IMDb\" = \"red\", \"Metacritic\" = \"green\", \"Metacritic users\" = \"darkgreen\", \"Rotten Tomatoes\" = \"purple\", \"Rotten Tomatoes users\" = \"violet\")) +\n  theme_minimal()\n\nversion_one\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_one.png\") \n\nggsave(filename = figure_file, plot = version_one) # saving the plot as \"version_one\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-two-editing-graph-to-better-match-the-original-version",
    "href": "presentation-exercise/presentation-exercise.html#version-two-editing-graph-to-better-match-the-original-version",
    "title": "Presentation Exercise",
    "section": "Version Two: Editing Graph to Better Match the Original Version",
    "text": "Version Two: Editing Graph to Better Match the Original Version\nI manually edited the colors to fit the ones assigned to each source within the original graph. I simply did this by using hex codes online or typing in basic colors (such as “red”). Additionally, I changed the title under the labs() function.\n\nversion_two &lt;- ggplot() +\n  geom_density(data = df, aes(x = Fandango_Stars_norm, fill = \"Fandango\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = IMDB_norm, fill = \"IMDb\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_norm, fill = \"Metacritic\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), alpha = 0.5) +\n  labs(title = \"Fandango LOVES Movies\",\n       x = \"Normalized Rating\",\n       y = \"Density\") +\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"red\", \"IMDb\" = \"gold\", \"Metacritic\" = \"#8d31c1\", \"Metacritic users\" = \"#94e13d\", \"Rotten Tomatoes\" = \"#7b7b7c\", \"Rotten Tomatoes users\" = \"#309dd8\")) + # update colors to match those of the original graph \n  theme_minimal() \n\nversion_two\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_two.png\")\n\nggsave(filename = figure_file, plot = version_two) # saving the plot as \"version_two\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-three-editing-graph-more-to-better-match-the-original-version",
    "href": "presentation-exercise/presentation-exercise.html#version-three-editing-graph-more-to-better-match-the-original-version",
    "title": "Presentation Exercise",
    "section": "Version Three: Editing Graph MORE to Better Match the Original Version",
    "text": "Version Three: Editing Graph MORE to Better Match the Original Version\nI asked ChatGPT: how do I get rid of the legend and add the labels within the legend next to their corresponding areas within the graph?\nAnswer: theme(legend.position = “none”) + # Removes the legend geom_text(aes(x = 1, y = 0.15, label = “Fandango”, color = “red”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Fandango geom_text(aes(x = 1, y = 0.13, label = “IMDb”, color = “gold”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for IMDb geom_text(aes(x = 1, y = 0.11, label = “Metacritic”, color = “#8d31c1”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Metacritic geom_text(aes(x = 1, y = 0.09, label = “Metacritic users”, color = “#94e13d”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Metacritic users geom_text(aes(x = 1, y = 0.07, label = “Rotten Tomatoes”, color = “#7b7b7c”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Rotten Tomatoes geom_text(aes(x = 1, y = 0.05, label = “Rotten Tomatoes users”, color = “#309dd8”), size = 5, data = data.frame(), inherit.aes = FALSE) # Add label for Rotten Tomatoes users\nI manually changed the color of the IMDb users density/text to be a darker gold for better visibility. Additionally, I changed the name that ChatGPT gave me for the label from “IBDb” to “IMDb users” to match the original graph.\nAfter a few more intermediate steps: - I changed the theme of the graph to include a gray background and gray gridlines - I changed the positions of the labels to match the original graph - I fixed the x and y axes - I added a subtitle - I changed geom_density to stat_density and added their respective colors - I made the area under the densities gray with an outline of their respective color\n\nversion_three &lt;- ggplot(df) +\n\n   # Adding densities/areas\n  stat_density(aes(x = Fandango_Stars_norm, fill = \"Fandango\"), geom = \"area\", alpha = 0.5, color = \"red\") +\n  stat_density(aes(x = IMDB_norm, fill = \"IMDb\"), geom = \"area\", alpha = 0.5, color = \"#e6c531\") +\n  stat_density(aes(x = Metacritic_norm, fill = \"Metacritic\"), geom = \"area\", alpha = 0.5, color = \"#8d31c1\") +\n  stat_density(aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), geom = \"area\", alpha = 0.5, color = \"#94e13d\") +\n  stat_density(aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), geom = \"area\", alpha = 0.5, color = \"#7b7b7c\") +\n  stat_density(aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), geom = \"area\", alpha = 0.5, color = \"#309dd8\") +\n  \n  # Adding title and subtitle\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of 146 films in theaters in 2015 that \\nhad 30+ reviews on Fandango.com\", \n       x = NULL,  \n       y = NULL) +\n \n   # Adding fill colors for the densities/areas\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"#f95858\", \"IMDb\" = \"grey\", \"Metacritic\" = \"grey\", \"Metacritic users\" = \"grey\", \"Rotten Tomatoes\" = \"grey\", \"Rotten Tomatoes users\" = \"grey\")) +\n  \n  # Changing background to grey with grey and thin gridlines\n  theme_minimal() +\n  theme(legend.position = \"none\", \n        plot.background = element_rect(fill = \"#ececec\"),   \n        panel.grid.major = element_line(color = \"#a7a7a5\", size = 0.1), \n        panel.grid.minor = element_line(color = \"#a7a7a5\", size = 0.1), \n        panel.grid = element_line(color = \"transparent\"), \n        plot.title = element_text(face = \"bold\", size = 14), \n        plot.subtitle = element_text(size = 10)) + \n  \n  # Adding labels and placing them in the appropriate spot on the graph \n  geom_text(aes(x = 0.97, y = 3.75, label = \"Fandango\"), color = \"red\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = .65, y = 4.2, label = \"IMDb users\"), color = \"#e6c531\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.3, y = 1.5, label = \"Metacritic\"), color = \"#8d31c1\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.57, y = 2.5, label = \"Metacritic \\n users\"), color = \"#94e13d\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.1, y = 1.2, label = \"Rotten \\n Tomatoes\"), color = \"#7b7b7c\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.45, y = 1.9 , label = \"Rotten \\n Tomatoes \\n users\"), color = \"#309dd8\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  \n  # Changing the x and y axis labels to match the original graph\n  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), \n                     labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +  # the stars appear in the png version\n  scale_y_continuous(breaks = c(0, 1, 2, 3, 4), \n                     labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\"))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nversion_three\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_three.png\")\n\nggsave(filename = figure_file, plot = version_three) # saving the plot as \"version_three\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-four-creating-final-version",
    "href": "presentation-exercise/presentation-exercise.html#version-four-creating-final-version",
    "title": "Presentation Exercise",
    "section": "Version Four: Creating Final Version",
    "text": "Version Four: Creating Final Version\nFor the final version, I made the following changes: - I made the “fandango” label bold (just like it is in the original graph) - I made the font size of the title and subtitle larger - I updated the color of the green in “Metacritic users” to be a bit darker - I removed the “minor” gridlines\nAdditionally, I asked Microsoft Copilot to change the geom from stat_density to stat_bin. Stat_bin will better help me recreate the original graph than geom_density or stat_density.\nAnswer: stat_bin(aes(x = Fandango_Stars_norm, fill = “Fandango”), geom = “area”, alpha = 0.5, color = “red”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = IMDB_norm, fill = “IMDb”), geom = “area”, alpha = 0.5, color = “#f7ca62”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = Metacritic_norm, fill = “Metacritic”), geom = “area”, alpha = 0.5, color = “#8d31c1”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = Metacritic_User_norm, fill = “Metacritic users”), geom = “area”, alpha = 0.5, color = “#94e13d”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = RottenTomatoes_norm, fill = “Rotten Tomatoes”), geom = “area”, alpha = 0.5, color = “#7b7b7c”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = RottenTomatoes_User_norm, fill = “Rotten Tomatoes users”), geom = “area”, alpha = 0.5, color = “#309dd8”, binwidth = 0.1, position = “identity”) +\nAfter fixing the geom/stat, I realized the y-axis had been messed up. I used code to fix the y-axis and fix the source labels within the graph.\n\nversion_four &lt;- ggplot(df) +\n \n   # Adding densities/areas\n  stat_bin(aes(x = IMDB_norm, fill = \"IMDb\"), geom = \"area\", alpha = 0.5, color = \"#e6c531\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = Metacritic_norm, fill = \"Metacritic\"), geom = \"area\", alpha = 0.5, color = \"#8d31c1\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), geom = \"area\", alpha = 0.5, color = \"#87c866\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), geom = \"area\", alpha = 0.5, color = \"#7b7b7c\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), geom = \"area\", alpha = 0.5, color = \"#309dd8\", binwidth = 0.1, position = \"identity\") + stat_bin(aes(x = Fandango_Stars_norm, fill = \"Fandango\"), geom = \"area\", alpha = 0.5, color = \"red\", binwidth = 0.1, position = \"identity\") +\n  \n  # Adding title and subtitle\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of 146 films in theaters in 2015 that \\nhad 30+ reviews on Fandango.com\", \n       x = NULL,  \n       y = NULL) +\n \n   # Adding fill colors for the densities/areas\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"#f95858\", \"IMDb\" = \"grey\", \"Metacritic\" = \"grey\", \"Metacritic users\" = \"grey\", \"Rotten Tomatoes\" = \"grey\", \"Rotten Tomatoes users\" = \"grey\")) +\n  \n  # Changing background to grey with grey and thin gridlines\ntheme_minimal() +\n  theme(legend.position = \"none\", \n        plot.background = element_rect(fill = \"#ececec\"),   \n        panel.grid.major = element_line(color = \"#a7a7a5\", size = 0.1), \n        panel.grid = element_line(color = \"transparent\"), \n        plot.title = element_text(face = \"bold\", size = 18),  # Bigger title\n        plot.subtitle = element_text(size = 14)) +  # Bigger subtitle\n  \n  # Adding labels and placing them in the appropriate spot on the graph \n  geom_text(aes(x = 0.97, y = 55, label = \"Fandango\"), color = \"red\", size = 3.2, fontface = \"bold\", data = data.frame(), inherit.aes = FALSE) + # only bold \"Fandango\"\n  geom_text(aes(x = .6, y = 60, label = \"IMDb users\"), color = \"#e6c531\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.3, y = 23, label = \"Metacritic\"), color = \"#8d31c1\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.55, y = 37, label = \"Metacritic \\n users\"), color = \"#87c866\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.1, y = 19, label = \"Rotten \\n Tomatoes\"), color = \"#7b7b7c\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.45, y = 30 , label = \"Rotten \\n Tomatoes \\n users\"), color = \"#309dd8\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  \n  # Changing the x and y axis labels to match the original graph\n  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), \n                     labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +  # the stars appear in the png version\n  scale_y_continuous(breaks = c(0, 15, 30, 45, 60), \n                     labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\"))\n\nversion_four\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_four.png\")\n\nggsave(filename = figure_file, plot = version_four) # saving the plot as \"version_four\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#final-version-of-graph-vs-original-graph",
    "href": "presentation-exercise/presentation-exercise.html#final-version-of-graph-vs-original-graph",
    "title": "Presentation Exercise",
    "section": "Final Version of Graph vs Original Graph",
    "text": "Final Version of Graph vs Original Graph\nThe stars used on the x-axis to represent the ratings did not appear in the output on my computer in R Studio. However, the stars appear in the .png file attached below.\n \nFor the most part, I believe that I was able to successfully recreate the original graph. It appears that the distriution of the ratings among the different sources is pretty similar to that of the original graph. There are miniscule differences. Another element that I had difficulty in recreating from the original graph was making the areas overlap in the same way that they do in the original graph."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#load-packages-1",
    "href": "presentation-exercise/presentation-exercise.html#load-packages-1",
    "title": "Presentation Exercise",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(gt)\nlibrary(stringr)\nlibrary(webshot2)\nlibrary(htmltools)\nlibrary(purrr)"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#create-individual-graphs",
    "href": "presentation-exercise/presentation-exercise.html#create-individual-graphs",
    "title": "Presentation Exercise",
    "section": "Create Individual Graphs",
    "text": "Create Individual Graphs"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#create-table",
    "href": "presentation-exercise/presentation-exercise.html#create-table",
    "title": "Presentation Exercise",
    "section": "Create Table",
    "text": "Create Table\nI used Microsoft Copilot and ChatGPT. I asked it to create a graph in which the columns were: mean, median, minimum, maximum rating values, and distribution plot. I asked for the rows to be: Fandango, IMDb, IMDb users, Metacritic, Metacritic users, Rotten Tomatoes, and Rotten Tomatoes users. There were MANY small things that I had to fix.\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gt)\nlibrary(purrr)\n\n# Step 1: Summarize the data\ntable &lt;- df %&gt;%\n  select(Fandango_Stars_norm, IMDB_norm, Metacritic_norm, Metacritic_User_norm, RottenTomatoes_norm, RottenTomatoes_User_norm) %&gt;%\n  summarise(\n    Fandango_mean = mean(Fandango_Stars_norm, na.rm = TRUE),\n    Fandango_median = median(Fandango_Stars_norm, na.rm = TRUE),\n    Fandango_min = min(Fandango_Stars_norm, na.rm = TRUE),\n    Fandango_max = max(Fandango_Stars_norm, na.rm = TRUE),\n    \n    IMDB_mean = mean(IMDB_norm, na.rm = TRUE),\n    IMDB_median = median(IMDB_norm, na.rm = TRUE),\n    IMDB_min = min(IMDB_norm, na.rm = TRUE),\n    IMDB_max = max(IMDB_norm, na.rm = TRUE),\n    \n    Metacritic_mean = mean(Metacritic_norm, na.rm = TRUE),\n    Metacritic_median = median(Metacritic_norm, na.rm = TRUE),\n    Metacritic_min = min(Metacritic_norm, na.rm = TRUE),\n    Metacritic_max = max(Metacritic_norm, na.rm = TRUE),\n    \n    Metacritic_User_mean = mean(Metacritic_User_norm, na.rm = TRUE),\n    Metacritic_User_median = median(Metacritic_User_norm, na.rm = TRUE),\n    Metacritic_User_min = min(Metacritic_User_norm, na.rm = TRUE),\n    Metacritic_User_max = max(Metacritic_User_norm, na.rm = TRUE),\n    \n    RottenTomatoes_mean = mean(RottenTomatoes_norm, na.rm = TRUE),\n    RottenTomatoes_median = median(RottenTomatoes_norm, na.rm = TRUE),\n    RottenTomatoes_min = min(RottenTomatoes_norm, na.rm = TRUE),\n    RottenTomatoes_max = max(RottenTomatoes_norm, na.rm = TRUE),\n    \n    RottenTomatoes_User_mean = mean(RottenTomatoes_User_norm, na.rm = TRUE),\n    RottenTomatoes_User_median = median(RottenTomatoes_User_norm, na.rm = TRUE),\n    RottenTomatoes_User_min = min(RottenTomatoes_User_norm, na.rm = TRUE),\n    RottenTomatoes_User_max = max(RottenTomatoes_User_norm, na.rm = TRUE)\n  )\n\n# Step 2: Reshape the data\ntable_long &lt;- table %&gt;%\n  pivot_longer(cols = everything(), names_to = c(\"Rating\", \"Statistic\"), names_pattern = \"(.*)_(.*)\") %&gt;%\n  pivot_wider(names_from = \"Statistic\", values_from = \"value\") %&gt;%\n  select(Rating, mean, median, min, max)\n\n# Step 3: Add the distribution plot path and other steps\nget_image_path &lt;- function(rating) {\n  # Modify the function to return the correct image path or URL\n  paste0(\"path_to_images/\", rating, \"_plot.png\")  # Example path, adjust to your data\n}\n\ntable_with_plots &lt;- table_long %&gt;%\n  mutate(\n    distribution = map(Rating, get_image_path)\n  )\n\n# Compute the maximum values for highlighting\nmax_mean &lt;- max(table_long$mean, na.rm = TRUE)\nmax_median &lt;- max(table_long$median, na.rm = TRUE)\nmax_min &lt;- max(table_long$min, na.rm = TRUE)\nmax_max &lt;- max(table_long$max, na.rm = TRUE)\n\n\n# Step 4: Create and format the GT table\n# Create the presentation table with images using text_transform\npresentation_table &lt;- table_with_plots %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = md(\"**Summary of Ratings from Movie Review Sources**\")\n  ) %&gt;%\n  cols_label(\n    Rating = \"Rating Source\",\n    mean = \"Mean\",\n    median = \"Median\",\n    min = \"Minimum\",\n    max = \"Maximum\",\n    distribution = \"Density Plot\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Rating Statistics\",\n    columns = c(\"mean\", \"median\", \"min\", \"max\")\n  ) %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_column_labels(columns = c(\"mean\", \"median\", \"min\", \"max\"))\n  ) %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_column_labels(columns = everything())\n  ) %&gt;%\n  # Use text_transform to render HTML &lt;img&gt; tags for images\n  text_transform(\n    locations = cells_body(columns = \"distribution\"),\n    fn = function(x) {\n      map(x, ~ ifelse(!is.na(.x),\n                      paste0('&lt;img src=\"file://', file.path(\"~/Desktop/MADA/nataliecann-MADA-portfolio/presentation-exercise/graphs\", .x), '\" height=\"100\"&gt;'),\n                      NA_character_))\n    }\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n    locations = cells_body(rows = seq(1, nrow(table_with_plots), by = 2))\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#ececec\"),\n    locations = cells_body(rows = seq(2, nrow(table_with_plots), by = 2))\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#dbf2cf\"),\n    locations = cells_body(columns = \"mean\", rows = mean == max_mean)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#cfedf8\"),\n    locations = cells_body(columns = \"median\", rows = median == max_median)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#f5e4f3\"),\n    locations = cells_body(columns = \"min\", rows = min == max_min)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#fee6cc\"),\n    locations = cells_body(columns = \"max\", rows = max == max_max)\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"*Cells highlighted in different colors indicate the highest value in that column.*\")\n  )\n\n# Display the table\npresentation_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Ratings from Movie Review Sources\n\n\nRating Source\n\nRating Statistics\n\nDensity Plot\n\n\nMean\nMedian\nMinimum\nMaximum\n\n\n\n\nFandango\n0.8178082\n0.800\n0.60\n1.00\n\n\n\nIMDB\n0.6736986\n0.690\n0.40\n0.86\n\n\n\nMetacritic\n0.5880822\n0.590\n0.13\n0.94\n\n\n\nMetacritic_User\n0.6519178\n0.685\n0.24\n0.96\n\n\n\nRottenTomatoes\n0.6084932\n0.635\n0.05\n1.00\n\n\n\nRottenTomatoes_User\n0.6387671\n0.665\n0.20\n0.94\n\n\n\n\nCells highlighted in different colors indicate the highest value in that column.\n\n\n\n\n\n\n\n# Save!\ngtsave(presentation_table, \"presentation_table.png\")\n\n\n# Define common scales\ncommon_x_scale &lt;- scale_x_continuous(\n  breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), \n  labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")\n)\n\ncommon_y_scale &lt;- scale_y_continuous(\n  breaks = c(0, 15, 30, 45, 60), \n  labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")\n)\n\ncommon_theme &lt;- theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"#ececec\"),\n    panel.grid.major = element_line(color = \"#a7a7a5\", size = 0.1),\n    panel.grid = element_line(color = \"transparent\"),\n    plot.title = element_text(face = \"bold\", size = 18),\n    plot.subtitle = element_text(size = 14)\n  )\n\n# IMDb Plot\nIMDb &lt;- ggplot(df) + \n  stat_bin(aes(x = IMDB_norm, fill = \"IMDb\"), geom = \"area\", alpha = 0.5, color = \"#e6c531\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"IMDb\" = \"#e6c531\")) +  \n  common_x_scale + common_y_scale + common_theme\nIMDb\n\n\n\n\n\n\n\n# Metacritic Plot\nMetacritic &lt;- ggplot(df) + \n  stat_bin(aes(x = Metacritic_norm, fill = \"Metacritic\"), geom = \"area\", alpha = 0.5, color = \"#8d31c1\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Metacritic\" = \"#8d31c1\")) +  \n  common_x_scale + common_y_scale + common_theme\nMetacritic\n\n\n\n\n\n\n\n# Metacritic User Plot\nMetacritic_User &lt;- ggplot(df) + \n  stat_bin(aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), geom = \"area\", alpha = 0.5, color = \"#87c866\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Metacritic users\" = \"#87c866\")) +  \n  common_x_scale + common_y_scale + common_theme\nMetacritic_User\n\n\n\n\n\n\n\n# Rotten Tomatoes Plot\nRottenTomatoes &lt;- ggplot(df) + \n  stat_bin(aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), geom = \"area\", alpha = 0.5, color = \"#7b7b7c\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Rotten Tomatoes\" = \"#7b7b7c\")) +  \n  common_x_scale + common_y_scale + common_theme\nRottenTomatoes\n\n\n\n\n\n\n\n# Rotten Tomatoes User Plot\nRottenTomatoes_User &lt;- ggplot(df) + \n  stat_bin(aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), geom = \"area\", alpha = 0.5, color = \"#309dd8\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Rotten Tomatoes users\" = \"#309dd8\")) +  \n  common_x_scale + common_y_scale + common_theme\nRottenTomatoes_User\n\n\n\n\n\n\n\n# Fandango Plot\nFandango &lt;- ggplot(df) + \n  stat_bin(aes(x = Fandango_Stars_norm, fill = \"Fandango\"), geom = \"area\", alpha = 0.5, color = \"red\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Fandango\" = \"red\")) +  \n  common_x_scale + common_y_scale + common_theme\nFandango\n\n\n\n\n\n\n\n\nI tried many different times to get the distributions to appear in the “density plot” column. I couldn’t get this to work. So, I took screenshots of the individual graphs created above and edited them into a screenshot of the table. The screenshot can be seen below:\n\n\n\nEdited Version of Table"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data used in this exercise will be using in this exercise is called “NCHS - Drug Poisoning Mortality by State: United States.” I downloaded this dataset from the CDC website linked here: https://data.cdc.gov/NCHS/NCHS-Drug-Poisoning-Mortality-by-State-United-Stat/jx6g-fdh6/about_data. This dataset contains entries on drug poisoning deaths in the U.S. and at the state-level. The data set’s time range is from 1999 to 2015.\n\n\n\nFirst, I will load any packages that may be of help in this exercise.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\nNow I will load in the CDC data for this exercise that was described in the section above. I will call it “data”.\n\ndata &lt;- read.csv(\"drug-data.csv\")\n\nI will use the summary(), dim(), and head() functions to get a better look at the data.\n\nsummary(data)\n\n      Year          Sex                Age            Race.and.Hispanic.Origin\n Min.   :1999   Length:2703        Length:2703        Length:2703             \n 1st Qu.:2003   Class :character   Class :character   Class :character        \n Median :2007   Mode  :character   Mode  :character   Mode  :character        \n Mean   :2007                                                                 \n 3rd Qu.:2011                                                                 \n Max.   :2015                                                                 \n                                                                              \n    State               Deaths        Population        Crude.Death.Rate\n Length:2703        Min.   :    1   Min.   :   238163   Min.   : 0.00   \n Class :character   1st Qu.:  124   1st Qu.:  2723372   1st Qu.: 3.90   \n Mode  :character   Median :  478   Median :  6403290   Median : 8.80   \n                    Mean   : 1873   Mean   : 16586603   Mean   :10.19   \n                    3rd Qu.: 1406   3rd Qu.: 18116192   3rd Qu.:14.30   \n                    Max.   :52404   Max.   :321418820   Max.   :53.50   \n                                                                        \n Standard.Error.for.Crude.Rate Low.Confidence.Limit.for.Crude.Rate\n Min.   :0.0000                Min.   : 0.000                     \n 1st Qu.:0.2000                1st Qu.: 3.300                     \n Median :0.4000                Median : 8.000                     \n Mean   :0.4271                Mean   : 9.364                     \n 3rd Qu.:0.6000                3rd Qu.:13.200                     \n Max.   :1.8000                Max.   :52.200                     \n                                                                  \n Upper.Confidence.Limit.for.Crude.Rate Age.adjusted.Rate\n Min.   : 0.10                         Min.   : 1.80    \n 1st Qu.: 4.60                         1st Qu.: 7.40    \n Median : 9.60                         Median :10.80    \n Mean   :11.07                         Mean   :11.24    \n 3rd Qu.:15.40                         3rd Qu.:14.00    \n Max.   :54.80                         Max.   :41.50    \n                                       NA's   :1632     \n Standard.Error.Age.adjusted.Rate Lower.Confidence.Limit.for.Age.adjusted.rate\n Min.   :0.000                    Min.   : 0.9                                \n 1st Qu.:0.300                    1st Qu.: 6.5                                \n Median :0.500                    Median : 9.8                                \n Mean   :0.543                    Mean   :10.2                                \n 3rd Qu.:0.700                    3rd Qu.:12.8                                \n Max.   :1.900                    Max.   :38.4                                \n NA's   :1632                     NA's   :1632                                \n Upper.Confidence.Limit.for.Age.adjusted.Rate State.Crude.Rate.in.Range\n Min.   : 2.20                                Length:2703              \n 1st Qu.: 8.20                                Class :character         \n Median :11.80                                Mode  :character         \n Mean   :12.34                                                         \n 3rd Qu.:15.30                                                         \n Max.   :44.60                                                         \n NA's   :1632                                                          \n US.Crude.Rate   US.Age.adjusted.Rate\n Min.   : 6.00   Min.   : 6.10       \n 1st Qu.: 8.90   1st Qu.: 8.90       \n Median :12.00   Median :11.90       \n Mean   :11.01   Mean   :10.96       \n 3rd Qu.:13.20   3rd Qu.:13.10       \n Max.   :16.30   Max.   :16.30       \n                                     \n\ndim(data)\n\n[1] 2703   18\n\nhead(data)\n\n  Year        Sex      Age Race.and.Hispanic.Origin         State Deaths\n1 2004 Both Sexes All Ages    All Races-All Origins         Texas   1813\n2 2012 Both Sexes All Ages    All Races-All Origins       Arizona   1134\n3 2008       Male All Ages                 Hispanic United States   1875\n4 2009 Both Sexes All Ages    All Races-All Origins    Washington    987\n5 2015     Female All Ages    All Races-All Origins United States  19447\n6 2008 Both Sexes All Ages    All Races-All Origins          Utah    468\n  Population Crude.Death.Rate Standard.Error.for.Crude.Rate\n1   22394023              8.1                           0.2\n2    6553255             17.3                           0.5\n3   24322906              7.7                           0.2\n4    6667426             14.8                           0.5\n5  163189523             11.9                           0.1\n6    2663029             17.6                           0.8\n  Low.Confidence.Limit.for.Crude.Rate Upper.Confidence.Limit.for.Crude.Rate\n1                                 7.7                                   8.5\n2                                16.3                                  18.3\n3                                 7.4                                   8.1\n4                                13.9                                  15.7\n5                                11.7                                  12.1\n6                                16.0                                  19.2\n  Age.adjusted.Rate Standard.Error.Age.adjusted.Rate\n1               8.2                              0.2\n2              17.7                              0.5\n3               8.4                              0.2\n4              14.3                              0.5\n5              11.8                              0.1\n6              19.0                              0.9\n  Lower.Confidence.Limit.for.Age.adjusted.rate\n1                                          7.8\n2                                         16.6\n3                                          8.0\n4                                         13.4\n5                                         11.7\n6                                         17.2\n  Upper.Confidence.Limit.for.Age.adjusted.Rate State.Crude.Rate.in.Range\n1                                          8.5                     7–9.8\n2                                         18.7                 15.2–41.5\n3                                          8.8                   8.3–8.4\n4                                         15.2                 12.3–15.2\n5                                         12.0                 10.6–11.8\n6                                         20.7                 15.2–41.5\n  US.Crude.Rate US.Age.adjusted.Rate\n1           9.4                  9.4\n2          13.2                 13.1\n3          12.0                 11.9\n4          12.1                 11.9\n5          16.3                 16.3\n6          12.0                 11.9\n\n\nAs seen by the output, this dataset contains 18 variables with 2,703 observations.\n\n\n\nBelow, I will select the variables that I will work with in this exercise. I will reassign this to a new dataset called “processed_data”.\n\nprocessed_data &lt;- data %&gt;% select(Year, Sex, Age, Race.and.Hispanic.Origin, State, Deaths, Population, Crude.Death.Rate)\n\nI will rename the variables to be easier to understand. I will use head() to ensure this worked.\n\nprocessed_data &lt;- processed_data %&gt;%\n  rename(Race_and_Hispanic_Origin = Race.and.Hispanic.Origin,\n         Crude_Death_Rate = Crude.Death.Rate)\nhead(processed_data)\n\n  Year        Sex      Age Race_and_Hispanic_Origin         State Deaths\n1 2004 Both Sexes All Ages    All Races-All Origins         Texas   1813\n2 2012 Both Sexes All Ages    All Races-All Origins       Arizona   1134\n3 2008       Male All Ages                 Hispanic United States   1875\n4 2009 Both Sexes All Ages    All Races-All Origins    Washington    987\n5 2015     Female All Ages    All Races-All Origins United States  19447\n6 2008 Both Sexes All Ages    All Races-All Origins          Utah    468\n  Population Crude_Death_Rate\n1   22394023              8.1\n2    6553255             17.3\n3   24322906              7.7\n4    6667426             14.8\n5  163189523             11.9\n6    2663029             17.6\n\n\n\n\n\nI will create a scatterplot to assess the relationship between deaths and population. Then, I will use the cor() function to obtain the correlation coefficient to determine the strength of this relationship.\n\nggplot(processed_data, aes(x = Deaths, y = Population)) +\n  geom_point(color = \"#429bf9\") + # giving the points of the scatterplot a blue color\n  labs(title = \"Scatterplot of Relationship between Number of Deaths \\n From Drug Poisoning and Population Size\",\n       x = \"Number of Deaths\",\n       y = \"Size of Population\") + # labeling axes and titles!\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # removing legend; making title and axes bold and giving them a larger font size\n\n\n\n\n\n\n\ncor(processed_data$Deaths, processed_data$Population) # correlation coefficient\n\n[1] 0.8997261\n\n\nThe scatterplot above, shows a positive linear relationship between the number of deaths from drug poisoning and the size of the population. The correlation coefficient is 0.8997261, which indicates a strong positive linear relationship.\nNow, I will assess the relationship between year and death. Then, I will use the cor() function to obtain the correlation coefficient to determine the strength of this relationship.\n\nggplot(processed_data, aes(x = Year, y = Deaths)) +\n  geom_point(color = \"#33d11a\") + # giving the points of the scatterplot a green color\n  labs(title = \"Scatterplot of Relationship between Number of Deaths \\n From Drug Poisoning and Year\",\n       x = \"Year\",\n       y = \"Number of Deaths\") + # labeling axes and titles!\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # removing legend; making title and axes bold and giving them a larger font size\n\n\n\n\n\n\n\ncor(processed_data$Deaths, processed_data$Year) # correlation coefficient\n\n[1] 0.1246065\n\n\nThe scatterplot above shows that there may be a slight relationship between year and number deaths due to drug poisoning. The correlation coefficient is 0.1246065; this indicates a VERY weak relationship (possibly no realtionship at all).\nI will create a correlation matrix. I did this in the “data-exercise” and found it to be helpful.\n\n# use only numberic variables!\ncor_matrix &lt;- cor(processed_data %&gt;% select(where(is.numeric)))\n\n# print out!\nprint(cor_matrix)\n\n                       Year    Deaths Population Crude_Death_Rate\nYear             1.00000000 0.1246065 0.01947317       0.32354160\nDeaths           0.12460651 1.0000000 0.89972612       0.31923101\nPopulation       0.01947317 0.8997261 1.00000000       0.06736837\nCrude_Death_Rate 0.32354160 0.3192310 0.06736837       1.00000000\n\n\nI will create a frequency table to display the total number of deaths from drug poisoning by year.\n\n# Create frequency table for the total number of deaths by year\ndeath_by_year &lt;- processed_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Deaths = sum(Deaths, na.rm = TRUE)) %&gt;%\n  arrange(Year)\n\n# View the frequency table\ndeath_by_year\n\n# A tibble: 17 × 2\n    Year Total_Deaths\n   &lt;int&gt;        &lt;int&gt;\n 1  1999       149899\n 2  2000       155057\n 3  2001       172732\n 4  2002       209642\n 5  2003       229715\n 6  2004       244414\n 7  2005       265629\n 8  2006       306901\n 9  2007       321148\n10  2008       324838\n11  2009       329252\n12  2010       341295\n13  2011       367830\n14  2012       368968\n15  2013       391308\n16  2014       418345\n17  2015       465632\n\n\n\n\n\nI will assess the distribution of both the number of deaths due to drug poisoning and the population size.\n\n# Deaths\nggplot(processed_data, aes(x = Deaths)) +\n  geom_histogram(bins = 30, fill = \"#ee5050\", color = \"#c41616\", alpha = 0.7) +\n  labs(title = \"Histogram of Deaths from Drug Poisoning\",\n       x = \"Number of Deaths\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n# Population Size\nggplot(processed_data, aes(x = Population)) +\n  geom_histogram(bins = 30, fill = \"#f9c363\", color = \"#ffa303\", alpha = 0.7) +\n  labs(title = \"Histogram of Population Size\",\n       x = \"Population Size\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nBoth histograms for number of deaths due to drug poisoning and population size appeared to be skewed to the right.\nI will create a table to display the mean and standard deviations of the number of deaths due to drug poisoning from 1999 to 2015 as well as the population size from 1999 to 2015.\n\ndistribution_table &lt;- processed_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Mean_Deaths = mean(Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Deaths, na.rm = TRUE),\n    Mean_Population = mean(Population, na.rm = TRUE),\n    SD_Population = sd(Population, na.rm = TRUE)\n  )\n\nprint(distribution_table) # print table to see it!\n\n# A tibble: 17 × 5\n    Year Mean_Deaths SD_Deaths Mean_Population SD_Population\n   &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  1999        943.     2155.       15462000.     32759001.\n 2  2000        975.     2247.       15584898.     32954121.\n 3  2001       1086.     2507.       15769468.     33258376.\n 4  2002       1319.     3045.       15906943.     33474245.\n 5  2003       1445.     3339.       16035120.     33675188.\n 6  2004       1537.     3559.       16175519.     33902213.\n 7  2005       1671.     3839.       16316254.     34126543.\n 8  2006       1930.     4434.       16465436.     34368590.\n 9  2007       2020.     4659.       16614163.     34609414.\n10  2008       2043.     4734.       16763441.     34851622.\n11  2009       2071.     4789.       16902888.     35076777.\n12  2010       2147.     4963.       17005636.     35242661.\n13  2011       2313.     5337.       17154800.     35484630.\n14  2012       2321.     5335.       17270704.     35678128.\n15  2013       2461.     5628.       17382466.     35862756.\n16  2014       2631.     6014.       17517589.     36089522.\n17  2015       2929.     6683.       17644933.     36306209."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#description-of-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#description-of-data",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data used in this exercise will be using in this exercise is called “NCHS - Drug Poisoning Mortality by State: United States.” I downloaded this dataset from the CDC website linked here: https://data.cdc.gov/NCHS/NCHS-Drug-Poisoning-Mortality-by-State-United-Stat/jx6g-fdh6/about_data. This dataset contains entries on drug poisoning deaths in the U.S. and at the state-level. The data set’s time range is from 1999 to 2015."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#loading-in-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#loading-in-the-data",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "First, I will load any packages that may be of help in this exercise.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\nNow I will load in the CDC data for this exercise that was described in the section above. I will call it “data”.\n\ndata &lt;- read.csv(\"drug-data.csv\")\n\nI will use the summary(), dim(), and head() functions to get a better look at the data.\n\nsummary(data)\n\n      Year          Sex                Age            Race.and.Hispanic.Origin\n Min.   :1999   Length:2703        Length:2703        Length:2703             \n 1st Qu.:2003   Class :character   Class :character   Class :character        \n Median :2007   Mode  :character   Mode  :character   Mode  :character        \n Mean   :2007                                                                 \n 3rd Qu.:2011                                                                 \n Max.   :2015                                                                 \n                                                                              \n    State               Deaths        Population        Crude.Death.Rate\n Length:2703        Min.   :    1   Min.   :   238163   Min.   : 0.00   \n Class :character   1st Qu.:  124   1st Qu.:  2723372   1st Qu.: 3.90   \n Mode  :character   Median :  478   Median :  6403290   Median : 8.80   \n                    Mean   : 1873   Mean   : 16586603   Mean   :10.19   \n                    3rd Qu.: 1406   3rd Qu.: 18116192   3rd Qu.:14.30   \n                    Max.   :52404   Max.   :321418820   Max.   :53.50   \n                                                                        \n Standard.Error.for.Crude.Rate Low.Confidence.Limit.for.Crude.Rate\n Min.   :0.0000                Min.   : 0.000                     \n 1st Qu.:0.2000                1st Qu.: 3.300                     \n Median :0.4000                Median : 8.000                     \n Mean   :0.4271                Mean   : 9.364                     \n 3rd Qu.:0.6000                3rd Qu.:13.200                     \n Max.   :1.8000                Max.   :52.200                     \n                                                                  \n Upper.Confidence.Limit.for.Crude.Rate Age.adjusted.Rate\n Min.   : 0.10                         Min.   : 1.80    \n 1st Qu.: 4.60                         1st Qu.: 7.40    \n Median : 9.60                         Median :10.80    \n Mean   :11.07                         Mean   :11.24    \n 3rd Qu.:15.40                         3rd Qu.:14.00    \n Max.   :54.80                         Max.   :41.50    \n                                       NA's   :1632     \n Standard.Error.Age.adjusted.Rate Lower.Confidence.Limit.for.Age.adjusted.rate\n Min.   :0.000                    Min.   : 0.9                                \n 1st Qu.:0.300                    1st Qu.: 6.5                                \n Median :0.500                    Median : 9.8                                \n Mean   :0.543                    Mean   :10.2                                \n 3rd Qu.:0.700                    3rd Qu.:12.8                                \n Max.   :1.900                    Max.   :38.4                                \n NA's   :1632                     NA's   :1632                                \n Upper.Confidence.Limit.for.Age.adjusted.Rate State.Crude.Rate.in.Range\n Min.   : 2.20                                Length:2703              \n 1st Qu.: 8.20                                Class :character         \n Median :11.80                                Mode  :character         \n Mean   :12.34                                                         \n 3rd Qu.:15.30                                                         \n Max.   :44.60                                                         \n NA's   :1632                                                          \n US.Crude.Rate   US.Age.adjusted.Rate\n Min.   : 6.00   Min.   : 6.10       \n 1st Qu.: 8.90   1st Qu.: 8.90       \n Median :12.00   Median :11.90       \n Mean   :11.01   Mean   :10.96       \n 3rd Qu.:13.20   3rd Qu.:13.10       \n Max.   :16.30   Max.   :16.30       \n                                     \n\ndim(data)\n\n[1] 2703   18\n\nhead(data)\n\n  Year        Sex      Age Race.and.Hispanic.Origin         State Deaths\n1 2004 Both Sexes All Ages    All Races-All Origins         Texas   1813\n2 2012 Both Sexes All Ages    All Races-All Origins       Arizona   1134\n3 2008       Male All Ages                 Hispanic United States   1875\n4 2009 Both Sexes All Ages    All Races-All Origins    Washington    987\n5 2015     Female All Ages    All Races-All Origins United States  19447\n6 2008 Both Sexes All Ages    All Races-All Origins          Utah    468\n  Population Crude.Death.Rate Standard.Error.for.Crude.Rate\n1   22394023              8.1                           0.2\n2    6553255             17.3                           0.5\n3   24322906              7.7                           0.2\n4    6667426             14.8                           0.5\n5  163189523             11.9                           0.1\n6    2663029             17.6                           0.8\n  Low.Confidence.Limit.for.Crude.Rate Upper.Confidence.Limit.for.Crude.Rate\n1                                 7.7                                   8.5\n2                                16.3                                  18.3\n3                                 7.4                                   8.1\n4                                13.9                                  15.7\n5                                11.7                                  12.1\n6                                16.0                                  19.2\n  Age.adjusted.Rate Standard.Error.Age.adjusted.Rate\n1               8.2                              0.2\n2              17.7                              0.5\n3               8.4                              0.2\n4              14.3                              0.5\n5              11.8                              0.1\n6              19.0                              0.9\n  Lower.Confidence.Limit.for.Age.adjusted.rate\n1                                          7.8\n2                                         16.6\n3                                          8.0\n4                                         13.4\n5                                         11.7\n6                                         17.2\n  Upper.Confidence.Limit.for.Age.adjusted.Rate State.Crude.Rate.in.Range\n1                                          8.5                     7–9.8\n2                                         18.7                 15.2–41.5\n3                                          8.8                   8.3–8.4\n4                                         15.2                 12.3–15.2\n5                                         12.0                 10.6–11.8\n6                                         20.7                 15.2–41.5\n  US.Crude.Rate US.Age.adjusted.Rate\n1           9.4                  9.4\n2          13.2                 13.1\n3          12.0                 11.9\n4          12.1                 11.9\n5          16.3                 16.3\n6          12.0                 11.9\n\n\nAs seen by the output, this dataset contains 18 variables with 2,703 observations."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#processing-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#processing-the-data",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "Below, I will select the variables that I will work with in this exercise. I will reassign this to a new dataset called “processed_data”.\n\nprocessed_data &lt;- data %&gt;% select(Year, Sex, Age, Race.and.Hispanic.Origin, State, Deaths, Population, Crude.Death.Rate)\n\nI will rename the variables to be easier to understand. I will use head() to ensure this worked.\n\nprocessed_data &lt;- processed_data %&gt;%\n  rename(Race_and_Hispanic_Origin = Race.and.Hispanic.Origin,\n         Crude_Death_Rate = Crude.Death.Rate)\nhead(processed_data)\n\n  Year        Sex      Age Race_and_Hispanic_Origin         State Deaths\n1 2004 Both Sexes All Ages    All Races-All Origins         Texas   1813\n2 2012 Both Sexes All Ages    All Races-All Origins       Arizona   1134\n3 2008       Male All Ages                 Hispanic United States   1875\n4 2009 Both Sexes All Ages    All Races-All Origins    Washington    987\n5 2015     Female All Ages    All Races-All Origins United States  19447\n6 2008 Both Sexes All Ages    All Races-All Origins          Utah    468\n  Population Crude_Death_Rate\n1   22394023              8.1\n2    6553255             17.3\n3   24322906              7.7\n4    6667426             14.8\n5  163189523             11.9\n6    2663029             17.6"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#exploratorydescriptive-analysis-and-frequency-table",
    "href": "cdcdata-exercise/cdcdata-exercise.html#exploratorydescriptive-analysis-and-frequency-table",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "I will create a scatterplot to assess the relationship between deaths and population. Then, I will use the cor() function to obtain the correlation coefficient to determine the strength of this relationship.\n\nggplot(processed_data, aes(x = Deaths, y = Population)) +\n  geom_point(color = \"#429bf9\") + # giving the points of the scatterplot a blue color\n  labs(title = \"Scatterplot of Relationship between Number of Deaths \\n From Drug Poisoning and Population Size\",\n       x = \"Number of Deaths\",\n       y = \"Size of Population\") + # labeling axes and titles!\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # removing legend; making title and axes bold and giving them a larger font size\n\n\n\n\n\n\n\ncor(processed_data$Deaths, processed_data$Population) # correlation coefficient\n\n[1] 0.8997261\n\n\nThe scatterplot above, shows a positive linear relationship between the number of deaths from drug poisoning and the size of the population. The correlation coefficient is 0.8997261, which indicates a strong positive linear relationship.\nNow, I will assess the relationship between year and death. Then, I will use the cor() function to obtain the correlation coefficient to determine the strength of this relationship.\n\nggplot(processed_data, aes(x = Year, y = Deaths)) +\n  geom_point(color = \"#33d11a\") + # giving the points of the scatterplot a green color\n  labs(title = \"Scatterplot of Relationship between Number of Deaths \\n From Drug Poisoning and Year\",\n       x = \"Year\",\n       y = \"Number of Deaths\") + # labeling axes and titles!\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # removing legend; making title and axes bold and giving them a larger font size\n\n\n\n\n\n\n\ncor(processed_data$Deaths, processed_data$Year) # correlation coefficient\n\n[1] 0.1246065\n\n\nThe scatterplot above shows that there may be a slight relationship between year and number deaths due to drug poisoning. The correlation coefficient is 0.1246065; this indicates a VERY weak relationship (possibly no realtionship at all).\nI will create a correlation matrix. I did this in the “data-exercise” and found it to be helpful.\n\n# use only numberic variables!\ncor_matrix &lt;- cor(processed_data %&gt;% select(where(is.numeric)))\n\n# print out!\nprint(cor_matrix)\n\n                       Year    Deaths Population Crude_Death_Rate\nYear             1.00000000 0.1246065 0.01947317       0.32354160\nDeaths           0.12460651 1.0000000 0.89972612       0.31923101\nPopulation       0.01947317 0.8997261 1.00000000       0.06736837\nCrude_Death_Rate 0.32354160 0.3192310 0.06736837       1.00000000\n\n\nI will create a frequency table to display the total number of deaths from drug poisoning by year.\n\n# Create frequency table for the total number of deaths by year\ndeath_by_year &lt;- processed_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Deaths = sum(Deaths, na.rm = TRUE)) %&gt;%\n  arrange(Year)\n\n# View the frequency table\ndeath_by_year\n\n# A tibble: 17 × 2\n    Year Total_Deaths\n   &lt;int&gt;        &lt;int&gt;\n 1  1999       149899\n 2  2000       155057\n 3  2001       172732\n 4  2002       209642\n 5  2003       229715\n 6  2004       244414\n 7  2005       265629\n 8  2006       306901\n 9  2007       321148\n10  2008       324838\n11  2009       329252\n12  2010       341295\n13  2011       367830\n14  2012       368968\n15  2013       391308\n16  2014       418345\n17  2015       465632"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#assessing-distribution",
    "href": "cdcdata-exercise/cdcdata-exercise.html#assessing-distribution",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "I will assess the distribution of both the number of deaths due to drug poisoning and the population size.\n\n# Deaths\nggplot(processed_data, aes(x = Deaths)) +\n  geom_histogram(bins = 30, fill = \"#ee5050\", color = \"#c41616\", alpha = 0.7) +\n  labs(title = \"Histogram of Deaths from Drug Poisoning\",\n       x = \"Number of Deaths\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n# Population Size\nggplot(processed_data, aes(x = Population)) +\n  geom_histogram(bins = 30, fill = \"#f9c363\", color = \"#ffa303\", alpha = 0.7) +\n  labs(title = \"Histogram of Population Size\",\n       x = \"Population Size\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nBoth histograms for number of deaths due to drug poisoning and population size appeared to be skewed to the right.\nI will create a table to display the mean and standard deviations of the number of deaths due to drug poisoning from 1999 to 2015 as well as the population size from 1999 to 2015.\n\ndistribution_table &lt;- processed_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Mean_Deaths = mean(Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Deaths, na.rm = TRUE),\n    Mean_Population = mean(Population, na.rm = TRUE),\n    SD_Population = sd(Population, na.rm = TRUE)\n  )\n\nprint(distribution_table) # print table to see it!\n\n# A tibble: 17 × 5\n    Year Mean_Deaths SD_Deaths Mean_Population SD_Population\n   &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  1999        943.     2155.       15462000.     32759001.\n 2  2000        975.     2247.       15584898.     32954121.\n 3  2001       1086.     2507.       15769468.     33258376.\n 4  2002       1319.     3045.       15906943.     33474245.\n 5  2003       1445.     3339.       16035120.     33675188.\n 6  2004       1537.     3559.       16175519.     33902213.\n 7  2005       1671.     3839.       16316254.     34126543.\n 8  2006       1930.     4434.       16465436.     34368590.\n 9  2007       2020.     4659.       16614163.     34609414.\n10  2008       2043.     4734.       16763441.     34851622.\n11  2009       2071.     4789.       16902888.     35076777.\n12  2010       2147.     4963.       17005636.     35242661.\n13  2011       2313.     5337.       17154800.     35484630.\n14  2012       2321.     5335.       17270704.     35678128.\n15  2013       2461.     5628.       17382466.     35862756.\n16  2014       2631.     6014.       17517589.     36089522.\n17  2015       2929.     6683.       17644933.     36306209."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#this-section-is-contributed-by-yufei-wu",
    "href": "cdcdata-exercise/cdcdata-exercise.html#this-section-is-contributed-by-yufei-wu",
    "title": "CDC Data Exercise",
    "section": "This section is contributed by Yufei Wu",
    "text": "This section is contributed by Yufei Wu\n\nSummarize the distribution/composition of each variable\n\n# Load package\nlibrary(gtsummary)\nsummary_table &lt;- processed_data %&gt;% \n  tbl_summary(statistic = list(\n    all_continuous() ~ \"{mean} {median} {min} {max} {sd}\",\n    all_categorical() ~ \"{n} {N} {p}%\"\n  )) %&gt;% \n  as_tibble()\n#print(summary_table)\nprint(summary_table, n = Inf)\n\n# A tibble: 76 × 2\n   `**Characteristic**`     `**N = 2,703**`                                    \n   &lt;chr&gt;                    &lt;chr&gt;                                              \n 1 Year                     2,007 2,007 1,999 2,015 5                          \n 2 Sex                      &lt;NA&gt;                                               \n 3 Both Sexes               1,479 2,703 55%                                    \n 4 Female                   612 2,703 23%                                      \n 5 Male                     612 2,703 23%                                      \n 6 Age                      &lt;NA&gt;                                               \n 7 15-24 years              204 2,703 7.5%                                     \n 8 25-34 years              204 2,703 7.5%                                     \n 9 35-44 years              204 2,703 7.5%                                     \n10 45-54 years              204 2,703 7.5%                                     \n11 55-64 years              204 2,703 7.5%                                     \n12 65-74 years              204 2,703 7.5%                                     \n13 75+ years                204 2,703 7.5%                                     \n14 All Ages                 1,071 2,703 40%                                    \n15 Less than 15 years       204 2,703 7.5%                                     \n16 Race_and_Hispanic_Origin &lt;NA&gt;                                               \n17 All Races-All Origins    1,326 2,703 49%                                    \n18 Hispanic                 459 2,703 17%                                      \n19 Non-Hispanic Black       459 2,703 17%                                      \n20 Non-Hispanic White       459 2,703 17%                                      \n21 State                    &lt;NA&gt;                                               \n22 Alabama                  17 2,703 0.6%                                      \n23 Alaska                   17 2,703 0.6%                                      \n24 Arizona                  17 2,703 0.6%                                      \n25 Arkansas                 17 2,703 0.6%                                      \n26 California               17 2,703 0.6%                                      \n27 Colorado                 17 2,703 0.6%                                      \n28 Connecticut              17 2,703 0.6%                                      \n29 Delaware                 17 2,703 0.6%                                      \n30 District of Columbia     17 2,703 0.6%                                      \n31 Florida                  17 2,703 0.6%                                      \n32 Georgia                  17 2,703 0.6%                                      \n33 Hawaii                   17 2,703 0.6%                                      \n34 Idaho                    17 2,703 0.6%                                      \n35 Illinois                 17 2,703 0.6%                                      \n36 Indiana                  17 2,703 0.6%                                      \n37 Iowa                     17 2,703 0.6%                                      \n38 Kansas                   17 2,703 0.6%                                      \n39 Kentucky                 17 2,703 0.6%                                      \n40 Louisiana                17 2,703 0.6%                                      \n41 Maine                    17 2,703 0.6%                                      \n42 Maryland                 17 2,703 0.6%                                      \n43 Massachusetts            17 2,703 0.6%                                      \n44 Michigan                 17 2,703 0.6%                                      \n45 Minnesota                17 2,703 0.6%                                      \n46 Mississippi              17 2,703 0.6%                                      \n47 Missouri                 17 2,703 0.6%                                      \n48 Montana                  17 2,703 0.6%                                      \n49 Nebraska                 17 2,703 0.6%                                      \n50 Nevada                   17 2,703 0.6%                                      \n51 New Hampshire            17 2,703 0.6%                                      \n52 New Jersey               17 2,703 0.6%                                      \n53 New Mexico               17 2,703 0.6%                                      \n54 New York                 17 2,703 0.6%                                      \n55 North Carolina           17 2,703 0.6%                                      \n56 North Dakota             17 2,703 0.6%                                      \n57 Ohio                     17 2,703 0.6%                                      \n58 Oklahoma                 17 2,703 0.6%                                      \n59 Oregon                   17 2,703 0.6%                                      \n60 Pennsylvania             17 2,703 0.6%                                      \n61 Rhode Island             17 2,703 0.6%                                      \n62 South Carolina           17 2,703 0.6%                                      \n63 South Dakota             17 2,703 0.6%                                      \n64 Tennessee                17 2,703 0.6%                                      \n65 Texas                    17 2,703 0.6%                                      \n66 United States            1,836 2,703 68%                                    \n67 Utah                     17 2,703 0.6%                                      \n68 Vermont                  17 2,703 0.6%                                      \n69 Virginia                 17 2,703 0.6%                                      \n70 Washington               17 2,703 0.6%                                      \n71 West Virginia            17 2,703 0.6%                                      \n72 Wisconsin                17 2,703 0.6%                                      \n73 Wyoming                  17 2,703 0.6%                                      \n74 Deaths                   1,873 478 1 52,404 4,525                           \n75 Population               16,586,603 6,403,290 238,163 321,418,820 34,492,886\n76 Crude_Death_Rate         10 9 0 54 8                                        \n\n\n\n\nGenerate the synthetic data\nI would like to generate the synthetic data according to the composition of categorical variable and the distribution of continuous variable. I tried to use ChatGPT to generate the data for me, and here is my initial prompt: “Generate synthetic data: there are 8 variables and 2073 observations. For the continuous variable Year, it is from 1,999 to 2015, mean 2,007, median 2,007. For Sex, Age, Race_and_Hispanic_Origin, and State, refer to the gtsummary tibble. For Population, it is from 238,163 to 321,418,820, mean 16,586,603, median 6,403,290, sd 34,492,886, the distribution is skewed to the right. For Deaths, it is from 1 to 52,404, mean 1,873, median 478, sd 4,525, the distribution is skewed to the right. Crude_Death_Rate is from Deaths divided by Population. There is a positive linear relationship between Deaths and Population. The correlation coefficient is 0.8997. Give me the code to generate such data.” Then I manually add the specific content in the code to generate categorical data. At first, I tired to add correlation between Deaths and Population, but I found things get more and more complicated and could not work. I suspect that the correlation in the original data was due to some outliers, so I eventually generate the deaths data without considering the correlation.\n\nset.seed(123)  # Set seed for reproducibility\n\n# Number of observations\nn &lt;- 2073\n\n# 1. Generate 'Year' (Continuous Variable)\nyear_min &lt;- 1999\nyear_max &lt;- 2015\nyear_mean &lt;- 2007\nyear_median &lt;- 2007\n\n# Generating Year as uniformly distributed between min and max\nyear &lt;- sample(year_min:year_max, n, replace = TRUE)\n\n# 2. Generate 'Sex' (Categorical Variable)\nsex_levels &lt;- c(\"Both Sexes\", \"Female\", \"Male\")\nsex_probs &lt;- c(0.55, 0.225, 0.225) \nsex &lt;- sample(sex_levels, n, replace = TRUE, prob = sex_probs)\n\n# 3. Generate 'Age' (Categorical Variable)\nage_levels &lt;- c(\"15-24 years\", \"25-34 years\", \"35-44 years\", \"45-54 years\", \"55-64 years\", \"65-74 years\", \"75+ years\", \"All Ages\", \"Less than 15 years\")\nage_probs &lt;- c(0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.075, 0.4, 0.075) \nage &lt;- sample(age_levels, n, replace = TRUE, prob = age_probs)\n\n# 4. Generate 'Race_and_Hispanic_Origin' (Categorical Variable)\nrace_levels &lt;- c(\"All Races-All Origins\", \"Hispanic\", \"Non-Hispanic Black\", \"Non-Hispanic White\")\nrace_probs &lt;- c(0.49, 0.17, 0.17, 0.17) \nrace_and_hispanic_origin &lt;- sample(race_levels, n, replace = TRUE, prob = race_probs)\n\n# 5. Generate 'State' (Categorical Variable)\nstate_levels &lt;- c(\"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\", \"Delaware\", \n\"District of Columbia\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\", \n\"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\", \"Minnesota\", \n\"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\", \"New Jersey\", \n\"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\", \"Oregon\", \n\"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\", \n\"United States\", \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\") \nstate_probs &lt;- c(0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, \n0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, \n0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, \n0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063, \n0.0063, 0.0063, 0.0063, 0.0063, 0.6787, 0.0063, 0.0063, 0.0063, 0.0063, 0.0063,\n0.0063, 0.0063) \nstate &lt;- sample(state_levels, n, replace = TRUE, prob = state_probs)\n\n# 6. Generate 'Population' (Continuous Variable) - skewed to the right\npopulation_min &lt;- 238163\npopulation_max &lt;- 321418820\npopulation_mean &lt;- 16586603\npopulation_median &lt;- 6403290\npopulation_sd &lt;- 34492886\n\n# Generate skewed population data using a log-normal distribution\npopulation &lt;- rlnorm(n, meanlog = log(population_mean), sdlog = log(population_sd / population_mean))\n\n# Ensure values are within range\npopulation &lt;- pmin(pmax(population, population_min), population_max)\n\n# 7. Generate 'Deaths' (Continuous Variable) - skewed to the right\ndeaths_min &lt;- 1\ndeaths_max &lt;- 52404\ndeaths_mean &lt;- 1873\ndeaths_median &lt;- 478\ndeaths_sd &lt;- 4525\n\n# Compute meanlog correctly\nmeanlog &lt;- log(deaths_mean) - 0.5 * log(1 + (deaths_sd / deaths_mean)^2)\nsdlog &lt;- sqrt(log(1 + (deaths_sd / deaths_mean)^2))\n\n# Generate right-skewed death counts\ndeaths &lt;- rlnorm(n, meanlog = meanlog, sdlog = sdlog)\n\n# Ensure values are within range\ndeaths &lt;- pmin(pmax(deaths, deaths_min), deaths_max)\n\n# 8. Generate 'Crude_Death_Rate' (Calculated as Deaths / Population)\ncrude_death_rate &lt;- deaths / population * 1000  # Crude Death Rate per 1000 people\n\n# Combine all the variables into a data frame\nsynthetic_data &lt;- data.frame(\n  year = year,\n  sex = sex,\n  age = age,\n  race_and_hispanic_origin = race_and_hispanic_origin,\n  state = state,\n  population = population,\n  deaths = deaths,\n  crude_death_rate = crude_death_rate\n)\n\n\n\nExplore the synthetic data\n\n# Get an overview of the data\nsummary(synthetic_data)\n\n      year          sex                age            race_and_hispanic_origin\n Min.   :1999   Length:2073        Length:2073        Length:2073             \n 1st Qu.:2003   Class :character   Class :character   Class :character        \n Median :2007   Mode  :character   Mode  :character   Mode  :character        \n Mean   :2007                                                                 \n 3rd Qu.:2011                                                                 \n Max.   :2015                                                                 \n    state             population            deaths         crude_death_rate \n Length:2073        Min.   :  1170322   Min.   :    2.59   Min.   :0.00009  \n Class :character   1st Qu.:  9855446   1st Qu.:  261.33   1st Qu.:0.01399  \n Mode  :character   Median : 15827533   Median :  704.64   Median :0.04387  \n                    Mean   : 20958124   Mean   : 1750.65   Mean   :0.13607  \n                    3rd Qu.: 27226765   3rd Qu.: 1694.76   3rd Qu.:0.12293  \n                    Max.   :153425514   Max.   :52404.00   Max.   :4.15957  \n\ndim(synthetic_data)\n\n[1] 2073    8\n\nhead(synthetic_data)\n\n  year        sex         age race_and_hispanic_origin         state population\n1 2013       Male    All Ages       Non-Hispanic White       Vermont   18232163\n2 2012 Both Sexes    All Ages    All Races-All Origins United States   16474079\n3 2001 Both Sexes 25-34 years    All Races-All Origins  Pennsylvania   21448794\n4 2008     Female 15-24 years    All Races-All Origins United States   31220194\n5 2009       Male 35-44 years       Non-Hispanic Black United States   17713506\n6 2003       Male    All Ages                 Hispanic United States    9014091\n      deaths crude_death_rate\n1 4294.85196      0.235564594\n2   52.01475      0.003157369\n3  204.27761      0.009523967\n4  523.45079      0.016766417\n5 1111.77805      0.062764425\n6  455.80316      0.050565627\n\n# Assess the distribution of deaths and population\n# Deaths\nggplot(synthetic_data, aes(x = deaths)) +\n  geom_histogram(bins = 30, fill = \"#ee5050\", color = \"#c41616\", alpha = 0.7) +\n  labs(title = \"Histogram of Deaths from Drug Poisoning\",\n       x = \"Number of Deaths\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n# Population Size\nggplot(synthetic_data, aes(x = population)) +\n  geom_histogram(bins = 30, fill = \"#f9c363\", color = \"#ffa303\", alpha = 0.7) +\n  labs(title = \"Histogram of Population Size\",\n       x = \"Population Size\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n# Create frequency table for the total number of deaths by year\ndeath_by_year1 &lt;- synthetic_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(total_deaths = sum(deaths, na.rm = TRUE)) %&gt;%\n  arrange(year)\n\n# View the frequency table\ndeath_by_year1\n\n# A tibble: 17 × 2\n    year total_deaths\n   &lt;int&gt;        &lt;dbl&gt;\n 1  1999      205108.\n 2  2000      118721.\n 3  2001      211118.\n 4  2002      174512.\n 5  2003      222735.\n 6  2004      178661.\n 7  2005      227624.\n 8  2006      255111.\n 9  2007      208903.\n10  2008      316677.\n11  2009      204403.\n12  2010      229018.\n13  2011      239380.\n14  2012      164513.\n15  2013      223135.\n16  2014      209644.\n17  2015      239843.\n\n# Create table to display the mean and standard deviations of the number of deaths from 1999 to 2015 as well as populationfrom 1999 to 2015\ndistribution_table1 &lt;- synthetic_data %&gt;%\n  group_by(year) %&gt;%\n  summarise(\n    mean_deaths = mean(deaths, na.rm = TRUE),\n    sd_deaths = sd(deaths, na.rm = TRUE),\n    mean_population = mean(population, na.rm = TRUE),\n    sd_population = sd(population, na.rm = TRUE)\n  )\n\nprint(distribution_table1) \n\n# A tibble: 17 × 5\n    year mean_deaths sd_deaths mean_population sd_population\n   &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  1999       1991.     5333.       21005538.     15573960.\n 2  2000       1079.     1378.       21995782.     19791550.\n 3  2001       1804.     3718.       21937667.     16420190.\n 4  2002       1616.     3868.       21514401.     17040413.\n 5  2003       1727.     2471.       21073481.     18234934.\n 6  2004       1527.     2765.       19285640.     12489584.\n 7  2005       1765.     3191.       22046780.     17595844.\n 8  2006       1701.     3838.       20955534.     16268054.\n 9  2007       1685.     2600.       21269368.     17500794.\n10  2008       2246.     4657.       20562263.     17268516.\n11  2009       1585.     2819.       20466302.     13805349.\n12  2010       1818.     3550.       21414766.     18671639.\n13  2011       2216.     3630.       22605281.     18920198.\n14  2012       1348.     1649.       20251460.     17593593.\n15  2013       1771.     3742.       19885278.     11432565.\n16  2014       1906.     3649.       19016220.     17742970.\n17  2015       1934.     5178.       21178114.     16480345.\n\n\nThe overall distribution of variables in the synthetic data is very similar to the original data. The trends of total deaths and population by the year are also similar to the original ones, while the trend of mean deaths is not very similar to the original one. Generally speaking, the synthetic data is good while with some imperfection."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Introduction and Background\n\nMy name is Natalie and I am a first-year Epidemiology student. I’m from Radford, Virginia and graduated from Virginia Tech this past May with a degree in Microbiology.\nI participated in a few research projects during my time as an undergrad student. These were all wet lab focused (working with La Crosse virus and the fungus P. destructans). I am familiar with R since taking the EPID 7500 course last semester. However, I have not done any research involving programming and coding.\nA fun fact is that I have been to 46 out of the 50 states. The four I have not been to are: Alaska, Hawaii, New Mexico, and Vermont.\n\n\n\n\nGraduation Photo\n\n\n\n\nArticle and Brief Summary\nI chose to read a small article on the history of data analytics: A Brief History of Data Analytics\nThe article highlights how data analytics has been around for much longer than many of us realize. Data analytics looked much different than it does now, but it was still occurring in some form. For example, in 18,000 BCE bones and sticks were used to keep records of trading behaviors and the use of supplies. Statistics weren’t brought into the picture until 1663, when John Graunt kept mortality data in an effort to help track the plague. The article discusses how the internet had a great effect on the way data was collected and analyzed. The internet made communication between researchers much easier and for some data to be made public. The development of software also had a great impact on data analytics."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "I chose to complete option 2: generating a synthetic dataset. Note: I will create data with a simple “rectangular structure.”"
  },
  {
    "objectID": "data-exercise/data-exercise.html#loading-packages",
    "href": "data-exercise/data-exercise.html#loading-packages",
    "title": "Data Exercise",
    "section": "Loading Packages",
    "text": "Loading Packages\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows"
  },
  {
    "objectID": "data-exercise/data-exercise.html#setting-seed",
    "href": "data-exercise/data-exercise.html#setting-seed",
    "title": "Data Exercise",
    "section": "Setting Seed",
    "text": "Setting Seed\nI will set a seed so that the synthetic dataset will be reproducible with the code I create in this activity.\n\n# Set seed \nset.seed(116)\n# Assign how many patients/observations I want to generate\nn_patients &lt;- 100 # I will generate only 100 patients for this exercise since this is the first time I am creating a synthetic dataset and I want to keep things simple!"
  },
  {
    "objectID": "data-exercise/data-exercise.html#generating-data",
    "href": "data-exercise/data-exercise.html#generating-data",
    "title": "Data Exercise",
    "section": "Generating Data",
    "text": "Generating Data\nNow, I will generate my synthetic data set. I will create a data frame with fake data with several measures and risk factors. I will add correlations between some of the variables.\n\n# Create an empty data frame with placeholders for variables\nsynthetic_data &lt;- data.frame(\n  Patient_ID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = integer(n_patients),\n  Enrollment_Date = lubridate::as_date(character(n_patients)),\n  Height = numeric(n_patients),\n  Weight = numeric(n_patients),\n  Blood_Pressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  Diabetes = integer(n_patients),\n  Smoking = integer(n_patients))\n\n# Variable 1: Patient ID\nsynthetic_data$Patient_ID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\nsynthetic_data$Age &lt;- round(runif(n_patients, min = 18, max = 90), 1)\n\n# Variable 3: Gender (binary variable; 0 = Male, 1 = Female)\nsynthetic_data$Gender &lt;- as.numeric(sample(c(0, 1), n_patients, replace = TRUE))\n\n# Variable 4: Date of Enrollment (date variable)\nsynthetic_data$Enrollment_Date &lt;- lubridate::as_date(sample(seq(from = lubridate::as_date(\"2022-01-01\"), to = lubridate::as_date(\"2022-12-31\"), by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 5: Height (numeric variable; in inches)\nsynthetic_data$Height &lt;- round(runif(n_patients, min = 57, max = 78), 1)\n\n# Variable 6: Weight (numeric variable; in lbs; dependent on Height) \nsynthetic_data$Weight &lt;- ifelse(\n  synthetic_data$Height &gt;= 57 & synthetic_data$Height &lt;= 62, \n  round(rnorm(sum(synthetic_data$Height &gt;= 57 & synthetic_data$Height &lt;= 62), mean = 120, sd = 10), 1),ifelse(synthetic_data$Height &gt; 62 & synthetic_data$Height &lt;= 70, round(rnorm(sum(synthetic_data$Height &gt; 62 & synthetic_data$Height &lt;= 70), mean = 160, sd = 15), 1),\n    round(rnorm(sum(synthetic_data$Height &gt; 70 & synthetic_data$Height &lt;= 78), mean = 190, sd = 20), 1)))\n\n# Variable 7: Blood Pressure (numeric variable)\nsynthetic_data$Blood_Pressure &lt;- round(runif(n_patients, min = 90, max = 160), 1)\n\n# Variable 8: Cholesterol Level (numeric variable; in mg/dL; dependent on Weight)\nsynthetic_data$Cholesterol &lt;- ifelse(synthetic_data$Weight &gt;= 70 & synthetic_data$Weight &lt;= 130, round(rnorm(sum(synthetic_data$Weight &gt;= 70 & synthetic_data$Weight &lt;= 130), mean = 160, sd = 10), 1), ifelse(synthetic_data$Weight &gt;= 131 & synthetic_data$Weight &lt;= 180, round(rnorm(sum(synthetic_data$Weight &gt;= 131 & synthetic_data$Weight &lt;= 180), mean = 185, sd = 10), 1), round(rnorm(sum(synthetic_data$Weight &gt;= 181 & synthetic_data$Weight &lt;= 200), mean = 210, sd = 10), 1)))\n\n# Variable 9: Diabetes (binary variable; 0 = Not Diabetic, 1 = Diabetic)\nsynthetic_data$Diabetes &lt;- as.numeric(sample(c(0, 1), n_patients, replace = TRUE))\n\n# Variable 10: Smoking (binary variable; 0 = Does Not Smoke, 1 = Smokes)\nsynthetic_data$Smoking &lt;- as.numeric(sample(c(0, 1), n_patients, replace = TRUE))\n\n# Print the first few rows of the generated data\nhead(synthetic_data)\n\n  Patient_ID  Age Gender Enrollment_Date Height Weight Blood_Pressure\n1          1 71.3      0      2022-11-12   68.6  170.6           96.0\n2          2 42.2      0      2022-12-16   69.0  168.9          125.8\n3          3 31.9      1      2022-11-20   65.2  171.2          156.6\n4          4 38.2      0      2022-10-05   78.0  155.7          109.0\n5          5 89.8      1      2022-03-31   77.1  183.8          100.8\n6          6 56.5      0      2022-06-23   62.1  174.1           95.2\n  Cholesterol Diabetes Smoking\n1       177.0        1       0\n2       184.7        1       0\n3       192.5        1       0\n4       194.5        0       0\n5       205.4        0       0\n6       182.9        0       0\n\n\nLuckily from the head() output, I can see that this worked!"
  },
  {
    "objectID": "data-exercise/data-exercise.html#exploring-data-structure",
    "href": "data-exercise/data-exercise.html#exploring-data-structure",
    "title": "Data Exercise",
    "section": "Exploring Data Structure",
    "text": "Exploring Data Structure\nNow, I will explore the synthetic dataset I just created. I will do this with the summary(), str(), and glimpse.\n\nsummary(synthetic_data)\n\n   Patient_ID          Age            Gender    Enrollment_Date     \n Min.   :  1.00   Min.   :18.20   Min.   :0.0   Min.   :2022-01-06  \n 1st Qu.: 25.75   1st Qu.:33.95   1st Qu.:0.0   1st Qu.:2022-04-18  \n Median : 50.50   Median :57.00   Median :0.5   Median :2022-07-18  \n Mean   : 50.50   Mean   :55.38   Mean   :0.5   Mean   :2022-07-18  \n 3rd Qu.: 75.25   3rd Qu.:71.38   3rd Qu.:1.0   3rd Qu.:2022-10-25  \n Max.   :100.00   Max.   :89.90   Max.   :1.0   Max.   :2022-12-27  \n     Height          Weight      Blood_Pressure   Cholesterol       Diabetes  \n Min.   :57.20   Min.   : 93.7   Min.   : 90.1   Min.   :129.3   Min.   :0.0  \n 1st Qu.:62.02   1st Qu.:132.0   1st Qu.:107.2   1st Qu.:170.2   1st Qu.:0.0  \n Median :68.00   Median :160.9   Median :120.0   Median :182.4   Median :0.5  \n Mean   :67.55   Mean   :159.1   Mean   :123.4   Mean   :183.3   Mean   :0.5  \n 3rd Qu.:72.97   3rd Qu.:179.2   3rd Qu.:139.9   3rd Qu.:195.1   3rd Qu.:1.0  \n Max.   :78.00   Max.   :228.6   Max.   :158.9   Max.   :217.9   Max.   :1.0  \n    Smoking    \n Min.   :0.00  \n 1st Qu.:0.00  \n Median :0.00  \n Mean   :0.48  \n 3rd Qu.:1.00  \n Max.   :1.00  \n\nstr(synthetic_data)\n\n'data.frame':   100 obs. of  10 variables:\n $ Patient_ID     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Age            : num  71.3 42.2 31.9 38.2 89.8 56.5 87 18.2 48.1 67.3 ...\n $ Gender         : num  0 0 1 0 1 0 0 1 1 1 ...\n $ Enrollment_Date: Date, format: \"2022-11-12\" \"2022-12-16\" ...\n $ Height         : num  68.6 69 65.2 78 77.1 62.1 62.1 71.5 69.5 68.4 ...\n $ Weight         : num  171 169 171 156 184 ...\n $ Blood_Pressure : num  96 126 157 109 101 ...\n $ Cholesterol    : num  177 185 192 194 205 ...\n $ Diabetes       : num  1 1 1 0 0 0 0 0 1 1 ...\n $ Smoking        : num  0 0 0 0 0 0 0 0 1 0 ...\n\nglimpse(synthetic_data)\n\nRows: 100\nColumns: 10\n$ Patient_ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ Age             &lt;dbl&gt; 71.3, 42.2, 31.9, 38.2, 89.8, 56.5, 87.0, 18.2, 48.1, …\n$ Gender          &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ Enrollment_Date &lt;date&gt; 2022-11-12, 2022-12-16, 2022-11-20, 2022-10-05, 2022-…\n$ Height          &lt;dbl&gt; 68.6, 69.0, 65.2, 78.0, 77.1, 62.1, 62.1, 71.5, 69.5, …\n$ Weight          &lt;dbl&gt; 170.6, 168.9, 171.2, 155.7, 183.8, 174.1, 175.4, 182.6…\n$ Blood_Pressure  &lt;dbl&gt; 96.0, 125.8, 156.6, 109.0, 100.8, 95.2, 155.1, 99.9, 1…\n$ Cholesterol     &lt;dbl&gt; 177.0, 184.7, 192.5, 194.5, 205.4, 182.9, 163.2, 198.9…\n$ Diabetes        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ Smoking         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, …\n\n\nThe summary output shows the minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This data is more useful for the continuous variables, such as Height, Weight, and Cholesterol. The str output shows the class of each variable. The glimpse output shows the first few values of each variable."
  },
  {
    "objectID": "data-exercise/data-exercise.html#plots-tables-correlations",
    "href": "data-exercise/data-exercise.html#plots-tables-correlations",
    "title": "Data Exercise",
    "section": "Plots, Tables, Correlations",
    "text": "Plots, Tables, Correlations\n\nScatterplot and Correlation of Height and Weight\nSince I created the weight variable to be depenent on height, I will create a scatterplot to examine the reationship between these two.\n\nggplot(synthetic_data, aes(x = Weight, y = Height)) +\n  geom_point(color = \"#3067c2\") +\n  labs(title = \"Scatterplot of Weight and Height\",\n       x = \"Weight (lbs)\",\n       y = \"Height (inches)\") + \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nThe scaterplot above depicts a positive relationship between weight and height. This means that we see an increase in weight as height increases.\nNow, I will create code to assess the strength of the correlation between weight and height using the cor() function.\n\ncor(synthetic_data$Weight, synthetic_data$Height)\n\n[1] 0.709389\n\n\nThe correlation between weight and height is 0.750217, which is pretty high. This indicates that there is a relatively strong relationship between weight and height.\n\n\nScatterplot and Correlation of Weight and Cholesterol Level\nSince I created the cholesterol variable to be depenent on weight, I will create a scatterplot to examine the reationship between these two.\n\nggplot(synthetic_data, aes(x = Weight, y = Cholesterol)) +\n  geom_point(color = \"#63c230\") +\n  labs(title = \"Scatterplot of Weight and Cholesterol Level\",\n       x = \"Weight (lbs)\",\n       y = \"Cholesterol Level (mg/dL)\") + \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nThe scaterplot above reveals that there is a positive relationship between weight and cholesterol level. This means that as weight increases, cholesterol level also increases.\nNow, I will create code to assess the strength of the correlation between weight and cholesterol level by using the cor() function.\n\ncor(synthetic_data$Weight, synthetic_data$Cholesterol)\n\n[1] 0.8155417\n\n\nThe correlation between weight and cholesterol level is 0.7709426, which is pretty high. This indicates that there is a relatively strong relationship between weight and cholesterol level.\n\n\nScatterplot and Correlation of Height and Cholesterol Level\nSince I created a relationship between weight and cholesterol as well as a relationship between height and weight, I am checking to see if height and cholesterol level have a relationship as a result. I will do this by creating a scatterplot.\n\nggplot(synthetic_data, aes(x = Height, y = Cholesterol)) +\n  geom_point(color = \"#e83d3d\") +\n  labs(title = \"Scatterplot of Weight and Height\",\n       x = \"Height (inches)\",\n       y = \"Cholesterol Level (mg/dL)\") + \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nFrom the scatterplot above, it appears as though there is a relationship between cholesterol level and height due to the relationship I created between weight and cholesterol level. The points of the scatterplot appear to be a bit more spaced out than those of the previous scatterplots.\nNext, I will assess the strength of the correlation between height and cholesterol level by using the cor() function.\n\ncor(synthetic_data$Height, synthetic_data$Cholesterol)\n\n[1] 0.6777356\n\n\nThe correlation between height and cholesterol level is 0.5984226, which is moderate. This indicates there is a moderate relationship between height and cholesterol level. However, this relationship is not as strong as the relationship between weight and cholesterol level.\n\n\nCorrelation Matrix of All Variables\nI will create a correlation matrix to display the correlations between all of the variables within this synthetic dataset.\n\n# I will select only numeric variables (date wouldn't make sense to include here)\ncor_matrix &lt;- cor(synthetic_data %&gt;% select(where(is.numeric)))\n\n# Print out so we can see the correlation matrix!\nprint(cor_matrix)\n\n                Patient_ID          Age       Gender      Height       Weight\nPatient_ID      1.00000000  0.041808056 -0.090071146 -0.12744041 -0.171766405\nAge             0.04180806  1.000000000  0.009395789 -0.09858675  0.050323944\nGender         -0.09007115  0.009395789  1.000000000 -0.01433314  0.005319659\nHeight         -0.12744041 -0.098586750 -0.014333136  1.00000000  0.709388957\nWeight         -0.17176640  0.050323944  0.005319659  0.70938896  1.000000000\nBlood_Pressure  0.09974092  0.021928615  0.067946307 -0.03859859 -0.176520089\nCholesterol    -0.16167647  0.057061265  0.069728916  0.67773561  0.815541663\nDiabetes       -0.06997835  0.087519153  0.000000000 -0.03816801 -0.051150566\nSmoking         0.13590834  0.021204714 -0.040032038  0.05432258  0.051626270\n               Blood_Pressure Cholesterol    Diabetes     Smoking\nPatient_ID         0.09974092 -0.16167647 -0.06997835  0.13590834\nAge                0.02192862  0.05706127  0.08751915  0.02120471\nGender             0.06794631  0.06972892  0.00000000 -0.04003204\nHeight            -0.03859859  0.67773561 -0.03816801  0.05432258\nWeight            -0.17652009  0.81554166 -0.05115057  0.05162627\nBlood_Pressure     1.00000000 -0.23075388 -0.10877200  0.03482333\nCholesterol       -0.23075388  1.00000000 -0.04657569 -0.03045310\nDiabetes          -0.10877200 -0.04657569  1.00000000  0.04003204\nSmoking            0.03482333 -0.03045310  0.04003204  1.00000000\n\n\nI will now put this correlation matrix into a table and use the KableExtra package.\n\nkable(cor_matrix, caption = \"Correlation Matrix of Numeric Variables\") %&gt;%\n  kable_styling(\"striped\", full_width = F) %&gt;%\n  row_spec(0, background = \"#00509e\", color = \"white\") %&gt;%  # Blue header with white text\n  row_spec(1:nrow(cor_matrix), background = \"#d6eaf8\")  # Light blue for rows\n\n\nCorrelation Matrix of Numeric Variables\n\n\n\nPatient_ID\nAge\nGender\nHeight\nWeight\nBlood_Pressure\nCholesterol\nDiabetes\nSmoking\n\n\n\n\nPatient_ID\n1.0000000\n0.0418081\n-0.0900711\n-0.1274404\n-0.1717664\n0.0997409\n-0.1616765\n-0.0699784\n0.1359083\n\n\nAge\n0.0418081\n1.0000000\n0.0093958\n-0.0985867\n0.0503239\n0.0219286\n0.0570613\n0.0875192\n0.0212047\n\n\nGender\n-0.0900711\n0.0093958\n1.0000000\n-0.0143331\n0.0053197\n0.0679463\n0.0697289\n0.0000000\n-0.0400320\n\n\nHeight\n-0.1274404\n-0.0985867\n-0.0143331\n1.0000000\n0.7093890\n-0.0385986\n0.6777356\n-0.0381680\n0.0543226\n\n\nWeight\n-0.1717664\n0.0503239\n0.0053197\n0.7093890\n1.0000000\n-0.1765201\n0.8155417\n-0.0511506\n0.0516263\n\n\nBlood_Pressure\n0.0997409\n0.0219286\n0.0679463\n-0.0385986\n-0.1765201\n1.0000000\n-0.2307539\n-0.1087720\n0.0348233\n\n\nCholesterol\n-0.1616765\n0.0570613\n0.0697289\n0.6777356\n0.8155417\n-0.2307539\n1.0000000\n-0.0465757\n-0.0304531\n\n\nDiabetes\n-0.0699784\n0.0875192\n0.0000000\n-0.0381680\n-0.0511506\n-0.1087720\n-0.0465757\n1.0000000\n0.0400320\n\n\nSmoking\n0.1359083\n0.0212047\n-0.0400320\n0.0543226\n0.0516263\n0.0348233\n-0.0304531\n0.0400320\n1.0000000\n\n\n\n\n\n\n\nI will now create a graphical version of the correlation matrix that aids with visualization of the relationships between variables of this synthetic dataset. I will do this using the corrplot() function.\n\n# Define custom colors\ncorrplot_colors &lt;- colorRampPalette(c(\"#e83d3d\", \"white\", \"#63c230\"))(200)\n\n# Create the correlation plot with variable names on both sides\ncorrplot(cor_matrix, \n         method = \"circle\",       # Circle method\n         type = \"lower\",          # Lower half of the correlation matrix (since including the upper half would be repetitive)\n         order = \"hclust\",        # Order variables by hierarchical clustering\n         col = corrplot_colors,   # Custom color palette above!\n         tl.col = \"black\",        # Variable names in black\n         addCoef.col = \"black\",   # Correlation coefficients in black\n         number.cex = 0.5,        # Adjust coefficient size \n         number.digits = 2,       # Display two decimal places for coefficients\n         tl.srt = 45,             # Rotate labels on x-axis\n         mar = c(0, 0, 1, 0))     # Margins to adjust spacing\ntitle(\"Correlations from Synethic Dataset\", line = 2, cex.main = 1.5) # Adding a title!\n\n\n\n\n\n\n\n\nThe correlation matrix and plot above show the relationships between all of the variables in the synthetic dataset. The correlation matrix shows the correlation coefficients between each pair of variables. The correlation plot shows the same information in a visual format."
  },
  {
    "objectID": "data-exercise/data-exercise.html#simple-linear-models",
    "href": "data-exercise/data-exercise.html#simple-linear-models",
    "title": "Data Exercise",
    "section": "Simple Linear Models",
    "text": "Simple Linear Models\nI will now use the lm() function to fit a linear model with cholesterol as the outcome and weight as the predictor. Then, I will apply the summary() function.\n\nCholesterol_Weight &lt;- lm(Cholesterol ~ Weight, data = synthetic_data)\nsummary(Cholesterol_Weight)\n\n\nCall:\nlm(formula = Cholesterol ~ Weight, data = synthetic_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.070  -5.595   1.213   7.534  24.518 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 107.44682    5.54422   19.38   &lt;2e-16 ***\nWeight        0.47676    0.03417   13.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.86 on 98 degrees of freedom\nMultiple R-squared:  0.6651,    Adjusted R-squared:  0.6617 \nF-statistic: 194.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe intercept (a) is 100.39517; this means that the average cholesterol level (in mg/dL) is 100.39517 when weight is 0 lbs. The slope (b) is 0.55317; this means that for every 1 unit increase in weight (in lbs), cholesterol level (in mg/dL) increases by 0.55317 mg/dL. The p-value is &lt; 2.2e-16.\nI will now use the lm() function to fit a linear model with cholesterol as the outcome and height as the predictor. Then, I will apply the summary() function.\n\nCholesterol_Height &lt;- lm(Cholesterol ~ Height, data = synthetic_data)\nsummary(Cholesterol_Height)\n\n\nCall:\nlm(formula = Cholesterol ~ Height, data = synthetic_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.326  -7.246  -0.234   8.266  34.098 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.3665    15.0703   3.077  0.00271 ** \nHeight        2.0271     0.2222   9.124 9.62e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.79 on 98 degrees of freedom\nMultiple R-squared:  0.4593,    Adjusted R-squared:  0.4538 \nF-statistic: 83.26 on 1 and 98 DF,  p-value: 9.617e-15\n\n\nThe intercept (a) is 40.8193; this means that the average cholesterol level (in mg/dL) is 40.8193 when height is 0 inches. The slope (b) is 2.1923; this means that for every 1 unit increase in height (in inches), cholesterol level (in mg/dL) increases by 2.1923 mg/dL. The p-value is 4.856e-11.\nAt an alpha (significance level) of 0.05, the p-values indicated above are both significant. Meaning, we can reject the null hypothesis that is no relationship between height and cholesterol as well as between weight and cholesterol. However, p-value for weight and cholesterol (&lt; 2.2e-16) is lower than that for height and cholesterol (4.856e-11). This indicates that while both height and weight are significant predictors of cholesterol level, weight is a stronger predictor of cholesterol level than height."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "Loading and Checking Data",
    "text": "Loading and Checking Data\nI will now install the dslabs package. Then, I will load the package.\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\")) # including this because I could not render the website without it\ninstall.packages(\"dslabs\")\n\n\nThe downloaded binary packages are in\n    /var/folders/n7/2hn_592d3db_b95vx1v5wcjr0000gn/T//Rtmpc9mkia/downloaded_packages\n\nlibrary(dslabs)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nI will now use the “help()” function on the “gapminder” data.\n\nhelp(gapminder)\n\nFrom this, I learned that the gapminder data contains outcomes related to health and income for 184 countries between 1960 and 2016. oecd = OECD countries from 2016 opec = OPEC countries form 2016\nNow, I will use str() on the gapminder data.\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n\nAbove, I can see that the gapminder data frame contains 9 variables and 10,545 observations. The variables are: country, year, infant_mortality, life_expectancy, fertility, population, gdp, continent, and region. Some of the variables are numerics or intergers, while others are factors (country, continent, and region).\nI will use summary() on the gapminder data.\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\nThe summary above displays the countries, continents, and regions. It also shows the minimum, 1st quartile, median, mean, 3rd quartile and maximum values for the numeric/integer variables.\nNow, I will determine the type of object/class that gapminder is.\n\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nThis output states that gapminder is a data frame."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\nNow, I will filter the gapminder data to obtain only the countries in Africa. I will assign this to a variable called “africadata”.\n\nafricadata &lt;- filter(gapminder, continent == \"Africa\")\n\nI will run str() on “africadata”.\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n\nAs you can see, there are still 9 variables (each of the same data class as before). However, there are fewer observations (2907) than before (10,545) - because we are only included the African countries here.\nI will run summary() on “africadata”.\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nThis summary is similar to the one created for the entire gapminder data frame, however the values above are now different for the numeric/integer variables since we are only looking at the African countries.\nNow, I will create a new object that contains only the infant_mortality AND life_expectancy from the “africadata” data frame.\n\ninfantmort_lifeexp &lt;- select(africadata, infant_mortality, life_expectancy)\n\nI will use str() and summary() to take a look at the infantmort_lifeexp data.\n\nstr(infantmort_lifeexp)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(infantmort_lifeexp)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\nThese outputs show that the only two variables are infant_mortality and life_expectancy. Then the minimum, 1st quartile, median, mean, 3rd quartile, and maximum are displayed for infant_mortality and life_expectancy.\nThen, I will create a new object containing only population AND life-expectancy from the “africadata” data frame.\n\npop_lifeexp &lt;- select(africadata, population, life_expectancy)\n\nI will use str() and summary() to take a look at the pop_lifeexp data.\n\nstr(pop_lifeexp)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(pop_lifeexp)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\nThese outputs show that the only two variables are population and life_expectancy. Then the minimum, 1st quartile, median, mean, 3rd quartile, and maximum are displayed for population and life_expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting",
    "href": "coding-exercise/coding-exercise.html#plotting",
    "title": "R Coding Exercise",
    "section": "Plotting",
    "text": "Plotting\nI will now create a plot of life_expectancy as a function of infant_mortality.\n\nggplot(infantmort_lifeexp, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color = \"#5fafdc\") + # I will specify the points to be blue \n  labs(title = \"Life Expectancy as a Function of Infant \\n Mortality in African Countries\",\n       x = \"Infant Mortality (per 1,000)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold \n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs you can see from the plot above, there appears to be a negative relationship between infant mortality and life expectancy. This makes sense, because if there is higher infant mortality in a country, their overall life expectancy should be lower.\nI will now create a plot of life_expectancy as a function of population (size). The population (x-axis) will be set to a log scale.\n\nggplot(pop_lifeexp, aes(x = population, y = life_expectancy)) +\n  geom_point(color = \"#70dc5f\") + # I will specify the points to be green  scale_x_log10() + \n  scale_x_log10() + # I will set the x-axis to a log scale\n  labs(title = \"Life Expectancy as a Function of Population Size \\n (Log Scale) in African Countries\",\n       x = \"Population (Log Scale)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot above appears to have individual streaks which have a positive relationship between population size and life expectancy. However, as a whole the data appears to be relatively scattered.\nThere appear to be “streaks” of data in both plots above. This is because we are measuring individual countries over several different years."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "More Data Processing",
    "text": "More Data Processing\nI will find out which years contain NAs.\n\nyears_with_na_infant_mortality &lt;- unique(africadata$year[is.na(africadata$infant_mortality)])\n# use unique() and is.na() to find out which years contain NAs in infant_mortality\n\nprint(years_with_na_infant_mortality)\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 2016\n\n# Use print() to view the output\n\nAs you can see, the years 1960-1981 and 2016 contain NA values within the infant_mortality variable.\nWe will use the year 2000’s infant mortality data since it contains no missing values. I will create a new object that contains only data for the year 2000 from the “africadata” data frame.\n\nafricadata_2000 &lt;- filter(africadata, year == 2000)\n\nI will check that this worked with str() and summary().\n\nstr(africadata_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\nThis worked, as you can see all the years appear as “2000”."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More Plotting",
    "text": "More Plotting\nI will make the same plots as before, but this time with the “africadata_2000” data.\nI will create a plot of life_expectancy as a function of infant_mortality.\n\nggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color = \"#f576c1\") + # I will specify the points to be pink \n  labs(title = \"Life Expectancy as a Function of Infant Mortality \\n in African Countries in Year 2000\",\n       x = \"Infant Mortality (per 1,000)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold \n\n\n\n\n\n\n\n\nYou can see that the plot above has fewer data points that the previous plot because only data from the year 2000 for each country is included.\nI will now create a plot of life_expectancy as a function of population (size). The population (x-axis) will be set to a log scale.\n\nggplot(africadata_2000, aes(x = population, y = life_expectancy)) +\n  geom_point(color = \"#f7af32\") + # I will specify the points to be orange  \n  scale_x_log10() + # I will set the x-axis to a log scale\n  labs(title = \"Life Expectancy as a Function of Population Size \\n (Log Scale) in African Countries in Year 2000\",\n       x = \"Population (Log Scale)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold\n\n\n\n\n\n\n\n\nThe plot above also has fewer data points than the previous plot because it contain contains a data point from the year 2000 for each country."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Simple Model Fits",
    "text": "Simple Model Fits\nI will now use the lm() function to fit a linear model with life_expectancy as the outcome and infant_mortality as the predictor. Then, I will apply the summary() function.\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_2000)\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nThe intercept (a) is 71.29331; this means that the average life expectancy is 71.29331 years when infant mortality is 0. The slope (b) is -0.18916; this means that for every 1 unit increase in infant mortality, life expectancy decreases by 0.18916 years.\nI will now use the lm() function to fit a linear model with life_expectancy as the outcome and population size as the predictor. Then, I will apply the summary() function.\n\nfit2 &lt;- lm(life_expectancy ~ population, data = africadata_2000)\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nThe intercept (a) is 5.593e+01; this means that the average life expectancy is 55.93 years when the population size is equal to 0. The slope (b) is 2.756e-08; this means that for every 1 unit increase in population size, life expectancy increases by 2.756e-08 years. This intrepretation does not make sense - because if the population size is 0, the average life expectancy should be non-existent.\nThe p-value for “fit1” is 2.826e-08, while the p-value for “fit2” is 0.6159. 2.825e-08 is less than 0.05, therefore, the null hypothesis that there is no relationship between infant mortality and life expectancy can be rejected. 0.6159 is greater than 0.05, the null hypothesis that there is no relationship between population size and life expectancy cannot be rejected (we fail to reject). With this information, it can be said that infant mortality is a better predictor of life expectancy than population size. Therefore, “fit1” should be used."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-2",
    "href": "coding-exercise/coding-exercise.html#part-2",
    "title": "R Coding Exercise",
    "section": "Part 2",
    "text": "Part 2\nThis section is contributed by Yufei Wu."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#dataset-exploration",
    "href": "coding-exercise/coding-exercise.html#dataset-exploration",
    "title": "R Coding Exercise",
    "section": "Dataset exploration",
    "text": "Dataset exploration\nI choose the dataset “mice_weights” which contains body weights, bone density, and percent fat for mice under two diets: chow and high fat.\n\n# load package\nlibrary(dslabs)\n#look at help file for mice_weights data\nhelp(mice_weights)\n#get an overview of data structure\nstr(mice_weights)\n\n'data.frame':   780 obs. of  7 variables:\n $ body_weight : num  27.6 23 28.7 32.6 28.6 ...\n $ bone_density: num  0.616 0.769 0.684 0.644 0.53 ...\n $ percent_fat : num  7.26 4.95 6.02 9.54 6.99 ...\n $ sex         : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ diet        : Factor w/ 2 levels \"chow\",\"hf\": 1 1 1 1 1 1 1 1 1 1 ...\n $ gen         : Factor w/ 5 levels \"4\",\"7\",\"8\",\"9\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ litter      : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n\n#get a summary of data\nsummary(mice_weights)\n\n  body_weight     bone_density     percent_fat     sex       diet     gen     \n Min.   :18.13   Min.   :0.2708   Min.   : 2.552   F:398   chow:394   4 : 97  \n 1st Qu.:28.09   1st Qu.:0.4888   1st Qu.: 5.566   M:382   hf  :386   7 :195  \n Median :32.98   Median :0.5643   Median : 8.276                      8 :193  \n Mean   :34.08   Mean   :0.5697   Mean   : 8.594                      9 : 97  \n 3rd Qu.:39.37   3rd Qu.:0.6373   3rd Qu.:10.926                      11:198  \n Max.   :65.15   Max.   :0.9980   Max.   :22.154                              \n                 NA's   :4        NA's   :4                                   \n litter \n 1:442  \n 2:338  \n        \n        \n        \n        \n        \n\n#determine the type of object mice_weights is\nclass(mice_weights)\n\n[1] \"data.frame\"\n\n#get the overview of the data\nhead(mice_weights)\n\n  body_weight bone_density percent_fat sex diet gen litter\n1       27.60    0.6163850    7.255468   F chow   4      1\n2       23.03    0.7693496    4.951037   F chow   4      1\n3       28.72    0.6842564    6.020849   F chow   4      1\n4       32.57    0.6436947    9.536251   F chow   4      1\n5       28.61    0.5297713    6.987331   F chow   4      1\n6       28.16    0.5649217    6.767774   F chow   4      1"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#data-processing",
    "href": "coding-exercise/coding-exercise.html#data-processing",
    "title": "R Coding Exercise",
    "section": "Data processing",
    "text": "Data processing\nCreat a new data frame containing only famale rats.\n\n#assign only the female rat to a new object called femalerat\nfemalerat &lt;- mice_weights[mice_weights$sex == \"F\", ]\n\n#check the structure\nstr(femalerat)\n\n'data.frame':   398 obs. of  7 variables:\n $ body_weight : num  27.6 23 28.7 32.6 28.6 ...\n $ bone_density: num  0.616 0.769 0.684 0.644 0.53 ...\n $ percent_fat : num  7.26 4.95 6.02 9.54 6.99 ...\n $ sex         : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ diet        : Factor w/ 2 levels \"chow\",\"hf\": 1 1 1 1 1 1 1 1 1 1 ...\n $ gen         : Factor w/ 5 levels \"4\",\"7\",\"8\",\"9\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ litter      : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n\n#check the summary\nsummary(femalerat)\n\n  body_weight     bone_density     percent_fat     sex       diet     gen     \n Min.   :18.13   Min.   :0.2708   Min.   : 2.766   F:398   chow:200   4 : 50  \n 1st Qu.:25.35   1st Qu.:0.4531   1st Qu.: 5.690   M:  0   hf  :198   7 : 98  \n Median :28.71   Median :0.5090   Median : 8.181                      8 :100  \n Mean   :29.76   Mean   :0.5186   Mean   : 8.683                      9 : 50  \n 3rd Qu.:33.21   3rd Qu.:0.5793   3rd Qu.:11.017                      11:100  \n Max.   :65.15   Max.   :0.8519   Max.   :22.154                              \n                 NA's   :1        NA's   :1                                   \n litter \n 1:224  \n 2:174  \n        \n        \n        \n        \n        \n\n#get the overview of the data\nhead(femalerat)\n\n  body_weight bone_density percent_fat sex diet gen litter\n1       27.60    0.6163850    7.255468   F chow   4      1\n2       23.03    0.7693496    4.951037   F chow   4      1\n3       28.72    0.6842564    6.020849   F chow   4      1\n4       32.57    0.6436947    9.536251   F chow   4      1\n5       28.61    0.5297713    6.987331   F chow   4      1\n6       28.16    0.5649217    6.767774   F chow   4      1"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#figures-and-tables",
    "href": "coding-exercise/coding-exercise.html#figures-and-tables",
    "title": "R Coding Exercise",
    "section": "Figures and tables",
    "text": "Figures and tables\n\n#load package\nlibrary(ggplot2)\nlibrary(dplyr)\n\n#make plots of body weight vs body density, coloring by diet\nggplot(femalerat, aes(x = body_weight, y = bone_density, color = diet)) +\n  geom_point() +\n  labs(\n    title = \"Relationship between body density and body weight\",\n    x = \"Body weight\",\n    y = \"Body density\",\n    color = \"Diet\")\n\n\n\n\n\n\n\n#make plots of body weight vs percent fat, coloring by diet\nggplot(femalerat, aes(x = body_weight, y = percent_fat, color = diet)) +\n  geom_point() +\n  labs(\n    title = \"Relationship between percent fat and body weight\",\n    x = \"Body weight\",\n    y = \"Percent fat\",\n    color = \"Diet\")\n\n\n\n\n\n\n\n#summarize the mean and standard deviation of body weight grouped by diet\nsummary_table &lt;- femalerat %&gt;%\n  group_by(diet) %&gt;%\n  summarise(\n    mean_weight = mean(body_weight, na.rm = TRUE),\n    sd_weight = sd(body_weight, na.rm = TRUE),\n    mean_percent_fat = mean(percent_fat, na.rm = TRUE),\n    sd_percent_fat = sd(percent_fat, na.rm = TRUE),\n    count = n()\n  )\nprint(summary_table)\n\n# A tibble: 2 × 6\n  diet  mean_weight sd_weight mean_percent_fat sd_percent_fat count\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n1 chow         27.8      4.82             6.86           2.61   200\n2 hf           31.7      7.06            10.5            3.75   198\n\n\nFrom the first figure, we cannot see a clear relationship between body density and body weight. From the second figure, we can see that there is a positive relationship between percent fat and body weight. Besides, there are more points from high fat group locating in higher percent fat. From the table, we can see that the body weight and percent fat are both higher in the high fat group than chow group, which can be further explored."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#statistical-model",
    "href": "coding-exercise/coding-exercise.html#statistical-model",
    "title": "R Coding Exercise",
    "section": "Statistical model",
    "text": "Statistical model\nSince I noticed that body weight and percent fat are both higher in the high fat group than chow group, I will test if the differences are statistically significant.\n\n#run t-test for body weight comparison\nt_test_weight &lt;- t.test(body_weight ~ diet, data = femalerat)\nprint(t_test_weight)\n\n\n    Welch Two Sample t-test\n\ndata:  body_weight by diet\nt = -6.3936, df = 347.4, p-value = 5.224e-10\nalternative hypothesis: true difference in means between group chow and group hf is not equal to 0\n95 percent confidence interval:\n -5.073552 -2.686420\nsample estimates:\nmean in group chow   mean in group hf \n          27.82860           31.70859 \n\n#run t-test for percent fat comparison\nt_test_fat &lt;- t.test(percent_fat ~ diet, data = femalerat)\nprint(t_test_fat)\n\n\n    Welch Two Sample t-test\n\ndata:  percent_fat by diet\nt = -11.316, df = 349.56, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group chow and group hf is not equal to 0\n95 percent confidence interval:\n -4.310618 -3.034073\nsample estimates:\nmean in group chow   mean in group hf \n          6.860235          10.532581"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#report-the-results",
    "href": "coding-exercise/coding-exercise.html#report-the-results",
    "title": "R Coding Exercise",
    "section": "Report the results",
    "text": "Report the results\nFor the body weight comparison, the difference between high fat group and chow group is 3.88, p value is less than 0.05, so the mean body weight differs significantly between the high Fat and chow groups. For the percent fat comparison, the difference between high fat group and chow group is 3.67, p value is less than 0.05, so the mean percent fat differs significantly between the high Fat and chow groups."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Hi, everyone! :)\n\nNatalie Cann | MADA Spring 2025\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting Exercise",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(readr)\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(naniar)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(yardstick)\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ caret::precision()       masks yardstick::precision()\n✖ caret::recall()          masks yardstick::recall()\n✖ caret::sensitivity()     masks yardstick::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ caret::specificity()     masks yardstick::specificity()\n✖ recipes::step()          masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(workflows)\nlibrary(parsnip)\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\")) # including this because I could not render the website without it"
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#modeling-fitting",
    "href": "fitting-exercise/fitting-exercise.html#modeling-fitting",
    "title": "Fitting Exercise",
    "section": "Modeling Fitting",
    "text": "Modeling Fitting\nSetting seed specified above:\n\nset.seed(rngseed)\n\nI will now fit two linear models to Y (the continuous outcome of interest). I will use only the training data for this.\nFirst model: Y ~ DOSE (only DOSE as predictor)\n\nmodel_dose_10 &lt;- lm(Y ~ DOSE, data = train_data)\nsummary(model_dose_10)\n\n\nCall:\nlm(formula = Y ~ DOSE, data = train_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1257.85  -492.00   -71.31   370.71  2399.93 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  535.455    244.017   2.194   0.0308 *  \nDOSE          53.424      6.286   8.499 4.41e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 710.7 on 88 degrees of freedom\nMultiple R-squared:  0.4508,    Adjusted R-squared:  0.4446 \nF-statistic: 72.23 on 1 and 88 DF,  p-value: 4.406e-13\n\n\nThe intercept is 535.455. This means that when DOSE is 0, the estimated value of Y is 535.455. (P = 0.0308) –&gt; statistically significant\nThe slope for DOSE is 53.424. This means that for every 1-unit increase in DOSE, Y is expected to increase by 53.424 units. The p-value is less than 4.41e-13, which indicates that DOSE is a significant predictor of Y.\nRMSE = 710.7\nSecond model: Y ~ DOSE + AGE + WT + HT (all predictors)\n\nmodel_all_10 &lt;- lm(Y ~ DOSE + AGE + WT + HT, data = train_data)\nsummary(model_all_10)\n\n\nCall:\nlm(formula = Y ~ DOSE + AGE + WT + HT, data = train_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1423.3  -390.9  -190.4   407.1  2412.1 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 2192.480   1899.054   1.155  0.25153    \nDOSE          56.493      5.903   9.570 3.79e-15 ***\nAGE           -5.621      9.296  -0.605  0.54698    \nWT           -23.204      7.777  -2.984  0.00372 ** \nHT           185.838   1207.909   0.154  0.87809    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 660.7 on 85 degrees of freedom\nMultiple R-squared:  0.5416,    Adjusted R-squared:   0.52 \nF-statistic:  25.1 on 4 and 85 DF,  p-value: 9.657e-14\n\n\nThe intercept is 2192.480. This means that when DOSE, AGE, WT, and HT are 0, the estimated value of Y is 2192.480 (P = 0.25153) –&gt; not statistically significant\nThe slope for DOSE is 56.493. This means that for every 1-unit increase in DOSE, Y is expected to increase by 56.493 units. The p-value is 3.79e-15, which indicates that DOSE is a significant predictor of Y.\nThe slope for AGE is -5.621. This means that for every 1-unit increase in AGE, Y is expected to decrease by 5.621 units. The p-value is 0.54698, which indicates that AGE is not significant predictor of Y.\n\nThe slope for WT is -23.204. This means that for every 1-unit increase in WT, Y is expected to decrease by 23.204 units. The p-value is 0.000372, which indicates that WT is a significant predictor of Y.\nThe slope for HT is 185.838. This means that for every 1-unit increase in HT, Y is expected to increase by 185.838 units. The p-value is 0.87809, which indicates that HT is not a significant predictor of Y.\nRMSE = 660.7\nFrom the RMSEs above, we can see that the second model’s is smaller (660.7) than the first model’s (710.7). This suggests that the second model’s predictions deviate less from the actual values on average – indicating that the second model may be better."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-performance-assessment-1",
    "href": "fitting-exercise/fitting-exercise.html#model-performance-assessment-1",
    "title": "Fitting Exercise",
    "section": "Model Performance Assessment 1",
    "text": "Model Performance Assessment 1\nSetting seed specified above:\n\nset.seed(rngseed)\n\nNow, as per class instructions, I will run predictions for both models.\n\nset.seed(rngseed)\n\n# DOSE model\nlm_dose &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nlm_dose_workflow &lt;- workflow() %&gt;% \n  add_model(lm_dose) %&gt;% \n  add_formula(Y ~ DOSE)\n\n# ALL model\nlm_all &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\nlm_all_workflow &lt;- workflow() %&gt;% \n  add_model(lm_all) %&gt;% \n  add_formula(Y ~ .) # period includes all predictors\n\n# fit models to train data\nlm_dose_fit &lt;- lm_dose_workflow %&gt;% fit(train_data)\nlm_all_fit &lt;- lm_all_workflow %&gt;% fit(train_data)\n\n# predictions with train data\npreds_dose &lt;- predict(lm_dose_fit, train_data) %&gt;% bind_cols(train_data)\npreds_all &lt;- predict(lm_all_fit, train_data) %&gt;% bind_cols(train_data)\n\n# RMSE for both models\nrmse_dose &lt;- rmse(preds_dose, truth = Y, estimate = .pred)\nrmse_dose\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        703.\n\nrmse_all &lt;- rmse(preds_all, truth = Y, estimate = .pred)\nrmse_all\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        627.\n\n\nThe mean RMSE value for the prediction model containing simply DOSE is 702.8078. The mean RMSE value for the prediction model containing all predictors (DOSE, AGE, WT, HT) is 627.4408.\nNext, I will compute RMSE for a null model (with no predictors).\n\n# compute the mean of Y (null model prediction)\nmean_Y &lt;- mean(train_data$Y)\n\n# null model predictions (just the mean for all observations)\npred_null &lt;- rep(mean_Y, length(train_data$Y))\n\n# RMSE\nrmse_null &lt;- sqrt(mean((train_data$Y - pred_null)^2))\nrmse_null\n\n[1] 948.3526\n\n\nThe null model mean RMSE is 948.3526.\nThe ALL predictors prediction model had a mean RMSE of 627.4408, which is the lowest RMSE value. This indicates that it performs the best out of the three models. The DOSE predictor prediction model had an mean RMSE of 702.8078. The null predictor prediction model had a mean RMSE of 948.3526, which suggests it performs the worst."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#model-performance-assessment-2",
    "href": "fitting-exercise/fitting-exercise.html#model-performance-assessment-2",
    "title": "Fitting Exercise",
    "section": "Model Performance Assessment 2",
    "text": "Model Performance Assessment 2\nI will perform a 10-fold cross-validation using https://www.tidymodels.org/start/resampling/ as a reference.\n\nset.seed(rngseed) # set seed for reproducibility \n\ncv_folds &lt;- vfold_cv(train_data, v = 10) # specify 10 folds for cross-validation\n\nBelow, I will set-up the modeling process for the DOSE model.\n\n# set up linear regression model for the DOSE predictor model\nlm_dose &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow for the DOSE predictor model\ndose_workflow &lt;- \n  workflow() %&gt;%\n  add_model(lm_dose) %&gt;%\n  add_formula(Y ~ DOSE)\n\nset.seed(rngseed) # set seed for reproducibility\n\ndose_fit_rs &lt;- \n  dose_workflow %&gt;% \n  fit_resamples(cv_folds)\n\ndose_metrics &lt;- collect_metrics(dose_fit_rs) # mean RMSE of 10 folds\ndose_metrics\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   691.       10 67.5    Preprocessor1_Model1\n2 rsq     standard     0.512    10  0.0592 Preprocessor1_Model1\n\n\nThe mean RMSE is 690.5397 and the rsq is 0.5117.\nNow, I will do so for the ALL predictors model.\n\n# set up linear regression model for the ALL predictors model\nlm_all &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow for the ALL predictors model\nall_workflow &lt;- \n  workflow() %&gt;%\n  add_model(lm_all) %&gt;%\n  add_formula(Y ~ DOSE + AGE + WT + HT)\n\nset.seed(rngseed) # set seed for reproducibility\n\nall_fit_rs &lt;- \n  all_workflow %&gt;% \n  fit_resamples(cv_folds)\n\nall_metrics &lt;- collect_metrics(all_fit_rs) # mean RMSE of 10 folds\nall_metrics\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   663.       10 58.2    Preprocessor1_Model1\n2 rsq     standard     0.578    10  0.0769 Preprocessor1_Model1\n\n\nThe mean RMSE is 663.0291 and the rsq is 0.5775.\nWe see that the ALL predictors model has the lower mean RMSE (663.0291) and the higher r-squared value (0.5775). This suggests that this model is better at predicting Y. The DOSE model has a higher mean RMSE (690.5397) and a lower r-squared value (0.5117).\nNow, I will do this for the null model.\n\n# null model within cross-validation\nnull_model_rmse &lt;- function(split, data) {\n  # Get the training data for this fold\n  train_fold &lt;- training(split)\n  \n  # Compute the mean of Y in the training data for this fold\n  mean_Y_fold &lt;- mean(train_fold$Y)\n  \n  # Create predictions for the fold (just the mean of Y)\n  pred_null_fold &lt;- rep(mean_Y_fold, nrow(training(split)))\n  \n  # Calculate RMSE for this fold\n  rmse_null_fold &lt;- sqrt(mean((train_fold$Y - pred_null_fold)^2))\n  \n  return(rmse_null_fold)\n}\n\n# apply the null model RMSE function to the 10 folds\ncv_null_rmse &lt;- vfold_cv(train_data, v = 10) %&gt;%\n  mutate(rmse = map_dbl(splits, null_model_rmse, data = train_data))\n\n# mean RMSE for the null model across folds\nmean(cv_null_rmse$rmse)\n\n[1] 947.302\n\n# standard error for RMSE for null model\nnull_rmse_se &lt;- sd(cv_null_rmse$rmse) / sqrt(length(cv_null_rmse$rmse))\nnull_rmse_se\n\n[1] 8.092121\n\n\nThe mean RMSE for the ALL predictors model for assessment 1 was 627.4408, while it 663.0291 for assessment 2. This means that the mean RMSE for the ALL predictors model increased by 663.0291-627.4408=35.5883 units (or poitns). The mean RMSE for the DOSE predictor model for assessment 1 was 702.8078, while it was 690.5397 for assessment 2. The means that the mean RMSE for the DOSE predictor model decreased by 702.8078-690.5397=12.2681 units (or points).The mean RMSE for the null model was 948.3526 for assessment 1 and was 947.302 for assessment 2. Therefore, there was not a significant change here.\nThe mean standard error of RMSE for the DOSE model is 67.49509. The mean standard error of RMSE for the ALL predictors model is 58.22321. The mean standard error of RMSE for the null model is 7.085889.\nNow, I will run this code again however, I will choose a different value for the random seed.\n\n# different random seed for reproducibility\nset.seed(54321)  # Use a different seed value for this run\n\ncv_folds_new &lt;- vfold_cv(train_data, v = 10) # specify 10 folds for cross-validation\n\n# DOSE\nset.seed(54321)\n# set up linear regression model for the DOSE predictor model\nlm_dose_new &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow for the DOSE predictor model\ndose_workflow_new &lt;- \n  workflow() %&gt;%\n  add_model(lm_dose_new) %&gt;%\n  add_formula(Y ~ DOSE)\n\nset.seed(54321)\n\ndose_fit_rs_new &lt;- \n  dose_workflow_new %&gt;% \n  fit_resamples(cv_folds_new)\n\ndose_metrics_new &lt;- collect_metrics(dose_fit_rs_new) # mean RMSE of 10 folds\ndose_metrics_new\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   696.       10 57.8    Preprocessor1_Model1\n2 rsq     standard     0.459    10  0.0588 Preprocessor1_Model1\n\n# ALL\nset.seed(54321)\n# set up linear regression model for the ALL predictors model\nlm_all_new &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n# set up workflow for the ALL predictors model\nall_workflow_new &lt;- \n  workflow() %&gt;%\n  add_model(lm_all_new) %&gt;%\n  add_formula(Y ~ DOSE + AGE + WT + HT)\n\nset.seed(54321)\n\nall_fit_rs_new &lt;- \n  all_workflow_new %&gt;% \n  fit_resamples(cv_folds_new)\n\nall_metrics_new &lt;- collect_metrics(all_fit_rs_new) # mean RMSE of 10 folds\nall_metrics_new\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   650.       10 57.0    Preprocessor1_Model1\n2 rsq     standard     0.542    10  0.0605 Preprocessor1_Model1\n\n# null\nset.seed(54321)\n# null model within cross-validation\nnull_model_rmse_new &lt;- function(split, data) {\n  # Get the training data for this fold\n  train_fold &lt;- training(split)\n  \n  # Compute the mean of Y in the training data for this fold\n  mean_Y_fold &lt;- mean(train_fold$Y)\n  \n  # Create predictions for the fold (just the mean of Y)\n  pred_null_fold &lt;- rep(mean_Y_fold, nrow(training(split)))\n  \n  # Calculate RMSE for this fold\n  rmse_null_fold &lt;- sqrt(mean((train_fold$Y - pred_null_fold)^2))\n  \n  return(rmse_null_fold)\n}\n\n# apply the null model RMSE function to the 10 folds\ncv_null_rmse_new &lt;- vfold_cv(train_data, v = 10) %&gt;%\n  mutate(rmse = map_dbl(splits, null_model_rmse_new, data = train_data))\n\n# mean RMSE for the null model across folds\nmean_RMSE_null &lt;-mean(cv_null_rmse_new$rmse)\nmean_RMSE_null\n\n[1] 947.0423\n\n# standard error for RMSE for null model\nnull_rmse_se_new &lt;- sd(cv_null_rmse_new$rmse) / sqrt(length(cv_null_rmse$rmse))\nnull_rmse_se_new\n\n[1] 8.094641\n\n\nAfter changing the seed to 54321, the mean RMSE for the DOSE model is 695.9589, and its R-squared value is 0.4589. The mean SE(RMSE) is 57.77. The mean RMSE for the ALL predictors model is 650.2721, and its R-squared value is 0.5417. The mean SE(RMSE) is 57.05. The mean RMSE for the null model is 947.0423, while the mean SE(RMSE) is 8.094641."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#conclusions-on-overall-model-assessment",
    "href": "fitting-exercise/fitting-exercise.html#conclusions-on-overall-model-assessment",
    "title": "Fitting Exercise",
    "section": "Conclusions on Overall Model Assessment:",
    "text": "Conclusions on Overall Model Assessment:\nModel 1 = Only DOSE as a Predictor of Y (Y ~ DOSE) Model 2 = ALL predictors of Y (Y ~ .)\n\nBoth of our models, perform better than the null model. Model 1 (~690) and 2 (~627) have lower RMSE’s as compared to the null model (~947) (Note: RMSE’s taken from assessment 2).\nModel 1 (only DOSE as a predictor) does improve results compared to the null model. As described above, model 1’s RMSE from assessment 2 was 690, as compared to ~947 from the null model. However, I do not believe it to be “usable” for any real purpose, as it is too simple. We see that model 2 (with all predictors) is much better, and basically renders model 1 to be obsolete.\nModel 2 (which includes all of the predictors) appears to be the best model and improves results as compared to both the null model AND model 1. Out of all three models, model 2 had the lowest RMSE value of ~627. It appears that from the scatterplot above, the test data points are relatively close to the 45-degree line, indicating a good fit and generalization to new data for model 2. I would say that since model 2 accounts for more than simply just dose, this would be a much more usable model."
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html",
    "href": "ml-models-exercise/ml-models-exercise.html",
    "title": "Machine Learning Models Exercise",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(readr)\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(naniar)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(yardstick)\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ caret::precision()       masks yardstick::precision()\n✖ caret::recall()          masks yardstick::recall()\n✖ caret::sensitivity()     masks yardstick::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ caret::specificity()     masks yardstick::specificity()\n✖ recipes::step()          masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(ranger)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8"
  },
  {
    "objectID": "ml-models-exercise/ml-models-exercise.html#first-fit",
    "href": "ml-models-exercise/ml-models-exercise.html#first-fit",
    "title": "Machine Learning Models Exercise",
    "section": "First Fit",
    "text": "First Fit\nFirst, I will run a linear model with all of the predictors of Y in the dataset.\n\n# define the linear model\nlm_all &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\")\n\nlm_recipe &lt;- recipe(Y ~ ., data = data)\n\n# create a workflow\nlm_workflow &lt;- workflow() %&gt;%\n  add_model(lm_all) %&gt;%\n  add_recipe(lm_recipe)\n\n# fit the model\nlm_fit &lt;- lm_workflow %&gt;%\n  fit(data = data)\n\n# predict on the full dataset\nlm_predictions &lt;- predict(lm_fit, data) %&gt;%\n  bind_cols(data)\n\n# RMSE\nlm_rmse &lt;- lm_predictions %&gt;%\n  rmse(truth = Y, estimate = .pred)\nlm_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        581.\n\n# plot observed vs predicted\nggplot(lm_predictions, aes(x = Y, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"lightgreen\") +\n  geom_abline(slope = 1, intercept = 0, color = \"gray\") +\n  labs(title = \"Linear Model: Observed vs Predicted\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\nNote that the RMSE from the linear fit is 581.4177.\nNext, I will use the glmnet engine for the LASSO model. Per class instructions, the penalty will be set to 0.1.\n\n# define the LASSO model\nlasso_model &lt;- linear_reg(penalty = 0.1, mixture = 1) %&gt;%\n  set_engine(\"glmnet\")\n\nlasso_recipe &lt;- recipe(Y ~ ., data = data) %&gt;%\n  step_dummy(all_nominal_predictors())\n\n# create a workflow\nlasso_workflow &lt;- workflow() %&gt;%\n  add_model(lasso_model) %&gt;%\n  add_recipe(lasso_recipe)\n\n# fit the model\nlasso_fit &lt;- lasso_workflow %&gt;%\n  fit(data = data)\n\n# predict on the full dataset\nlasso_predictions &lt;- predict(lasso_fit, data) %&gt;%\n  bind_cols(data)\n\n# RMSE\nlasso_rmse &lt;- lasso_predictions %&gt;%\n  rmse(truth = Y, estimate = .pred)\nlasso_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        581.\n\n# plot observed vs predicted\nggplot(lasso_predictions, aes(x = Y, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"steelblue1\") +\n  geom_abline(slope = 1, intercept = 0, color = \"gray\") +\n  labs(title = \"LASSO: Observed vs Predicted\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\nThe RMSE for the LASSO model is 581.4708.This is very similar to the RMSE from the linear model. It can also be noted that the scatterplots look similar.\nI will use the ranger engine for the random forest model.\n\n# define the RF model (set seed in engine for reproducibility)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", seed = 1234) %&gt;%\n  set_mode(\"regression\")\n\nrf_recipe &lt;- recipe(Y ~ ., data = data)\n\n# create a workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_model) %&gt;%\n  add_recipe(rf_recipe)\n\n# fit the model\nrf_fit &lt;- rf_workflow %&gt;%\n  fit(data = data)\n\n# predict on the full dataset\nrf_predictions &lt;- predict(rf_fit, data) %&gt;%\n  bind_cols(data)\n\n# RMSE\nrf_rmse &lt;- rf_predictions %&gt;%\n  rmse(truth = Y, estimate = .pred)\nrf_rmse\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        359.\n\n# plot observed vs predicted\nggplot(rf_predictions, aes(x = Y, y = .pred)) +\n  geom_point(alpha = 0.6, color = \"gold\") +\n  geom_abline(slope = 1, intercept = 0, color = \"gray\") +\n  labs(title = \"Random Forest: Observed vs Predicted\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\nThe RMSE for the random forest model is 358.824, which is smaller than both the LASSO and linear fit models. A smaller RMSE is typically better. The points on the scattplot appear to follow the diagonal line best in this model as well. Random forest models are typically very flexible and capture lots of patterns. However, a downside is that they tend to be prone to overfitting.\nAfter searching a bit about the LASSO and linear fit models, it looks like the two models perform similarly here due a lack of strong correlation within the data (as seen in the pairwise correlation plot)."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. This data is example data created for the MADA course. The variables are: height, weight, gender, age, and hair color. The two variables I added are: age and hair color. Age is any number greater than 0 (years). Hair color fits into one of several categories: black, blonde, brunette, red, or other."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-boxplot",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-boxplot",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.1 Muhammad’s Boxplot",
    "text": "7.1 Muhammad’s Boxplot\n\n\n\n\n\nBoxplot created for Exercise 2.\n\n\n\n\nThe boxplot above shows the median height among the different genders within the dataset. As you can see, out of the three categories above, the median height of females within the dataset is the highest. The female category also contains the greatest amount of variation. The median heights of males and those selecting “other” are relatively similar. The male category contains the least amount of variation; it is also appears to contain an outlier.\nNote: The instructions specify for the boxplot to be between height and hair color. I will analyze the boxplot given above."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-scatterplot",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-scatterplot",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.2 Muhammad’s Scatterplot",
    "text": "7.2 Muhammad’s Scatterplot\n\n\n\n\n\nScatterplot created for Exercise 2.\n\n\n\n\nThe scatterplot above depicts the relationship between weight and height. There appears to be little to no correlation between the two variables as the points are relatively scattered throughout the plot.\nNote: The instructions specify for the scatterplot to be between weight and age. I will analyze the scatterplot given above."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-qq-plot",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-qq-plot",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.3 Muhammad’s QQ-Plot",
    "text": "7.3 Muhammad’s QQ-Plot\n\n\n\n\n\nQQ-Plot created for Exercise 2.\n\n\n\n\nMuhammad created a QQ-Plot to analyze the data. The red line in this QQ-Plot represents a normal distribution. As you can see the points follow relatively close to this red line, indicating that the data is normally distributed. There appear to be slight deviations from the red line, however these do not appear to be significant. Therefore, any skewness is minimal."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#natalies-table-3",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#natalies-table-3",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.4 Natalie’s Table 3",
    "text": "7.4 Natalie’s Table 3\n\n\n\nLinear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n159.2356979\n27.543979\n5.7811436\n0.0102925\n\n\nHair_ColorBLO\n11.4477498\n31.161763\n0.3673653\n0.7377324\n\n\nHair_ColorBRU\n24.0175439\n29.399389\n0.8169402\n0.4738031\n\n\nHair_ColorO\n8.8718535\n37.668646\n0.2355236\n0.8289647\n\n\nHair_ColorRED\n29.6544622\n28.516514\n1.0399049\n0.3748287\n\n\nAge\n-0.3272311\n1.177337\n-0.2779418\n0.7991117\n\n\n\n\n\nTable 3 displays a summary of a linear model modeling Hair Color and Age as predictors of height. Neither characteristic was significantly associated with height as the p-values are all very high."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\") # To set up location using here() function \nrawdata &lt;- readxl::read_excel(data_location) # open dataset\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                         `Allowed Values`\n  &lt;chr&gt;           &lt;chr&gt;                                         &lt;chr&gt;           \n1 Height          height in centimeters                         numeric value &gt;…\n2 Weight          weight in kilograms                           numeric value &gt;…\n3 Gender          identified gender (male/female/other)         M/F/O/NA        \n4 Age             age in years                                  numeric value ≥…\n5 Hair Color      color of hair (black/blonde/brunette/red/oth… BLA/BLO/BRU/RED…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height       &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"…\n$ Weight       &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender       &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"…\n$ Age          &lt;dbl&gt; 29, 24, 43, 27, 52, 18, 37, 41, 20, 35, 62, 31, 43, 16\n$ `Hair Color` &lt;chr&gt; \"BLO\", \"BRU\", \"BLA\", \"BRU\", \"BLA\", \"RED\", \"O\", \"BLO\", \"BL…\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :16.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:24.75  \n Mode  :character   Median :  70.0   Mode  :character   Median :33.00  \n                    Mean   : 602.7                      Mean   :34.14  \n                    3rd Qu.:  90.0                      3rd Qu.:42.50  \n                    Max.   :7000.0                      Max.   :62.00  \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M         29 BLO         \n2 175        70 O         24 BRU         \n3 sixty      60 F         43 BLA         \n4 178        76 F         27 BRU         \n5 192        90 NA        52 BLA         \n6 6          55 F         18 RED         \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n34.14\n13.27\n16\n24.75\n33\n42.5\n62\n▆▅▇▂▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% # delete observation with string \"sixty\"\n                  dplyr::mutate(Height = as.numeric(Height)) # convert the Height into numeric \nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n33.46\n13.55\n16\n24.00\n31\n41\n62\n▇▆▇▂▂\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) ) # Convert 6\" into Centimeter \nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n33.46\n13.55\n16\n24.00\n31\n41\n62\n▇▆▇▂▂\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n30.73\n11.42\n16\n22.0\n29\n39\n52\n▇▇▅▅▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n30.73\n11.42\n16\n22.0\n29\n39\n52\n▇▇▅▅▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n29.56\n9.51\n16\n24\n29\n37\n43\n▅▂▇▂▅\n\n\n\n\n\nCreating a new variable for age category\n\nd4 &lt;- d4 %&gt;% \n  mutate(age_cat= ifelse (Age &lt;= 30, 0,\n                  ifelse (Age &gt; 30 & Age &lt;= 40, 1,\n                  ifelse (Age &gt; 40 & Age &lt;= 50, 2,\n                  ifelse (Age &gt; 50, 3, NA_real_)))))\n\nExtra Note from natalie: Hair Color needs to be changed into a factor.\n\nclass(d4$age_cat)\n\n[1] \"numeric\"\n\nclass(d4$`Hair Color`)\n\n[1] \"character\"\n\nd4$`Hair Color` &lt;- as.factor(d4$`Hair Color`)\nclass(d4$`Hair Color`)\n\n[1] \"factor\"\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Loading Packages:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gt)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(readr)\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(naniar)\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\nlibrary(yardstick)\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nlibrary(caret)\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following objects are masked from 'package:yardstick':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ recipes      1.1.0\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\n✖ caret::lift()            masks purrr::lift()\n✖ caret::precision()       masks yardstick::precision()\n✖ caret::recall()          masks yardstick::recall()\n✖ caret::sensitivity()     masks yardstick::sensitivity()\n✖ yardstick::spec()        masks readr::spec()\n✖ caret::specificity()     masks yardstick::specificity()\n✖ recipes::step()          masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(workflows)\nlibrary(parsnip)\nlibrary(ranger)\nlibrary(glmnet)\n\nLoading required package: Matrix\n\nAttaching package: 'Matrix'\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nLoaded glmnet 4.1-8\n\nlibrary(tidytuesdayR)\nlibrary(readxl)\nlibrary(janitor)\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(kknn)\n\n\nAttaching package: 'kknn'\n\nThe following object is masked from 'package:caret':\n\n    contr.dummy\n\n\n\n\nAbout the Data:\nThis exercise will use the tidy tuesday data uploaded April 8th, 2025 (https://github.com/rfordatascience/tidytuesday/tree/main/data/2025/2025-04-08).\nThis data is from the Center for Medicare and Medicaid Services (CMS) and pertains to emergency room wait time variation between states.\n\n\nLoading in the Data:\nThe readme.md file specified several ways by which the data for this week could be brought in. Since I was able to download the data (from the link specified above), I will use the read_csv function from the readr package to load in the data.\n\ndata &lt;- read_csv(here(\"tidytuesday-exercise\", \"care_state.csv\"))\n\nRows: 1232 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): state, condition, measure_id, measure_name, footnote\ndbl  (1): score\ndate (2): start_date, end_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nPreliminary Data Exploration:\nFirst, I will perform a few simple functions to get a basic sense of the data.\n\nhead(data)\n\n# A tibble: 6 × 8\n  state condition   measure_id measure_name score footnote start_date end_date  \n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;    \n1 AK    Healthcare… HCP_COVID… Percentage …   7.3 &lt;NA&gt;     2024-01-01 2024-03-31\n2 AK    Healthcare… IMM_3      Healthcare …  80   &lt;NA&gt;     2023-10-01 2024-03-31\n3 AK    Emergency … OP_18b     Average (me… 140   25, 26   2023-04-01 2024-03-31\n4 AK    Emergency … OP_18b_HI… Average tim… 157   25, 26   2023-04-01 2024-03-31\n5 AK    Emergency … OP_18b_LO… Average tim… 136   25, 26   2023-04-01 2024-03-31\n6 AK    Emergency … OP_18b_ME… Average tim… 136   25, 26   2023-04-01 2024-03-31\n\ndim(data)\n\n[1] 1232    8\n\nsummary(data)\n\n    state            condition          measure_id        measure_name      \n Length:1232        Length:1232        Length:1232        Length:1232       \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n     score       footnote           start_date            end_date         \n Min.   :  1   Length:1232        Min.   :2023-01-01   Min.   :2023-12-31  \n 1st Qu.: 70   Class :character   1st Qu.:2023-04-01   1st Qu.:2024-03-31  \n Median : 93   Mode  :character   Median :2023-04-01   Median :2024-03-31  \n Mean   :134                      Mean   :2023-04-05   Mean   :2024-03-14  \n 3rd Qu.:193                      3rd Qu.:2023-04-01   3rd Qu.:2024-03-31  \n Max.   :730                      Max.   :2024-01-01   Max.   :2024-03-31  \n NA's   :155                                                               \n\ncolnames(data)\n\n[1] \"state\"        \"condition\"    \"measure_id\"   \"measure_name\" \"score\"       \n[6] \"footnote\"     \"start_date\"   \"end_date\"    \n\ngg_miss_var(data)\n\n\n\n\n\n\n\n\nThere are 8 variables and 1232 observations. The variable names are: state, condition, measure_id, measure_name, score, footnote, start_date, and end_date. The variables that are characters are: state, condition, measure_id, measure_name, and footnote. The variables that are numeric are: score, start_date, and end_date. It looks like there are several scores for variables (under the measure_name variable) for each state (under several conditions). It appears as though score and footnote have some missing values.\nI have placed the data dictionary below for ease: |variable |class |description | |:————|:———|:————————————-| |state |character |The two-letter code for the state (or territory, etc) where the hospital is located. | |condition |character |The condition for which the patient was admitted. Six categories of conditions are included in the data. | |measure_id |character |The ID of the thing being measured. Note that there are 22 unique IDs but only 21 unique names. | |measure_name |character |The name of the thing being measured. Note that there are 22 unique IDs but only 21 unique names. | |score |double |The score of the measure. | |footnote |character |Footnotes that apply to this measure: 5 = “Results are not available for this reporting period.”, 25 = “State and national averages include Veterans Health Administration (VHA) hospital data.”, 26 = “State and national averages include Department of Defense (DoD) hospital data.”. | |start_date |date |The date on which measurement began for this measure. | |end_date |date |The date on which measurement ended for this measure. |\n\n\nFormulation of Question and Hypothesis:\nHere are the two questions from within the readme.md script: - Is there a connection between state populations and wait times? - Which conditions have the longest wait times? The shortest?\nI would like to go more specific and assess if there is a connection between state populations and wait times in the Emergency Department.\n\n\nData Cleaning:\nFirst, I will only obtain the data that is relevant to answering my question (measurement_name = Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better AND condition = Emergency Department FOR EACH STATE). I will call this data set er_data.\n\n data %&gt;%\n  filter(condition == \"Emergency Department\") %&gt;%\n  distinct(measure_name) %&gt;%\n  pull(measure_name)\n\n [1] \"Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better\"                                          \n [2] \"Average time patients spent in the emergency department before being sent home A lower number of minutes is better (high)\"                                                   \n [3] \"Average time patients spent in the emergency department before being sent home A lower number of minutes is better (low)\"                                                    \n [4] \"Average time patients spent in the emergency department before being sent home A lower number of minutes is better (moderate)\"                                               \n [5] \"Average (median) time patients spent in the emergency department before leaving from the visit- Psychiatric/Mental Health Patients.  A lower number of minutes is better\"    \n [6] \"Average time patients spent in the emergency department before leaving from the visit - Psychiatric/Mental Health Patients.  A lower number of minutes is better (high)\"     \n [7] \"Average time patients spent in the emergency department before leaving from the visit - Psychiatric/Mental Health Patients.  A lower number of minutes is better (low)\"      \n [8] \"Average time patients spent in the emergency department before leaving from the visit - Psychiatric/Mental Health Patients.  A lower number of minutes is better (moderate)\" \n [9] \"Average time patients spent in the emergency department before leaving from the visit - Psychiatric/Mental Health Patients.  A lower number of minutes is better (very high)\"\n[10] \"Percentage of patients who left the emergency department before being seen Lower percentages are better\"                                                                     \n[11] \"Percentage of patients who came to the emergency department with stroke symptoms who received brain scan results within 45 minutes of arrival Higher percentages are better\" \n\ner_data &lt;- data %&gt;%\n  filter(\n    condition == \"Emergency Department\",\n    measure_name == \"Average (median) time patients spent in the emergency department before leaving from the visit A lower number of minutes is better\"\n  ) %&gt;% # obtain data of interest\n  select(state, condition, measure_id, measure_name, score) %&gt;% # select my columns\n  rename(er_wait_score = score) %&gt;% # rename so that it relfects emergency depts\n  filter(!is.na(er_wait_score)) # drop missing values if there are any\n\nNow, I need to add a column that shows the population size for each state. First, I will read in this states population dataset (from: https://www.britannica.com/topic/largest-U-S-state-by-population).\n\nstates_pop &lt;- read_excel(here(\"tidytuesday-exercise\", \"states.xlsx\"))\n\nNew names:\n• `` -&gt; `...4`\n• `` -&gt; `...5`\n\ncolnames(states_pop)\n\n[1] \"U.S. state\"                       \"population: estimate (2024 est.)\"\n[3] \"population: census (2020)\"        \"...4\"                            \n[5] \"...5\"                            \n\n\nNow, I want to add the state population data to the er_data dataset. I will do this with left join.\n\ner_data_with_pop &lt;- er_data %&gt;%\n  left_join(states_pop, by = c(\"state\" = \"U.S. state\"))\n\nhead(er_data_with_pop)\n\n# A tibble: 6 × 9\n  state condition   measure_id measure_name er_wait_score population: estimate…¹\n  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;      &lt;chr&gt;                &lt;dbl&gt;                  &lt;dbl&gt;\n1 AK    Emergency … OP_18b     Average (me…           140                 740133\n2 AL    Emergency … OP_18b     Average (me…           145                5157699\n3 AR    Emergency … OP_18b     Average (me…           133                3088354\n4 AZ    Emergency … OP_18b     Average (me…           168                7582384\n5 CA    Emergency … OP_18b     Average (me…           184               39431263\n6 CO    Emergency … OP_18b     Average (me…           133                5957493\n# ℹ abbreviated name: ¹​`population: estimate (2024 est.)`\n# ℹ 3 more variables: `population: census (2020)` &lt;dbl&gt;, ...4 &lt;lgl&gt;, ...5 &lt;lgl&gt;\n\n\nFrom viewing the data, I can see that PR and DC weren’t included in the population dataset. So, I will exclude them for this assignment.\n\ner_data_with_pop &lt;- er_data_with_pop %&gt;%\n  filter(state != \"PR\" & state != \"DC\")\n\ndim(er_data_with_pop)\n\n[1] 50  9\n\n\nI can see that this worked as we now only have 50 (instead of 52) observations.\nNow, I will remove the condition, measure_id, and measure_name columns as they are no longer needed since I have confirmed that only the correct data is present. I will also remove the two random columsn R added (…4 and …5).\n\ner_data_with_pop &lt;- er_data_with_pop %&gt;%\n  select(-condition, -measure_id, -measure_name, -...4, -...5)\n\nhead(er_data_with_pop)\n\n# A tibble: 6 × 4\n  state er_wait_score `population: estimate (2024 est.)` population: census (2…¹\n  &lt;chr&gt;         &lt;dbl&gt;                              &lt;dbl&gt;                   &lt;dbl&gt;\n1 AK              140                             740133                  733391\n2 AL              145                            5157699                 5024279\n3 AR              133                            3088354                 3011524\n4 AZ              168                            7582384                 7151502\n5 CA              184                           39431263                39538223\n6 CO              133                            5957493                 5773714\n# ℹ abbreviated name: ¹​`population: census (2020)`\n\n\nThis worked since we have the state, score, 2024 population, and 2020 population.\nLastly, I will rename the population columns to be shorter.\n\ner_data_with_pop &lt;- er_data_with_pop %&gt;%\n  rename(\n    population_2024 = `population: estimate (2024 est.)`,\n    population_2020 = `population: census (2020)`\n  )\n\nNote that I will use the 2024 population since this is more recent.\n\n\nExploratory Data Analysis:\nI will first obtain a summary of the wait time scores and the populations among the states.\n\nsummary(er_data_with_pop$er_wait_score)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  110.0   133.0   153.5   155.4   171.8   251.0 \n\nsummary(er_data_with_pop$population_2024)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  587618  2002580  4593056  6788175  7864231 39431263 \n\n\nThe average median wait time for the ER in US states (exluding DC and PR) is 155.4 minutes. The range is 251-110 = 141 minutes. The average population in US states (excluding DC and PR) is 6,788,175. The range is 39431263-587618 = 38843645. This is a much larger range than the wait time scores.\nNext, I will obtain a histogram for ER wait times in US staets.\n\nggplot(er_data_with_pop, aes(x = er_wait_score)) +\n  geom_histogram(binwidth = 5, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Emergency Room Wait Times\", x = \"Wait Time (minutes)\", y = \"Frequency\") + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\nAs you can tell, it looks like this histogram is slightly skewed to the right.\nNow, I will obtain a histogram for the populations of US states.\n\nggplot(er_data_with_pop, aes(x = population_2024)) +\n  geom_histogram(binwidth = 500000, fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Distribution of State Populations\", x = \"Population\", y = \"Frequency\") + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\nIt looks like this histogram is skewed to the right.\n\ncor(er_data_with_pop$er_wait_score, er_data_with_pop$population_2024, use = \"complete.obs\")\n\n[1] 0.2517527\n\n\nIt looks like there is a weak positive correlation between the ER wait times and the population size of the states (0.2517527).\nI will now create a scatterplot to visualize this correlation.\n\nggplot(er_data_with_pop, aes(x = population_2024, y = er_wait_score)) +\n  geom_point(color = \"lightpink\") +\n  labs(title = \"Scatterplot of ER Wait Times vs. State Population\", x = \"State Population\", y = \"ER Wait Time (minutes)\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray\") + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThere is alot of clustering in the lower left corner of this graph. There may be a few outliers, however, I will keep them in the dataset because I need to account for these states.\n\n\nAnalysis and Models:\nI will now split the data into train and test data.\n\nset.seed(123)\ndata_split &lt;- initial_split(er_data_with_pop, prop = 0.8)\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\nI will now do cross-validation. Folds = 5\n\nset.seed(123)\ncv_folds &lt;- vfold_cv(train_data, v = 5)\n\nNow, I will define the ‘recipes’ for the 3 models (and the basic model recipe with my predictor and outcome). The three models I will use are: linear regression, random forest, and KNN.\n\n# Basic recipe :)\nmodel_recipe &lt;- recipe(er_wait_score ~ population_2024, data = train_data)\n\n# Linear Regression\nlm_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\nlm_wf &lt;- workflow() %&gt;% \n  add_model(lm_spec) %&gt;%\n  add_recipe(model_recipe)\n\n# Random Forest\nrf_spec &lt;- rand_forest(mtry = 1, trees = 500, min_n = 5) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(model_recipe)\n\n# KNN\nknn_spec &lt;- nearest_neighbor(neighbors = tune()) %&gt;%\n  set_engine(\"kknn\") %&gt;%\n  set_mode(\"regression\")\n\nknn_wf &lt;- workflow() %&gt;%\n  add_model(knn_spec) %&gt;%\n  add_recipe(model_recipe)\n\nknn_grid &lt;- tibble(neighbors = seq(1, 20, by = 2))\n\nNow, I will fit all the models and obtain their metrics.\n\n# Linear Regression\nlm_fit &lt;- fit_resamples(\n  lm_wf,\n  resamples = cv_folds,\n  metrics = metric_set(rmse, rsq)\n)\n\ncollect_metrics(lm_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   32.3       5  4.36   Preprocessor1_Model1\n2 rsq     standard    0.127     5  0.0448 Preprocessor1_Model1\n\n# Random Forest\nrf_fit &lt;- fit_resamples(\n  rf_wf,\n  resamples = cv_folds,\n  metrics = metric_set(rmse, rsq)\n)\n\ncollect_metrics(rf_fit)\n\n# A tibble: 2 × 6\n  .metric .estimator   mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   36.0       5  6.68   Preprocessor1_Model1\n2 rsq     standard    0.184     5  0.0654 Preprocessor1_Model1\n\n# KNN\n\nset.seed(123)\nknn_fit &lt;- tune_grid(\n  knn_wf,\n  resamples = cv_folds,\n  grid = knn_grid,\n  metrics = metric_set(rmse, rsq)\n)\n\ncollect_metrics(knn_fit)\n\n# A tibble: 20 × 7\n   neighbors .metric .estimator   mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 rmse    standard   47.6       5  7.48   Preprocessor1_Model01\n 2         1 rsq     standard    0.188     5  0.0784 Preprocessor1_Model01\n 3         3 rmse    standard   36.3       5  5.98   Preprocessor1_Model02\n 4         3 rsq     standard    0.154     5  0.0436 Preprocessor1_Model02\n 5         5 rmse    standard   32.4       5  5.60   Preprocessor1_Model03\n 6         5 rsq     standard    0.174     5  0.0615 Preprocessor1_Model03\n 7         7 rmse    standard   31.1       5  5.15   Preprocessor1_Model04\n 8         7 rsq     standard    0.191     5  0.0643 Preprocessor1_Model04\n 9         9 rmse    standard   30.7       5  5.02   Preprocessor1_Model05\n10         9 rsq     standard    0.188     5  0.0757 Preprocessor1_Model05\n11        11 rmse    standard   30.4       5  5.03   Preprocessor1_Model06\n12        11 rsq     standard    0.201     5  0.0874 Preprocessor1_Model06\n13        13 rmse    standard   30.3       5  4.97   Preprocessor1_Model07\n14        13 rsq     standard    0.195     5  0.0832 Preprocessor1_Model07\n15        15 rmse    standard   30.5       5  4.81   Preprocessor1_Model08\n16        15 rsq     standard    0.180     5  0.0760 Preprocessor1_Model08\n17        17 rmse    standard   30.6       5  4.71   Preprocessor1_Model09\n18        17 rsq     standard    0.171     5  0.0726 Preprocessor1_Model09\n19        19 rmse    standard   30.8       5  4.66   Preprocessor1_Model10\n20        19 rsq     standard    0.165     5  0.0702 Preprocessor1_Model10\n\n\nNow, I will make a summary table:\n\nlm_metrics &lt;- collect_metrics(lm_fit) %&gt;% mutate(model = \"Linear Regression\")\nrf_metrics &lt;- collect_metrics(rf_fit) %&gt;% mutate(model = \"Random Forest\")\nknn_metrics &lt;- collect_metrics(knn_fit) %&gt;% mutate(model = \"KNN\")\n\nbind_rows(lm_metrics, rf_metrics, knn_metrics)\n\n# A tibble: 24 × 8\n   .metric .estimator   mean     n std_err .config               model neighbors\n   &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;     &lt;dbl&gt;\n 1 rmse    standard   32.3       5  4.36   Preprocessor1_Model1  Line…        NA\n 2 rsq     standard    0.127     5  0.0448 Preprocessor1_Model1  Line…        NA\n 3 rmse    standard   36.0       5  6.68   Preprocessor1_Model1  Rand…        NA\n 4 rsq     standard    0.184     5  0.0654 Preprocessor1_Model1  Rand…        NA\n 5 rmse    standard   47.6       5  7.48   Preprocessor1_Model01 KNN           1\n 6 rsq     standard    0.188     5  0.0784 Preprocessor1_Model01 KNN           1\n 7 rmse    standard   36.3       5  5.98   Preprocessor1_Model02 KNN           3\n 8 rsq     standard    0.154     5  0.0436 Preprocessor1_Model02 KNN           3\n 9 rmse    standard   32.4       5  5.60   Preprocessor1_Model03 KNN           5\n10 rsq     standard    0.174     5  0.0615 Preprocessor1_Model03 KNN           5\n# ℹ 14 more rows\n\n\nLinear Regression: The RMSE is 32.25 and the R-squared is 0.127 (indicates that approximately 12.7% of the variance is explained by the model).\nRandom Forest: The RMSE is 35.59 and the R-squared is 0.194 (indicates that approximately 19.4% of the variance is explained by the model).\nKNN: The best RMSE is the KNN model with 13 neighbors, RMSE = 30.32 (corresponding R-squared of 0.195). The best R-squared is the KNN model with 11 neighbors, R-squared = is 0.201 (corresponding RMSE of 30.35).\nNow, I will obtain residual plots for the three models.\n\n# Calculate residuals helper function\nget_residuals &lt;- function(model_fit, data, model_name) {\n  predict(model_fit, new_data = data) %&gt;%\n    bind_cols(data) %&gt;%\n    mutate(residual = er_wait_score - .pred,\n           model = model_name)\n}\n\n# Linear Regression Residuals\nlm_final &lt;- fit(lm_wf, data = train_data)\n\nlm_resid &lt;- get_residuals(lm_final, train_data, \"Linear Regression\")\n\n# Random Forest Residuals \nrf_final &lt;- fit(rf_wf, data = train_data)\n\nrf_resid &lt;- get_residuals(rf_final, train_data, \"Random Forest\")\n\n# KNN Residuals\nbest_k &lt;- select_best(knn_fit, metric = \"rmse\")\n\nknn_final_wf &lt;- finalize_workflow(knn_wf, best_k)\nknn_final &lt;- fit(knn_final_wf, data = train_data)\n\nknn_resid &lt;- get_residuals(knn_final, train_data, \"KNN\")\n\n# Plot! \nall_resid &lt;- bind_rows(lm_resid, rf_resid, knn_resid)\n\nggplot(all_resid, aes(x = .pred, y = residual, color = model)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"black\") +\n  facet_wrap(~model) +\n  labs(title = \"Residual Plots by Model\",\n       x = \"Predicted ER Wait Time\",\n       y = \"Residual (Actual - Predicted)\",\n       color = \"Model\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") + \n  scale_color_manual(values = c(\"Linear Regression\" = \"cadetblue3\",\n                                \"Random Forest\" = \"green3\",\n                                \"KNN\" = \"palevioletred2\")) +\ntheme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\n\n\nBest Model: Random Forest\nI would say that the random forest model performed the best. While it doesn’t have the lowest RMSE, it has the least scatter in the plot above. You can see that the residuals are closer to the dashed line at 0. The random forest’s RMSE value is 35.59 and its R-squared value is 0.194. It can be noted that all of the RMSE and R-squared values were relatively similar. So when it came to the residual plots above, I saw the least scatter within the random forest plot (which is ultimately what led me to make this decision).\n\n\nTest Data with Random Forest Model:\nNow, per class instruction, I will run the random forest model on the test data. It should be noted that the test data is very small in this exercise\n\n# Final fit on full training data\nrf_final &lt;- fit(rf_wf, data = train_data)\n\n# Predict on test data\nrf_predictions &lt;- predict(rf_final, new_data = test_data) %&gt;%\n  bind_cols(test_data)\n\n# Calculate metrics (RMSE, R-squared) on the test data\nrf_metrics_test &lt;- rf_predictions %&gt;%\n  metrics(truth = er_wait_score, estimate = .pred)\n\n# Print performance metrics\nprint(rf_metrics_test)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard      20.7  \n2 rsq     standard       0.358\n3 mae     standard      17.8  \n\n# Residuals on test data\nrf_predictions &lt;- rf_predictions %&gt;%\n  mutate(residual = er_wait_score - .pred)\n\n# Plot residuals\nggplot(rf_predictions, aes(x = .pred, y = residual)) +\n  geom_point(alpha = 0.7, size = 2, color = \"orange\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  labs(title = \"Residual Plot: Random Forest (Test Data)\", \n       x = \"Predicted ER Wait Time\", y = \"Residual (Actual - Predicted)\") +\n  theme_minimal() + \n  theme(\n    plot.title = element_text(hjust = 0.5,  # center the title\n                              face = \"bold\",  # make title bold\n                              size = 16)      # make title bigger\n  )\n\n\n\n\n\n\n\n\nThe RMSE is 21.28, and the R-squared value is 0.334 (indicating that approximately 33.4% of the variance is explained by the model). The residuals are somewhat scattered (ranging from roughly -40 to 40).\n\n\nSummary and Disucssion:\nIn this tidy-tuesday exercise, I aimed to assess whether there is a connection between the size of a state’s population and their median emergency department wait times (in minutes). As described previously, the data was obtained from CMS and includes information about emergency department (and others) wait times for various states. After performing some basic data cleaning and exploration, I decided to look specifically at median emergency department wait times (in minutes). Since this dataset did not include the population size of each state, I had to obtain this information from elsewhere online. The dataset I used did not include the populations of DC and PR, therefore I decided to exclude them from my analysis.\nAfter cleaning my data and running some basic summary functions on it, I found that the average median emergency department wait time across all states was 155.4 minutes, with a range of 110 to 251 minutes (141 minutes). After assessing the histogram for median emergency department wait times, I found that it was right-skewed. As for population, I found that the average population in US states is 6,788,175 people. The range for state population was found to be 587618 to 39431263 people, which is much larger than the range for median emergency department wait times. The histogram for state population was found to be right-skewed as well, but to a greater degree than the skewness of the median emergency department wait times histogram. I then calculated the correlation between median emergency department wait times and state population sizes. I found a correlation coefficient of 0.2517527, which indicates a weak postitive relationship. Then, I ran a scatterplot between the two variables and found that most of the points were clustered in the lower left corner of the graph. A few points deviated from the overall pattern. However, I decided to keep in the potential outliers, since each US state should be represented in this analysis. Overall, this reveals that larger states tend to have slightly longer ER wait times. However, this relationship is not strong.\nAfter setting up my test and train data (as well as cross-validation folds), I developed three models for this exercise. The first model was a simple linear regression model; it had an RMSE of 32.25 and an R-squared value of 0.127. Then I ran a scatterplot of the residuals and found that they appeared to be quite scattered. The next model I developed was the random forest model; it had an RMSE of 35.59 and an R-squared value of 0.194. The residuals within the residual scatterplot appeared to be less scattered about the 0 line. The last model that I chose to develop was the KNN model; the best RMSE was found to be 30.32 (with 13 neighbors) and the best R-squared value was 0.201 (with 11 neighbors). The residuals for this model were also quite scattered. After comparing all three models, I decided that the random forest model was the best due to it having the least scatter of residuals (despite it not having the lowest RMSE value). I then ran the random forest model on the test data and found that the RMSE was 21.28 and the R-squared value was 0.334. the residuals for this model were somewhat scattered. However, I believe that they would be more scattered if I had chosen another model to run with.\nOverall, my analysis for tidy-tuesday suggests that state population had a weak positive association with median emergency department wait times (in minutes). However, it should be noted that the analysis would be enriched by inclusion of other elements in the dataset, such as the number of hospitals within each state/hopsital density.\n\n# Create a data frame with the results, including correlation for ER wait time and population\nmodel_results &lt;- data.frame(\n  Model = c(\"Linear Regression\", \"Random Forest\", \"KNN (13 neighbors)\", \"Correlation between ER wait time and State Population\"),\n  Correlation = c(NA, NA, NA, 0.15),  \n  RMSE = c(32.25, 35.59, 30.32, NA),\n  R_squared = c(0.127, 0.194, 0.201, NA)\n)\n\n# Use GT to format the table\nlibrary(gt)\n\n# Create the GT table\nmodel_results_gt &lt;- model_results %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Model Performance and Correlation Results\"\n  ) %&gt;%\n  cols_label(\n    Model = \"Model / Metric\",\n    Correlation = \"Correlation between ER wait time and State Population\",\n    RMSE = \"Root Mean Squared Error (RMSE)\",\n    R_squared = \"R-squared\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Model Metrics\",\n    columns = c(RMSE, R_squared)\n  ) %&gt;%\n  tab_style(\n    style = list(\n      cell_borders(sides = \"all\", color = \"gray\", weight = px(0.5))\n    ),\n    locations = cells_body(columns = c(Correlation, RMSE, R_squared))\n  )\n\n# Print the GT table\nmodel_results_gt\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Performance and Correlation Results\n\n\nModel / Metric\nCorrelation between ER wait time and State Population\n\nModel Metrics\n\n\n\nRoot Mean Squared Error (RMSE)\nR-squared\n\n\n\n\nLinear Regression\nNA\n32.25\n0.127\n\n\nRandom Forest\nNA\n35.59\n0.194\n\n\nKNN (13 neighbors)\nNA\n30.32\n0.201\n\n\nCorrelation between ER wait time and State Population\n0.15\nNA\nNA\n\n\n\n\n\n\n\n\n# Bind together the residuals for all models from both train and test data\nall_resid_combined &lt;- bind_rows(\n  lm_resid %&gt;% mutate(Color = \"blue\", Dataset = \"Train\"),\n  rf_resid %&gt;% mutate(Color = \"red\", Dataset = \"Train\"),\n  knn_resid %&gt;% mutate(Color = \"green\", Dataset = \"Train\"),\n  rf_predictions %&gt;% mutate(Color = \"orange\", Dataset = \"Test\")\n)\n\n# Plotting the combined residuals with the legend\nggplot(all_resid_combined, aes(x = .pred, y = residual, color = Color)) +\n  geom_point(alpha = 0.7, size = 2) +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n  labs(\n    title = \"Residual Plots for Different Models and Datasets\",\n    x = \"Predicted ER Wait Time\",\n    y = \"Residual (Actual - Predicted)\"\n  ) +\n  scale_color_manual(\n    name = \"Model/Dataset\",\n    values = c(\"blue\", \"red\", \"green\", \"orange\"),\n    labels = c(\"Linear Regression (Train)\", \"Random Forest (Train)\", \"KNN (Train)\", \"Random Forest (Test)\")\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\", size = 16),\n    legend.position = \"right\"\n  )"
  },
  {
    "objectID": "course-project/Project-Manuscript.html",
    "href": "course-project/Project-Manuscript.html",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "",
    "text": "Authors\nAuthor Affiliations\n\\(\\dagger\\) Disclaimer: The opinions expressed in this article are the author’s own and don’t reflect those of their university affiliations.\nFor full project, visit the repository: https://github.com/NatalieCann16/Cann-MADA-project/tree/main"
  },
  {
    "objectID": "course-project/Project-Manuscript.html#general-background-information",
    "href": "course-project/Project-Manuscript.html#general-background-information",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "2.1 General Background Information",
    "text": "2.1 General Background Information\nRespiratory illnesses are a large burden to both medical and public health systems. These illnesses are known to spike in the colder months, such as December, January, and February. One such respiratory virus is COVID-19, also known as severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) (1). COVID-19 was declared a pandemic in 2020. The virus is known to spread through fluid droplets shared between individuals in close contact as well as through the air (1). The spike protein is known to mediate entry of the virus into host cells via its S1 subunit, which binds to a receptor on the host cell, and S2 subunit, which allows the virus to fuse with the host cell (2). Therefore, the spike protein is a major antigen of the COVID-19 virus recognized by the immune system.\nThere are two different types of COVID-19 vaccinations available in the United States (U.S.). The first is the mRNA vaccine, which uses mRNA created in a laboratory that triggers an immune response within the body that produces antibodies to protect against COVID-19 (3). The second is the protein subunit vaccine, which contains spike proteins from the virus as well as an adjuvant; this enables the body to produce antibodies and better recognize the spike protein in the future (3). The Pfizer-BioNTech and Moderna vaccines are both mRNA vaccines (4). The Novavax vaccine is a protein subunit vaccine (4). The Janssen vaccine, also known as the Johnson and Johnson vaccine, utilized an adenovirus as a vector (5). Adenovirus vaccines use an omnipresent virus that is altered so that it cannot cause illness or integrate into the host’s DNA (5). The adenovirus is engineered to contain genes involved in making the COVID-19 spike protein, which then leads to the immune system producing antibodies for the spike protein (5). The Janssen vaccine was discontinued in May 2023 (4).\nResearch on the distribution of vaccines as a whole has been relatively limited. Vaccine distribution and administration have not had the best documentation throughout history in both the U.S. and globally (6). Few states have effective systems to track vaccine distribution, administration, inventory, and demand (6). As a result of this, in the U.S., some regions receive more vaccines than are used, while other regions do not receive enough vaccines to fulfill the demand (6). Not only is this an ineffective use of resources, but it also has an economic cost (6). Successful distribution and administration of vaccines, especially in times of emergency, is crucial as it can lower the burden on healthcare systems and lower mortality (7). These issues convey the importance of effective vaccine data collection and reporting in order to improve the efficiency of vaccination campaigns.\nWhile the COVID-19 vaccines were developed in a timely response, the U.S. was not prepared to effectively distribute these vaccines. Healthcare workers and elderly individuals were prioritized for vaccination, which was a great first step. However, there proved to be issues with the distribution of the vaccines. First, vaccines distributed to pharmacies and other small venues were administered at a low rate, indicating poor distribution plans (7). Additionally, the storage and transport of the COVID-19 vaccines was a challenge as the vaccine needed to be kept at a very low temperature. This proved to be difficult to incorporate into planning effective distribution of the COVID-19 vaccine (7). Research must be done in order to better respond to regional vaccine demand in emergency, and non-emergency, situations in the future.\nResearch has been done to determine ways in which the distribution of the COVID-19 vaccine during the pandemic could be improved. A study by Muckstadt et al. (2023) proposed a new distribution strategy that would have allow for more effective distribution of the COVID-19 vaccine in the U.S. during the pandemic (7). Another study by Bertsimas et al. (2021) proposed the use of the DELPHI model to capture the effects of the vaccine and differing mortality rates across age groups to improve the allocation of the COVID-19 vaccination (8). Building on this body of research, this project aims to assess how regional and manufacturer-specific distribution and administration trends differ and have changed over time. Furthermore, it attempts to determine whether the manufacturer-specific proportion of total vaccines administered can be predicted by several synthetic variables. Understanding these patterns may offer additional insight into how vaccine characteristics, supply chain logistics, and demographic or regional factors influence distribution outcomes."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#description-of-data-and-data-sources",
    "href": "course-project/Project-Manuscript.html#description-of-data-and-data-sources",
    "title": "Analysis of COVID-19 and Influenza Vaccine Associations",
    "section": "2.2 Description of Data and Data Sources",
    "text": "2.2 Description of Data and Data Sources\nThe datasets used in this analysis are both from the CDC. The first dataset, regarding COVID-19 vaccinations, contains information on the distribution and administration of COVID-19 vaccinations between 2020 and 2023. The second dataset, focusing on influenza vaccinations, contains information on the distribution of flu vaccines between 2018 and 2024. The influenza vaccination dataset is grouped by flu-season. More information on the datasets can be found in the “Methods” section under “Data Acquisition.”"
  },
  {
    "objectID": "course-project/Project-Manuscript.html#questions-to-be-addressed",
    "href": "course-project/Project-Manuscript.html#questions-to-be-addressed",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "2.2 Questions to be Addressed",
    "text": "2.2 Questions to be Addressed\nAs introduced previously, the three questions that will be addressed in this project are as follows:\n\nHow do regional differences in the United States influence the distribution and administration of COVID-19 vaccines, and are these patterns associated with specific vaccine manufacturers?\nHow have regional patterns and manufacturer-specific trends in COVID-19 vaccine distribution and administration evolved over time in the United States?\nCan the proportion of Pfizer vaccines distributed be predicted by several synthetic variables (listed in the modeling section), year, MMWR week, and doses distributed per 100k?\n\nIt is hypothesized that the Northeast region of the U.S. will have the highest uptake of the COVID-19 vaccine due to the area being predominantly liberal. Left-leaning individuals tend to be more trusting of the COVID-19 vaccine (9). It is believed that Pfizer will have the highest distribution and administration of the vaccine as it received a significant amount of media coverage as compared to other manufacturers (10). Furthermore, it is likely that the distribution and administration of the COVID-19 vaccine peaked around 2022 during the height of the pandemic. Lastly, it is hypothesized that the models will not sufficiently predict the proportion of Pfizer vaccines distributed since synthetic variables were used."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#schematic-of-workflow-for-analysis",
    "href": "course-project/Project-Manuscript.html#schematic-of-workflow-for-analysis",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "3.1 Schematic of Workflow for Analysis",
    "text": "3.1 Schematic of Workflow for Analysis\nFirst, the data was obtained from the Centers for Disease Control and Prevention (CDC) and the population data was obtained from the U.S. Census Bureau. For more details on the datasets, see the Data Acquisition section below. Then, the data was processed and cleaned. The data was sorted by MMWR week and region. For more details on this, see the Data Cleaning Process section. Then, exploratory data analysis was performed. From this, the answers to the first two research questions were obtained. Finally, in order to answer the final research question, models were performed. The data was split into train and test datasets and five-fold cross-validation repeated five times was performed. The models used were: simple linear regression, multiple linear regression, least absolute shrinkage and selection operator (LASSO) regression, and random forest modeling. The models were evaluated using RMSE (Root Mean Square Error) values, R-squared values, and their residual plots were observed.\n\n\n\n\n\nSchematic of Workflow for Analysis"
  },
  {
    "objectID": "course-project/Project-Manuscript.html#data-aquisition",
    "href": "course-project/Project-Manuscript.html#data-aquisition",
    "title": "Analysis of COVID-19 and Influenza Vaccine Associations",
    "section": "3.2 Data Aquisition",
    "text": "3.2 Data Aquisition\nCOVID-19 Dataset: https://data.cdc.gov/Vaccinations/COVID-19-Vaccinations-in-the-United-States-Jurisdi/unsk-b7fc/about_data\nThis dataset came from the CDC and contains information on the distribution and administration of COVID-19 vaccinations between 2020 and 2023.Data is included from all sites that distribute and administer vaccinations in the U.S.; such as jurisdictional partner clinics, retail pharmacies, dialysis centers, long-term care facilities, Federal Emergency Management Agency (FEMA), and federal entity facilities. The CDC states that the dataset was provided by IISInfo. I downloaded the data directly from the CDC website provided above. This dataset was last updated May 12, 2023.\nInfluenza Dataset: https://data.cdc.gov/Vaccinations/Weekly-Cumulative-Doses-in-Millions-of-Influenza-V/k87d-gv3u/about_data\nThis dataset also came from the CDC and contains information regarding the doses of flu vaccines in Millions distributed between 2018 and 2024. According to the CDC website, the dataset was provided by the National Center for Immunization and Respiratory Diseases (NCIRD). This dataset was last updated March 29, 2024."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#data-cleaning-process",
    "href": "course-project/Project-Manuscript.html#data-cleaning-process",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "3.3 Data Cleaning Process",
    "text": "3.3 Data Cleaning Process\nThe COVID-19 vaccine dataset contains 109 variables and 38,488 observations. The variables of special interest in this dataset are: Date, MMWR week, Location, Total Distributed Doses, Total Manufacturer-Specific Distributed Doses, Total Administered Doses, and Total Manufacturer-Specific Administered Doses.\nThe following data processing and cleaning steps were performed:\n\nDefined variables of interest\nAssessed missingness of data and variable classes\nUsed the lubridate package to fix the date format\nSelect variables of interest to work with for analysis\nCreated Region variable and sorted U.S. states into their appropriate geographic regions\nManipulated dataset to only include data from 2021 to 2023 (data from 2020 was not as well documented, and therefore was excluded)\nSummed distribution and administration variables by MMWR week and region (each region had its own row for each MMWR week)\nAdded U.S. Census Bureau Regional Population Data in order to properly control for population by calculating distributed or administered vaccines per 100,000\nCreated synthetic variables to use in modeling\nSave final dataset as an RDS file\n\nIn order to categorize the states into their appropriate regions, the CDC’s geographic divisions were used (14). The graphic below displays these regions. For simplicity, only the Northeast, Midwest, South, and West regions were used in this project. Note the states included in each region within the map below.\n\n\n\n\n\nCDC’s Geographic Division Regions"
  },
  {
    "objectID": "course-project/Project-Manuscript.html#statistical-analysis",
    "href": "course-project/Project-Manuscript.html#statistical-analysis",
    "title": "Analysis of COVID-19 and Influenza Vaccine Associations",
    "section": "3.4 Statistical Analysis",
    "text": "3.4 Statistical Analysis\nI will perform simple linear regressions, a granger-causality test, and cross-correlation functions to assess the relationships between the administration of the COVID-19 vaccine and its manufacturer as well as the administration of the flu vaccine. I will also run a few machine learning models, such as random forest."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#exploratory-and-descriptive-analysis",
    "href": "course-project/Project-Manuscript.html#exploratory-and-descriptive-analysis",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "4.1 Exploratory and Descriptive analysis",
    "text": "4.1 Exploratory and Descriptive analysis\nTable 1 displays the total counts of COVID-19 vaccines distributed and administered in 2021, 2022, and 2023 for each region of the U.S. The mean total distributed doses of the vaccine for the U.S. as a whole between 2021 and 2023 was determined to be 5.667e+08; the mean total administered doses of the vaccine for the U.S. between 2021 and 2023 was 4.479e+08 (Supplement Table 1). There was a nearly perfect positive correlation of 0.99 between the number of total weekly doses distributed and administered (Supplement Figure 1). It can be seen that Pfizer had the highest number of doses distributed and administered, followed by Moderna, Janssen, and Novavax. It should be noted that the Novavax vaccine was not made available until 2022 and that the Janssen vaccine was discontinued in 2023. Across all three years of interest, the South had both the highest number of COVID-19 vaccines distributed and administered. The West had the second highest number of vaccines distributed and administered. The Midwest had the third highest number of vaccines distributed and administered. The Northeast had the lowest number of vaccines distributed and administered. However, the Midwest and Northeast’s numbers were very similar.\n\n\n\n\n\nTable 1: COVID-19 Vaccine Administration and Distribution by Region\n\n\n\n\nTable 2 displays the total counts of COVID-19 vaccines distributed and administered in 2021, 2022, and 2023 for each region of the U.S. after being adjusted by population size. After this adjustment, the mean total distributed doses per 100,000 population in the U.S. between 2021 and 2023 was 687,974; the mean total administered doses per 100,000 population in the U.S. between 2021 and 2023 was 549,053 (Supplement Table 1). These numbers are likely very high due to the fact that multiple doses are required for the COVID-19 vaccine. After adjustment, the Northeast actually had the highest number of doses distributed and administered between 2021 and 2023. The West followed behind the Northeast in doses distributed and administered. In general, the South and Midwest had comparable numbers. The Midwest had higher numbers than the South in all but one case, the distributed doses in 2023. Pfizer had the greatest number of doses administered and distributed, followed by Moderna and then Janssen. For quick comparison with Table 1, the formatting of reporting doses by Millions was kept. Therefore, Novavax, as well as Unknown Manufacturer, were reported as 0 million as they were not high enough to surprass this threshold.\n\n\n\n\n\nTable 2: Population Adjusted COVID-19 Vaccine Administration and Distribution by Region\n\n\n\n\nFigure 1 shows the distribution of the COVID-19 vaccine by region from 2021 to 2023. The Northeast’s dominance in the average number of COVID-19 vaccines distributed and administered is visually apparent. The West comes in second. The Midwest and South have similar numbers. Figure 1 also displays the administration of the COVID-19 vaccine by region between 2021 and 2023. It can be seen that the Northeast also had the highest average weekly number of COVID-19 vaccines administered. The West follows and actually had the highest correlation (0.999) between doses administered and distributed (Supplement Table 2). The Midwest and South have similar numbers. Overall, regional trends in vaccine distribution and administration were relatively consistent.\n\n\n\n\n\nFigure 1: Population Adjusted COVID-19 Vaccine Administration and Distribution by Region in the U.S.\n\n\n\n\nIn Figure 2, the distribution of the COVID-19 vaccine by manufacturer in the U.S. is displayed. The Pfizer vaccine had the highest number of doses distributed, followed by Moderna, Janssen, and Novavax. Figure 2 also displays the administration of the COVID-19 vaccine by manufacturer in the U.S. The trends are similar to those of distribution. Pfizer had the highest number of doses administered, followed by Moderna, Janssen, and lastly, Novavax. A significant number of doses were attributed to unknown manufacturers, likely due to incomplete vaccine data records.\n\n\n\n\n\nFigure 2: Population Adjusted COVID-19 Vaccine Administration and Distribution by Manufacturer in the U.S.\n\n\n\n\nFigure 3 displays the distribution of the COVID-19 vaccine by region over time. It can be seen that the distribution of the vaccine rises between 2021 and 2022, but then begins to rapidly fall. All regions follow this trend. However, the doses distributed in the Northeast appear to be significantly higher than the other regions. Between 2022 and 2023, there was an 87.4% decrease in doses distributed in the South; an 87.0% decrease in doses distributed in the West; an 86.7% decrease in doses distributed in the Midwest; and an 86.9% decrease in doses distributed in the Northeast (Supplement Table 3).\nFigure 4 displays the administration of the COVID-19 vaccine by region over time. The administration of the vaccine is also on the rise between 2021 and 2022, but then rapidly drops off as 2022 goes on. Each region’s trend is similar for both the distribution and administration of the vaccine. Between 2022 and 2023, there was an 88.9% decrease in doses administered in the South; an 88.4% decrease in doses administered in the West; an 88.4% decrease in doses administered in the Midwest; and an 88.5% decrease in doses administered in the Northeast (Supplement Table 3).\n\n\n\n\n\nFigures 3 and 4: COVID-19 Vaccine Distribution and Administration by Region Over Time (2021-2023)\n\n\n\n\nFigure 5 displays the distribution of the COVID-19 vaccine by manufacturer over time. It can be seen that the distribution of the vaccine rises between 2021 and 2022, but then begins to rapidly fall. All manufacturers follow this trend. This trend was observed in the distribution and administration of vaccines by region (Figure 3) as well. Between 2022 and 2023, Janssen observed a 90.1% decrease in doses distributed; Moderna observed an 87.8% decrease; Pfizer observed an 87.4%; and Novavax observed a 33.3% decrease (Supplement Table 4.1). Novavax’s percent decrease in doses distributed is much lower due to the fact that this vaccine did not start rolling out until 2022. By then, Moderna and Pfizer had been dominating the distribution field.\nIn Figure 6, the administration of the COVID-19 vaccine by manufacturer over time can be seen. The administration of the vaccine is also on the rise between 2021 and 2022, but then rapidly drops off as 2022 goes on. This trend was also observed in the distribution and administration of vaccines by region (Figure 4). In both Figures 5 and 6, Pfizer and Moderna dominated in the distribution and administration of the vaccine. This goes along with what was seen in Figure 2. Between 2022 and 2023, Janssen observed a 90.1% decrease in doses administered; Moderna observed an 88.9% decrease; Pfizer observed an 88.8% decrease; and Novavax observed a 92.6% decrease (Supplement Table 4.2).\n\n\n\n\n\nFigures 5 and 6: Population Adjusted COVID-19 Vaccine Distribution and Administration by Primary Manufacturers Over Time (2021-2023)"
  },
  {
    "objectID": "course-project/Project-Manuscript.html#basic-statistical-analysis",
    "href": "course-project/Project-Manuscript.html#basic-statistical-analysis",
    "title": "Analysis of COVID-19 and Influenza Vaccine Associations",
    "section": "4.2 Basic Statistical Analysis",
    "text": "4.2 Basic Statistical Analysis\nI performed a linear regression analysis to assess the significance of doses .\n\n\n\n\n\nTable 3: Linear Regression Analysis to Assess Vaccine Distribution by Manufacturer\n\n\n\n\nWith an alpa value of 0.05, the p-values for each manufacturer are less than 0.5. This means that the manufacturer has a statistically significant impact on the number of doses administered. It can be noted that Pfizer and Moderna both have very small p-values (both of 2e-16); furthermore they have positive estimate values, indicating they have a higher number of administered does than Janssen. The negative estimate values for Novavax and Unknown Manufacturer indicate that they have a lower number of administered doses compared to Janssen."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#full-analysis",
    "href": "course-project/Project-Manuscript.html#full-analysis",
    "title": "Analysis of COVID-19 and Influenza Vaccine Associations",
    "section": "4.3 Full analysis",
    "text": "4.3 Full analysis\nNot completed yet - use one or several statistical/machine learning methods to analyze your data and make figures/tables\nI completed a Granger Causality Test to assess the relationship between the administration of the COVID-19 vaccine and the flu vaccine. The results of the test are displayed in the table below.\n\n\n\n\n\nTable 4: Granger Causality Test\n\n\n\n\nWe must fail to reject the null hypothesis (F = 0.65; p = 0.42). This suggests that changes in COVID vaccination rates do not appear to influence flu vaccination rates in a predictive sense.\nI performed a Cross-Correlation Function to further assess the relationship between the administration of the COVID-19 vaccine and the flu vaccine. Specifically, this analysis assesses correlation between administration of the vaccines through time. The results of the test are displayed in the graph below.\n\n\n\n\n\nFigure 3: Cross-Correlation Function\n\n\n\n\nThe small peaks and bars suggest that there is a very weak correlation between the administration of covid vaccines and flu vaccines during this time period (2021-2023). There appear to be a few small negative lags, indicating that flu vaccinations might slightly precede Covid-19 vaccinations. Overall, there is no strong relationship beween the two.\nI then ran a few models: simple linear regression, cross-validated linear regression, LASSO regression, and random forest. Their Root Mean Squared Errors (RMSEs) and R-squared values can be seen below.\n\n\n\n\n\nTable 5: Model Comparison\n\n\n\n\nAs can be seen, it appears as though there was an issue with overfitting with each of the models.\nThe corresponding plots of observed vs. predicted points for each of the models can be seen below.\n\n\n\n\n\nFigure 4: Model Comparison Plots\n\n\n\n\nFrom Table 5 and Figure 4, it appears as though overfitting was an issue with the models. The Cross-Validated Linear Regression model appears to have performed the best, with an RMSE value equivalent to 0 and an R-squared value of 1.0. I used LASSO Regression in an attempt to reduce overfitting. However, the RMSE value is 1,314505.0 while its R-squared value is 0.99999. Unfortunatley, it seems that overfitting was not significantly reduced after LASSO Regression was performed. It appears that Simple Linear Regression performed the worst, with an incredibly high RMSE of 143,707,237,506.00 and an R-squared value of 1.0."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#summary-and-interpretation",
    "href": "course-project/Project-Manuscript.html#summary-and-interpretation",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "5.1 Summary and Interpretation",
    "text": "5.1 Summary and Interpretation\nThis project examined the distribution and administration of COVID-19 vaccines across regions of the United States from 2021 to 2023. Three key research questions were addressed: (1) How do regional differences impact the distribution and administration of COVID-19 vaccines, and are these patterns associated with specific vaccine manufacturers?; (2) How have regional patterns and manufacturer-specific trends in distribution and administration changed over time?; (3) Can the proportion of Pfizer vaccines distributed be predicted by several synthetic variables, year, MMWR week, and doses distributed per 100k? The synthetic variables created for this model were: the average age of those vaccinated in the region that week, the weekly COVID-19 vaccine hesitancy index, the weekly COVID-19 vaccine accessibility index, the weekly proportion of vaccines administered to males, the weekly public health campaign intensity score, and the weekly burden of COVID-19 score.\nDescriptive analyses exhibited that Pfizer had the highest number of doses distributed and administered, followed by Moderna, Janssen, then Novavax between 2021 and 2023. It should be noted that the Novavax vaccine was not made available until 2022 and that the Janssen vaccine was discontinued in 2023. Furthermore, it was revealed that the Northeast consistently had the highest number of both distributed and administered doses across 2021 to 2023, followed by the West, the Midwest, and lastly, the South. Temporal analyses revealed that the distribution and administration of COVID-19 vaccines peaked between 2021 and 2022, followed by a sharp decline. This trend may be associated with public health campaigns, initial rollout efforts, and changes in public perception of the vaccine and the COVID-19 pandemic.\nTo assess whether the proportion of Pfizer doses distributed could be predicted by the previously described synthetic variables, as well as doses distributed per 100k, year, and MMWR week. Along with simple linear regression to assess each individual predictor, three models were utilized: multiple linear regression, LASSO regression, and random forest. The multiple linear regression models and LASSO regression models performed poorly. Overfitting was a significant issue with the first random forest model. The modified random forest model appeared to perform the best. The test data was fitted to this modified random forest model. The resulting metrics revealed that the model did not sufficiently fit the test data. This suggests that the synthetic variables used in this analysis were not strong predictors of the proportion of Pfizer vaccines distributed."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#strengths-and-limitations",
    "href": "course-project/Project-Manuscript.html#strengths-and-limitations",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "5.2 Strengths and Limitations",
    "text": "5.2 Strengths and Limitations\nA strength of this research project is that the data was obtained from the CDC, which is a reliable source. The data contained a large number of observations from all states within the U.S., which enhanced the representativeness of the study. The use of standardized temporal units (MMWR weeks) allowed for consistent analysis of distribution and administration trends over time. Furthermore, data from the U.S. Census Bureau allowed for population differences to be controlled for.\nHowever, there were a significant number of limitations to this project. First and foremost, there are always drawbacks to not collecting one’s own data. An issue that stemmed from this was that there were not enough predictor variables to use in the modeling analysis. Therefore, synthetic variables had to be created. These synthetic variables may not have fully reflected the influences on manufacturer-specific uptake (such as the proportion of Pfizer vaccines distributed). Overfitting appeared to be an issue with some of the models used in the analysis, such as the first random forest model. Additionally, many distributed and administered doses were reported with an “unknown manufacturer”, which may have limited the ability to obtain accurate estimates for each manufacturer. Furthermore, the dataset was limited to the United States, which may limit the generalizability of the findings to other countries."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#conclusions",
    "href": "course-project/Project-Manuscript.html#conclusions",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "5.3 Conclusions",
    "text": "5.3 Conclusions\nIn conclusion, this project revealed distinct regional and temporal trends in both the distribution and administration of the COVID-19 vaccine across the U.S. from 2021 to 2023. However, attempts to predict the proportion of Pfizer vaccines distributed using synthetic predictors and MMWR week were ultimately unsuccessful. Further research needs to be done to assess the factors that influence COVID-19 vaccine manufacturer-specific uptake. The collection of open-ended qualitative data from the public would be of great benefit to obtaining a deeper understanding of these factors."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#description-of-the-data-and-data-acquisition",
    "href": "course-project/Project-Manuscript.html#description-of-the-data-and-data-acquisition",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "3.2 Description of the Data and Data Acquisition",
    "text": "3.2 Description of the Data and Data Acquisition\nCOVID-19 Dataset\nThis dataset came from the CDC and contains information on the distribution and administration of COVID-19 vaccinations between 2020 and 2023 (11). Data is included from all sites that distribute and administer vaccinations in the U.S., such as jurisdictional partner clinics, retail pharmacies, dialysis centers, long-term care facilities, Federal Emergency Management Agency (FEMA), and federal entity facilities. The CDC states that the dataset was provided by IISInfo. The data was downloaded directly from the CDC and was last updated May 12, 2023. The dataset contains both overall distribution and administration data, while also including distribution and administration data specific to each manufacturer. Furthermore, there are variables stating the morbidity and mortality weekly report (MMWR) week that the data was reported during as well as the state from which the data was reported. The data from 2020 is not as well documented as that of 2021 through 2023; therefore, only data from 2021 to 2023 will be used in this project.\nU.S. Census Bureau Dataset\nSome U.S. regions, such as the South, has a disproportionately higher population than the rest of the major regions. In 2022, the South had 128,716,192 residents (12). Therefore, population must be controlled for in this analysis. In order to control for regional population, U.S. Census Bureau data of the four major U.S. region’s populations between 2021 and 2023 was used (13). The four major regions are the Midwest, Northeast, South, and West. Note the states included in each region within the map included in the data cleaning section."
  },
  {
    "objectID": "course-project/Project-Manuscript.html#modeling",
    "href": "course-project/Project-Manuscript.html#modeling",
    "title": "Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "4.2 Modeling",
    "text": "4.2 Modeling\nThe population adjusted data was split into training data and test data based on year. In order to account for the unique trends present in the three different years, 75% of the data from each year within the dataset went into the training data, while the remaining 25% of the data from each year went into the test data. Then, a null model containing no predictors was fitted in order to predict the outcome, the proportion of Pfizer doses distributed. This null model properly predicted the mean proportion of Pfizer doses distributed. The RMSE of the null model was 0.169 (Supplement Table 5). Five-fold cross-validation was performed and repeated five times to minimize potential overfitting. Then, each predictor was tested by themselves using Simple Linear Regression. The predictors used were: doses distributed per 100k, average age of those vaccinated, vaccine hesitancy index, vaccine accessability index, the proportion of males who received vaccines, public health campaign intensity score, burden of COVID-19 score, year, and MMWR week. All predictors were synthetic except for year, MMWR week, and doses distributed per 100k. None of the predictors stood out as having outstanding metrics. However, year appeared to be the best predictor, with an RMSE of 0.142 and an R-squared of 0.154 (Supplement Table 6). See Supplement Table 6 for all Simple Linear Regression results.\nThree models were fit to the training data: multiple linear regression, LASSO regression, and random forest modeling. For each type of model, two sets of predictors were run. The first set contained all synthetic predictors, year, MMWR week, and doses distributed per 100k. The second set of predictors contained all synthetic predictors and year. This model, to which this set of predictors is fit, will be referred to as the ‘modified model.’ When fit to the training data, the first multiple linear regression model had an RMSE of 0.103 and an R-squared of 0.561. Figure 7 displays the observed vs predicted plot for this model, which displays significant clustering. The modified multiple linear regression model performed considerably worse and had an RMSE of 0.144 and an R-squared value of 0.138. Figure 8 displays the observed vs predicted plot for the modified multiple linear regression model. This plot also displays significant clustering.\n\n\n\n\n\nFigure 7 and 8: Multiple Linear Regression Plots\n\n\n\n\nNext, LASSO regression was performed in an attempt to reduce overfitting. The original LASSO regression model had an RMSE of 0.002 with an R-squared of 0.003. This R-squared is quite low; the multiple linear regression model’s R-squared value of 0.561 was much better. The observed vs predicted plot for this LASSO model is shown in Figure 9. The clustering pattern appears to be similar to that of the original multiple linear regression model in Figure 7. Next, the modified set of predictors was used in a LASSO regression model. This modified LASSO regression model had an RMSE of 0.004 and an R-squared of 0.011. Within the model’s observed vs predicted plot in Figure 10, there appears to be clustering with a similar pattern to that of the modified multiple linear regression model in Figure 8. Overall, it seems that the LASSO regression models did not perform well.\n\n\n\n\n\nFigures 9 and 10: LASSO Regression Plots\n\n\n\n\nThe final models run were random forest models. The first random forest model, including all predictors, resulted in an RMSE value of 0.008 and an R-squared value of 0.998. This indicated overfitting. In Figure 11, the model’s observed vs predicted plot is shown to display significant overfitting. Next, the modified random forest model was run. With an RMSE of 0.049 and an R-squared value of 0.929, this model performed better than the first random forest model. As can be seen, there appears to be some clustering within the bottom left and upper right corners of the modified model’s observed vs predicted plot in Figure 12. Overall, this model appeared to perform the best out of all the models run in this analysis. Therefore, this model would be fitted to the test data. Table 3 displays a summary table of the metrics of each of the models run.\n\n\n\n\n\nFigures 11 and 12: Random Forest Plots\n\n\n\n\n\n\n\n\n\nTable 3: Summary of Model Performance\n\n\n\n\nWhen the modified random forest model was fit to the test data, an RMSE of 0.117 and an R-squared of 0.427 were obtained. This RMSE is higher than that of the model fit to the training data. Furthermore, the R-squared value is significantly lower, indicating that this model did not sufficiently fit the data. Figure 13 displays the model’s observed vs predicted plot. There appears to be much less of a close fit to the diagonal line than in Figure 12. Out of curiosity, the original random forest model that suffered from significant overfitting was fitted to the test data. The resulting RMSE and R-squared values were 0.013 and 0.993 respectively (Supplement Table 7). The observed vs predicted plot confirmed that overfitting remained an issue when the model was fitted to the test data (Supplement Figure 6).\n\n\n\n\n\nTable 4: Modified Random Forest Model Performance on Test Data\n\n\n\n\n\n\n\n\n\nFigure 13: Modified Random Forest Model Observed vs Predicted (Test Data Fit)"
  },
  {
    "objectID": "course-project/Supplementary-Material.html",
    "href": "course-project/Supplementary-Material.html",
    "title": "Supplementary-Material: Analysis of COVID-19 Vaccination Trends: Distribution and Administration Between 2021 and 2023",
    "section": "",
    "text": "For full project, visit the repository: https://github.com/NatalieCann16/Cann-MADA-project/tree/main\n\nOverview\nAnalysis of COVID-19 Vaccination Trends: Distribution and Administration\nThis document provides supplementary material for the manuscript “Analysis of COVID-19 Vaccination Trends: Distribution and Administration”. It includes additional details on the methods used in the analysis, as well as additional results that were not included in the main manuscript.\n\n\nCode and file information\n\n“Cann-MADA-project.Rproj”: Establishes relative file paths for project\n“README.md”: Provides brief order of scripts for reproducing and summarizes the folders within the project\n“code” folder: Contains all code for processing, exploratory data analysis, and modeling analysis\n\n“processing-code” subfolder:\n\n“processing.qmd”: Contains code for processing the raw data into the processed data\n\n“eda-code” subfolder:\n\n“eda.qmd”: Contains code for exploratory data analysis\n\n“analysis-code” subfolder:\n\n“analysis.qmd”: Contains code for modeling analysis\n\n\n“data” folder:\n\n“raw-data” subfolder: Contains the raw COVID-19 Vaccine data\n“processed-data” subfolder: Contains the processed data used in the analysis\n\n“results” folder: Contains all results from the analysis\n\n“figures” subfolder: Contains all figures generated from eda and analysis\n“tables” subfolder: Contains all tables generated from eda and analysis\n\n“assets” folder:\n\nContains workflow schematic image\nContains the CDC U.S. Regions image\nContains american journal of epidemiology reference style and vancouver reference style (.csl files)\n“references” subfolder:\n\n“project-citations.bib”: Contains the references used in the manuscript\n\n\n“products” folder:\n\n“manuscript” subfolder: Contains manuscript.qmd file to create project manuscript\n\n“supplement” subfolder: Contains this file and the supplementary figures and tables\n\n\n\n\n\n\nReproducing Results\nReproducing this project requires R, RStudio, and Microsoft Word. Files should be run in the following order.\n\nIn the code &gt; processing-code folder: processing.qmd\nIn the code &gt; eda-code folder: exploratoryanalysis.qmd\nIn the code &gt; analysis-code folder: analysis.Rmd\nIn the products &gt; manuscript folder: manuscript.qmd\nIn the products &gt; manuscript &gt; supplement folder: Supplementary-Material.qmd\n\n\n\n\nSupplementary Results\nTable one displays a summary of each variable of the COVID-19 vaccine dataset.\n\n\n\n\n\nVariable\nMean\n\n\n\n\nTotal Distributed (All)\n5.667000e+08\n\n\nTotal Distributed – Janssen\n2.550002e+07\n\n\nTotal Distributed – Moderna\n2.141924e+08\n\n\nTotal Distributed – Pfizer\n3.186000e+08\n\n\nTotal Distributed – Novavax\n2.410320e+05\n\n\nTotal Distributed – Unknown\n7.701900e+04\n\n\nTotal Administered (All)\n4.479000e+08\n\n\nTotal Administered – Janssen\n1.504262e+07\n\n\nTotal Administered – Moderna\n1.733713e+08\n\n\nTotal Administered – Pfizer\n2.582345e+08\n\n\nTotal Administered – Novavax\n1.365400e+04\n\n\nTotal Administered – Unknown\n4.439420e+05\n\n\nTotal Distributed per 100k\n4.996000e-01\n\n\nDistributed – Janssen per 100k\n3.090400e+04\n\n\nDistributed – Moderna per 100k\n2.594080e+05\n\n\nDistributed – Pfizer per 100k\n3.873940e+05\n\n\nDistributed – Novavax per 100k\n2.790220e+02\n\n\nDistributed – Unknown per 100k\n9.357000e+01\n\n\nTotal Administered per 100k\n5.490530e+05\n\n\nAdministered – Janssen per 100k\n1.856900e+04\n\n\nAdministered – Moderna per 100k\n2.122234e+05\n\n\nAdministered – Pfizer per 100k\n3.167740e+05\n\n\nAdministered – Novavax per 100k\n1.617800e+01\n\n\nAdministered – Unknown per 100k\n4.892019e+02\n\n\n\nSupplement Table 1: Summary Statistics of the Vaccination Data in Original Dataset and Population Adjusted Dataset\n\n\nFigure one displays the correlations between all variables within the COVID-19 vaccine population adjusted dataset.\n\n\n\n\n\nSupplement Figure 1: Overall Correlation Plot of COVID-19 Vaccine Data\n\n\n\n\nFigure two shows the correlations of specifically the distribution and administration variables in the dataset. The correlation between total_administered and total_distributed is 0.89; the correlation between total_admin_janssen and total_dist_janssen is 0.96; the correlation between total_admin_moderna and total_dist_moderna is 0.87; the correlation between total_admin_pfizer and total_dist_pfizer is 0.90; the correlation between total_admin_novavax and total_dist_novavax is 0.90, which is the lowest of all manufacturers; and the correlation between total_admin_unk and total_dist_unk is -0.01. The distributed and administered unknown variables are likely a part of this dataset due to poor data recording.\n\n\n\n\n\nSupplement Figure 2: Correlation Plot of Distributed vs. Administrated COVID-19 Vaccine Data\n\n\n\n\nBelow, you will several scatterplots. Figure three is an overall scatterplot of the relationship between administered and distributed doses. The points closely follow the diagonal line, indicating a strong relationship between the two variables (confirmed by the correlation coeffecient of 0.89).\n\n\n\n\n\nSupplement Figure 3: Scatterplot of Distributed vs Administered Doses\n\n\n\n\nFigures 4.1, 4.2, 4.3, and 4.2 show the scatterplots of the relationship between administered and distributed doses for each vaccine manufacturer. It appears as though all regions have a strong positive correlation between the number of vaccines distributed and administered. However, Pfizer and Moderna’s points still follow the diagonal line the closest.\n\n\n\n\n\nSupplement Figure 4.1: Scatterplot of Distributed vs Administered Moderna Doses\n\n\n\n\n\n\n\n\n\nSupplement Figure 4.2: Scatterplot of Distributed vs Administered Janssen Doses\n\n\n\n\n\n\n\n\n\nSupplement Figure 4.3: Scatterplot of Distributed vs Administered Pfizer Doses\n\n\n\n\n\n\n\n\n\nSupplement Figure 4.4: Scatterplot of Distributed vs Administered Novavax Doses\n\n\n\n\nFigures 5.1, 5.2, 5.3, and 5.4 show the scatterplots of the relationship between administered and distributed doses for each region of the U.S..\n\n\n\n\n\nSupplement Figure 5.1: Scatterplot of Distributed vs Administered Doses in the South\n\n\n\n\n\n\n\n\n\nSupplement Figure 5.2: Scatterplot of Distributed vs Administered Doses in the Northeast\n\n\n\n\n\n\n\n\n\nSupplement Figure 5.3: Scatterplot of Distributed vs Administered Doses in the Midwest\n\n\n\n\n\n\n\n\n\nSupplement Figure 5.4: Scatterplot of Distributed vs Administered Doses in the West\n\n\n\n\nTable two shows the correlations between doses administered and distributed in each region of the U.S.. All four regions have high correlations, however the west is the highest.\n\n\n\n\n\nRegion\ncor\n\n\n\n\nMidwest\n0.9980132\n\n\nNortheast\n0.9983011\n\n\nSouth\n0.9987066\n\n\nWest\n0.9990827\n\n\n\nSupplement Table 2: Regional Correlations Between Doses Administered and Distributed in Pouplation Adjusted Dataset\n\n\nTable three shows the percent rate change in distribution and administration of the COVID-19 vaccine across time in each region of the U.S.. The drops depicted in figures 3 and 4 in the manuscript can be seen in this table by the large percent drops in doses administered and distributed.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nRegion\ntotal_distributed\ntotal_administered\npct_change_distributed\npct_change_administered\n\n\n\n\n2021\nMidwest\n35652869\n29311185\nNA\nNA\n\n\n2021\nNortheast\n40435158\n34364697\nNA\nNA\n\n\n2021\nSouth\n36233233\n28111986\nNA\nNA\n\n\n2021\nWest\n38700528\n32296475\nNA\nNA\n\n\n2022\nMidwest\n39811314\n31061255\n11.663702\n5.970653\n\n\n2022\nNortheast\n46080990\n37372257\n13.962682\n8.751889\n\n\n2022\nSouth\n39742533\n29711380\n9.685308\n5.689365\n\n\n2022\nWest\n42680522\n34861661\n10.284083\n7.942619\n\n\n2023\nMidwest\n5306961\n3600733\n-86.669716\n-88.407638\n\n\n2023\nNortheast\n6028417\n4298397\n-86.917780\n-88.498429\n\n\n2023\nSouth\n5005472\n3294600\n-87.405250\n-88.911320\n\n\n2023\nWest\n5557185\n4045517\n-86.979576\n-88.395514\n\n\n\nSupplement Table 3: Percent Rate Change in Distribution and Administration of COVID-19 Vaccine with time in each Region (in Population Adjusted Dataset)\n\n\nTable 4.1 and 4.2 show the percent rate change in distribution (4.1) and administration (4.2) of the COVID-19 vaccine across time for each manufacturer. The drops depicted in figures 5 and 6 in the manuscript can be seen in this table by the large percent drops in doses administered and distributed.\n\n\n\n\n\nManufacturer\nYear\nTotal Doses Distributed\nRate of Change\n\n\n\n\nJanssen\n2021\n7485221.42\n-\n\n\nJanssen\n2022\n7131071.61\n-4.73%\n\n\nJanssen\n2023\n712227.99\n-90.01%\n\n\nModerna\n2021\n59085477.92\n-\n\n\nModerna\n2022\n62011953.91\n4.95%\n\n\nModerna\n2023\n7569083.75\n-87.79%\n\n\nNovavax\n2021\n0.00\n-\n\n\nNovavax\n2022\n19611.74\nInf%\n\n\nNovavax\n2023\n26147.80\n33.33%\n\n\nPfizer\n2021\n80496466.34\n-\n\n\nPfizer\n2022\n99150229.37\n23.17%\n\n\nPfizer\n2023\n12500508.65\n-87.39%\n\n\n\nSupplement Table 4.1: Percent Rate Change in Distribution of COVID-19 Vaccine with time for each Manufacturer (in Population Adjusted Dataset)\n\n\n\n\n\n\n\nManufacturer\nYear\nTotal Doses Administered\nRate of Change\n\n\n\n\nJanssen\n2021\n4.433968e+06\n-\n\n\nJanssen\n2022\n4.345841e+06\n-1.99%\n\n\nJanssen\n2023\n4.303261e+05\n-90.1%\n\n\nModerna\n2021\n4.966932e+07\n-\n\n\nModerna\n2022\n5.007688e+07\n0.82%\n\n\nModerna\n2023\n5.516585e+06\n-88.98%\n\n\nNovavax\n2021\n0.000000e+00\n-\n\n\nNovavax\n2022\n9.065808e+02\nInf%\n\n\nNovavax\n2023\n1.746609e+03\n92.66%\n\n\nPfizer\n2021\n6.986818e+07\n-\n\n\nPfizer\n2022\n7.846953e+07\n12.31%\n\n\nPfizer\n2023\n8.782378e+06\n-88.81%\n\n\n\nSupplement Table 4.2: Percent Rate Change in Administration of COVID-19 Vaccine with time for each Manufacturer (in Population Adjusted Dataset)\n\n\nTable 5 shows the RMSE value of the null model (with no predictors) from the modeling analysis. The R-squared can be assumed to be 0 here.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.1547925\n\n\nmae\nstandard\n0.0864073\n\n\nrsq\nstandard\nNA\n\n\n\nSupplement Table 5: Null Model RMSE\n\n\nTable 6 shows the Simple Linear Regression Metrics results from the modeling analysis.\n\n\n\n\n\nSupplement Table 6: Simple Linear Regression Metrics\n\n\n\n\nAn additional model (the random forest model including all predictors) was fitted to the test data out of curiosity. The metrics are shown in Table 7.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n0.0132909\n\n\nrsq\nstandard\n0.9930341\n\n\nmae\nstandard\n0.0052497\n\n\n\nSupplement Table 7: Original/All Predictors Random Forest Model Test Data Metrics\n\n\nFigure 6 displays the corresponding observed vs predicted plot for this model when it was fitted to the test data.\n\n\n\n\n\nSupplement Figure 6: All Predictors Random Forest Model Observed vs Predicted (Test Data Fit)"
  }
]