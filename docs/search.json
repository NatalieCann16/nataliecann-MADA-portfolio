[
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          6     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  4     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Hair Color            0             1 FALSE          5\n  top_counts                  \n1 M: 4, F: 3, O: 2            \n2 BLO: 3, BRU: 3, BLA: 1, O: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate    mean     sd  p0 p25 p50 p75 p100\n1 Height                0             1 166.    16.0   133 156 166 178  183\n2 Weight                0             1  70.1   21.2    45  55  70  80  110\n3 Age                   0             1  29.6    9.51   16  24  29  37   43\n4 age_cat               0             1   0.667  0.866   0   0   0   1    2\n  hist \n1 ▂▁▃▃▇\n2 ▇▂▃▂▂\n3 ▅▂▇▂▅\n4 ▇▁▃▁▃\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + \n  geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + \n  geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + \n  geom_point() + \n  geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + # this is the model, color= stratify the analysis by gender \n  geom_point() + #make point in the plot \n  geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nMuhammad’s Boxplot:\n\nplot_boxplot &lt;- ggplot(data = mydata, aes(x = Gender, y = Height, fill = Gender)) +\n\n  geom_boxplot() +\n\n  scale_fill_manual(values = c(\"blue\", \"pink\", \"green\"))\n\nplot_boxplot\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"boxplot-gender-height.png\")\n\nggsave(filename = figure_file, plot=plot_boxplot)\n\nSaving 7 x 5 in image\n\n\nMuhammad’s Scatterplot:\n\nplot_scatter &lt;- ggplot(data = mydata, aes(x = Weight, y = Height)) +\n\n  geom_point() +\n\n  labs(\n\n    title = \"Scatter Plot of Weight vs. Height\",\n\n    x = \"Weight\",\n\n    y = \"Height\"\n\n  ) +\n\n  theme_minimal() \n\nplot_scatter\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Scatterplot-Height-Weight.png\")\n\nggsave(filename = figure_file, plot=plot_scatter)\n\nSaving 7 x 5 in image\n\n\nMuhammad’s QQ-Plot: I create QQ-Plot for height Note from Natalie: “d4” was not being recognized here, so I noticed you assigned d4 to “processeddata2”. As you can see “processeddata2” was assigned to “mydata” previously in this document. So “mydata” needs to be used here.\n\np5 &lt;- ggplot(mydata, aes(sample = Height)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot for Height\", x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\nplot(p5)\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"QQ-Plot.jpg\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Hi, everyone! :)\n\nNatalie Cann | MADA Spring 2025\nWelcome to my website and data analysis portfolio.\n\nPlease use the Menu Bar above to look around."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "Placeholder file for the future R coding exercise."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "href": "coding-exercise/coding-exercise.html#loading-and-checking-data",
    "title": "R Coding Exercise",
    "section": "Loading and Checking Data",
    "text": "Loading and Checking Data\nI will now install the dslabs package. Then, I will load the package.\n\noptions(repos = c(CRAN = \"https://cran.rstudio.com/\")) # including this because I could not render the website without it\ninstall.packages(\"dslabs\")\n\n\nThe downloaded binary packages are in\n    /var/folders/n7/2hn_592d3db_b95vx1v5wcjr0000gn/T//Rtmpi8Ltvb/downloaded_packages\n\nlibrary(dslabs)\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nI will now use the “help()” function on the “gapminder” data.\n\nhelp(gapminder)\n\nFrom this, I learned that the gapminder data contains outcomes related to health and income for 184 countries between 1960 and 2016. oecd = OECD countries from 2016 opec = OPEC countries form 2016\nNow, I will use str() on the gapminder data.\n\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n\nAbove, I can see that the gapminder data frame contains 9 variables and 10,545 observations. The variables are: country, year, infant_mortality, life_expectancy, fertility, population, gdp, continent, and region. Some of the variables are numerics or intergers, while others are factors (country, continent, and region).\nI will use summary() on the gapminder data.\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n\nThe summary above displays the countries, continents, and regions. It also shows the minimum, 1st quartile, median, mean, 3rd quartile and maximum values for the numeric/integer variables.\nNow, I will determine the type of object/class that gapminder is.\n\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\nThis output states that gapminder is a data frame."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#processing-data",
    "href": "coding-exercise/coding-exercise.html#processing-data",
    "title": "R Coding Exercise",
    "section": "Processing Data",
    "text": "Processing Data\nNow, I will filter the gapminder data to obtain only the countries in Africa. I will assign this to a variable called “africadata”.\n\nafricadata &lt;- filter(gapminder, continent == \"Africa\")\n\nI will run str() on “africadata”.\n\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\n\nAs you can see, there are still 9 variables (each of the same data class as before). However, there are fewer observations (2907) than before (10,545) - because we are only included the African countries here.\nI will run summary() on “africadata”.\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0  \n\n\nThis summary is similar to the one created for the entire gapminder data frame, however the values above are now different for the numeric/integer variables since we are only looking at the African countries.\nNow, I will create a new object that contains only the infant_mortality AND life_expectancy from the “africadata” data frame.\n\ninfantmort_lifeexp &lt;- select(africadata, infant_mortality, life_expectancy)\n\nI will use str() and summary() to take a look at the infantmort_lifeexp data.\n\nstr(infantmort_lifeexp)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(infantmort_lifeexp)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\n\nThese outputs show that the only two variables are infant_mortality and life_expectancy. Then the minimum, 1st quartile, median, mean, 3rd quartile, and maximum are displayed for infant_mortality and life_expectancy.\nThen, I will create a new object containing only population AND life-expectancy from the “africadata” data frame.\n\npop_lifeexp &lt;- select(africadata, population, life_expectancy)\n\nI will use str() and summary() to take a look at the pop_lifeexp data.\n\nstr(pop_lifeexp)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\nsummary(pop_lifeexp)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51                         \n\n\nThese outputs show that the only two variables are population and life_expectancy. Then the minimum, 1st quartile, median, mean, 3rd quartile, and maximum are displayed for population and life_expectancy."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#plotting",
    "href": "coding-exercise/coding-exercise.html#plotting",
    "title": "R Coding Exercise",
    "section": "Plotting",
    "text": "Plotting\nI will now create a plot of life_expectancy as a function of infant_mortality.\n\nggplot(infantmort_lifeexp, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color = \"#5fafdc\") + # I will specify the points to be blue \n  labs(title = \"Life Expectancy as a Function of Infant \\n Mortality in African Countries\",\n       x = \"Infant Mortality (per 1,000)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold \n\nWarning: Removed 226 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nAs you can see from the plot above, there appears to be a negative relationship between infant mortality and life expectancy. This makes sense, because if there is higher infant mortality in a country, their overall life expectancy should be lower.\nI will now create a plot of life_expectancy as a function of population (size). The population (x-axis) will be set to a log scale.\n\nggplot(pop_lifeexp, aes(x = population, y = life_expectancy)) +\n  geom_point(color = \"#70dc5f\") + # I will specify the points to be green  scale_x_log10() + \n  scale_x_log10() + # I will set the x-axis to a log scale\n  labs(title = \"Life Expectancy as a Function of Population Size \\n (Log Scale) in African Countries\",\n       x = \"Population (Log Scale)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold\n\nWarning: Removed 51 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nThe plot above appears to have individual streaks which have a positive relationship between population size and life expectancy. However, as a whole the data appears to be relatively scattered.\nThere appear to be “streaks” of data in both plots above. This is because we are measuring individual countries over several different years."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-data-processing",
    "href": "coding-exercise/coding-exercise.html#more-data-processing",
    "title": "R Coding Exercise",
    "section": "More Data Processing",
    "text": "More Data Processing\nI will find out which years contain NAs.\n\nyears_with_na_infant_mortality &lt;- unique(africadata$year[is.na(africadata$infant_mortality)])\n# use unique() and is.na() to find out which years contain NAs in infant_mortality\n\nprint(years_with_na_infant_mortality)\n\n [1] 1960 1961 1962 1963 1964 1965 1966 1967 1968 1969 1970 1971 1972 1973 1974\n[16] 1975 1976 1977 1978 1979 1980 1981 2016\n\n# Use print() to view the output\n\nAs you can see, the years 1960-1981 and 2016 contain NA values within the infant_mortality variable.\nWe will use the year 2000’s infant mortality data since it contains no missing values. I will create a new object that contains only data for the year 2000 from the “africadata” data frame.\n\nafricadata_2000 &lt;- filter(africadata, year == 2000)\n\nI will check that this worked with str() and summary().\n\nstr(africadata_2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata_2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0  \n\n\nThis worked, as you can see all the years appear as “2000”."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#more-plotting",
    "href": "coding-exercise/coding-exercise.html#more-plotting",
    "title": "R Coding Exercise",
    "section": "More Plotting",
    "text": "More Plotting\nI will make the same plots as before, but this time with the “africadata_2000” data.\nI will create a plot of life_expectancy as a function of infant_mortality.\n\nggplot(africadata_2000, aes(x = infant_mortality, y = life_expectancy)) +\n  geom_point(color = \"#f576c1\") + # I will specify the points to be pink \n  labs(title = \"Life Expectancy as a Function of Infant Mortality \\n in African Countries in Year 2000\",\n       x = \"Infant Mortality (per 1,000)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold \n\n\n\n\n\n\n\n\nYou can see that the plot above has fewer data points that the previous plot because only data from the year 2000 for each country is included.\nI will now create a plot of life_expectancy as a function of population (size). The population (x-axis) will be set to a log scale.\n\nggplot(africadata_2000, aes(x = population, y = life_expectancy)) +\n  geom_point(color = \"#f7af32\") + # I will specify the points to be orange  \n  scale_x_log10() + # I will set the x-axis to a log scale\n  labs(title = \"Life Expectancy as a Function of Population Size \\n (Log Scale) in African Countries in Year 2000\",\n       x = \"Population (Log Scale)\",\n       y = \"Life Expectancy (years)\") + # I will specify the title and the names of the x and y axes\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # I will remove a legend (since it is unneeded here) and make the title and axis labels larger and bold\n\n\n\n\n\n\n\n\nThe plot above also has fewer data points than the previous plot because it contain contains a data point from the year 2000 for each country."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#simple-model-fits",
    "href": "coding-exercise/coding-exercise.html#simple-model-fits",
    "title": "R Coding Exercise",
    "section": "Simple Model Fits",
    "text": "Simple Model Fits\nI will now use the lm() function to fit a linear model with life_expectancy as the outcome and infant_mortality as the predictor. Then, I will apply the summary() function.\n\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africadata_2000)\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africadata_2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\n\nThe intercept (a) is 71.29331; this means that the average life expectancy is 71.29331 years when infant mortality is 0. The slope (b) is -0.18916; this means that for every 1 unit increase in infant mortality, life expectancy decreases by 0.18916 years.\nI will now use the lm() function to fit a linear model with life_expectancy as the outcome and population size as the predictor. Then, I will apply the summary() function.\n\nfit2 &lt;- lm(life_expectancy ~ population, data = africadata_2000)\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africadata_2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\n\n\nThe intercept (a) is 5.593e+01; this means that the average life expectancy is 55.93 years when the population size is equal to 0. The slope (b) is 2.756e-08; this means that for every 1 unit increase in population size, life expectancy increases by 2.756e-08 years. This intrepretation does not make sense - because if the population size is 0, the average life expectancy should be non-existent.\nThe p-value for “fit1” is 2.826e-08, while the p-value for “fit2” is 0.6159. 2.825e-08 is less than 0.05, therefore, the null hypothesis that there is no relationship between infant mortality and life expectancy can be rejected. 0.6159 is greater than 0.05, the null hypothesis that there is no relationship between population size and life expectancy cannot be rejected (we fail to reject). With this information, it can be said that infant mortality is a better predictor of life expectancy than population size. Therefore, “fit1” should be used."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#part-2",
    "href": "coding-exercise/coding-exercise.html#part-2",
    "title": "R Coding Exercise",
    "section": "Part 2",
    "text": "Part 2\nThis section is contributed by Yufei Wu."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#dataset-exploration",
    "href": "coding-exercise/coding-exercise.html#dataset-exploration",
    "title": "R Coding Exercise",
    "section": "Dataset exploration",
    "text": "Dataset exploration\nI choose the dataset “mice_weights” which contains body weights, bone density, and percent fat for mice under two diets: chow and high fat.\n\n# load package\nlibrary(dslabs)\n#look at help file for mice_weights data\nhelp(mice_weights)\n#get an overview of data structure\nstr(mice_weights)\n\n'data.frame':   780 obs. of  7 variables:\n $ body_weight : num  27.6 23 28.7 32.6 28.6 ...\n $ bone_density: num  0.616 0.769 0.684 0.644 0.53 ...\n $ percent_fat : num  7.26 4.95 6.02 9.54 6.99 ...\n $ sex         : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ diet        : Factor w/ 2 levels \"chow\",\"hf\": 1 1 1 1 1 1 1 1 1 1 ...\n $ gen         : Factor w/ 5 levels \"4\",\"7\",\"8\",\"9\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ litter      : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n\n#get a summary of data\nsummary(mice_weights)\n\n  body_weight     bone_density     percent_fat     sex       diet     gen     \n Min.   :18.13   Min.   :0.2708   Min.   : 2.552   F:398   chow:394   4 : 97  \n 1st Qu.:28.09   1st Qu.:0.4888   1st Qu.: 5.566   M:382   hf  :386   7 :195  \n Median :32.98   Median :0.5643   Median : 8.276                      8 :193  \n Mean   :34.08   Mean   :0.5697   Mean   : 8.594                      9 : 97  \n 3rd Qu.:39.37   3rd Qu.:0.6373   3rd Qu.:10.926                      11:198  \n Max.   :65.15   Max.   :0.9980   Max.   :22.154                              \n                 NA's   :4        NA's   :4                                   \n litter \n 1:442  \n 2:338  \n        \n        \n        \n        \n        \n\n#determine the type of object mice_weights is\nclass(mice_weights)\n\n[1] \"data.frame\"\n\n#get the overview of the data\nhead(mice_weights)\n\n  body_weight bone_density percent_fat sex diet gen litter\n1       27.60    0.6163850    7.255468   F chow   4      1\n2       23.03    0.7693496    4.951037   F chow   4      1\n3       28.72    0.6842564    6.020849   F chow   4      1\n4       32.57    0.6436947    9.536251   F chow   4      1\n5       28.61    0.5297713    6.987331   F chow   4      1\n6       28.16    0.5649217    6.767774   F chow   4      1"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#data-processing",
    "href": "coding-exercise/coding-exercise.html#data-processing",
    "title": "R Coding Exercise",
    "section": "Data processing",
    "text": "Data processing\nCreat a new data frame containing only famale rats.\n\n#assign only the female rat to a new object called femalerat\nfemalerat &lt;- mice_weights[mice_weights$sex == \"F\", ]\n\n#check the structure\nstr(femalerat)\n\n'data.frame':   398 obs. of  7 variables:\n $ body_weight : num  27.6 23 28.7 32.6 28.6 ...\n $ bone_density: num  0.616 0.769 0.684 0.644 0.53 ...\n $ percent_fat : num  7.26 4.95 6.02 9.54 6.99 ...\n $ sex         : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 1 1 1 1 1 1 ...\n $ diet        : Factor w/ 2 levels \"chow\",\"hf\": 1 1 1 1 1 1 1 1 1 1 ...\n $ gen         : Factor w/ 5 levels \"4\",\"7\",\"8\",\"9\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ litter      : Factor w/ 2 levels \"1\",\"2\": 1 1 1 1 1 1 1 1 1 1 ...\n\n#check the summary\nsummary(femalerat)\n\n  body_weight     bone_density     percent_fat     sex       diet     gen     \n Min.   :18.13   Min.   :0.2708   Min.   : 2.766   F:398   chow:200   4 : 50  \n 1st Qu.:25.35   1st Qu.:0.4531   1st Qu.: 5.690   M:  0   hf  :198   7 : 98  \n Median :28.71   Median :0.5090   Median : 8.181                      8 :100  \n Mean   :29.76   Mean   :0.5186   Mean   : 8.683                      9 : 50  \n 3rd Qu.:33.21   3rd Qu.:0.5793   3rd Qu.:11.017                      11:100  \n Max.   :65.15   Max.   :0.8519   Max.   :22.154                              \n                 NA's   :1        NA's   :1                                   \n litter \n 1:224  \n 2:174  \n        \n        \n        \n        \n        \n\n#get the overview of the data\nhead(femalerat)\n\n  body_weight bone_density percent_fat sex diet gen litter\n1       27.60    0.6163850    7.255468   F chow   4      1\n2       23.03    0.7693496    4.951037   F chow   4      1\n3       28.72    0.6842564    6.020849   F chow   4      1\n4       32.57    0.6436947    9.536251   F chow   4      1\n5       28.61    0.5297713    6.987331   F chow   4      1\n6       28.16    0.5649217    6.767774   F chow   4      1"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#figures-and-tables",
    "href": "coding-exercise/coding-exercise.html#figures-and-tables",
    "title": "R Coding Exercise",
    "section": "Figures and tables",
    "text": "Figures and tables\n\n#load package\nlibrary(ggplot2)\nlibrary(dplyr)\n\n#make plots of body weight vs body density, coloring by diet\nggplot(femalerat, aes(x = body_weight, y = bone_density, color = diet)) +\n  geom_point() +\n  labs(\n    title = \"Relationship between body density and body weight\",\n    x = \"Body weight\",\n    y = \"Body density\",\n    color = \"Diet\")\n\n\n\n\n\n\n\n#make plots of body weight vs percent fat, coloring by diet\nggplot(femalerat, aes(x = body_weight, y = percent_fat, color = diet)) +\n  geom_point() +\n  labs(\n    title = \"Relationship between percent fat and body weight\",\n    x = \"Body weight\",\n    y = \"Percent fat\",\n    color = \"Diet\")\n\n\n\n\n\n\n\n#summarize the mean and standard deviation of body weight grouped by diet\nsummary_table &lt;- femalerat %&gt;%\n  group_by(diet) %&gt;%\n  summarise(\n    mean_weight = mean(body_weight, na.rm = TRUE),\n    sd_weight = sd(body_weight, na.rm = TRUE),\n    mean_percent_fat = mean(percent_fat, na.rm = TRUE),\n    sd_percent_fat = sd(percent_fat, na.rm = TRUE),\n    count = n()\n  )\nprint(summary_table)\n\n# A tibble: 2 × 6\n  diet  mean_weight sd_weight mean_percent_fat sd_percent_fat count\n  &lt;fct&gt;       &lt;dbl&gt;     &lt;dbl&gt;            &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n1 chow         27.8      4.82             6.86           2.61   200\n2 hf           31.7      7.06            10.5            3.75   198\n\n\nFrom the first figure, we cannot see a clear relationship between body density and body weight. From the second figure, we can see that there is a positive relationship between percent fat and body weight. Besides, there are more points from high fat group locating in higher percent fat. From the table, we can see that the body weight and percent fat are both higher in the high fat group than chow group, which can be further explored."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#statistical-model",
    "href": "coding-exercise/coding-exercise.html#statistical-model",
    "title": "R Coding Exercise",
    "section": "Statistical model",
    "text": "Statistical model\nSince I noticed that body weight and percent fat are both higher in the high fat group than chow group, I will test if the differences are statistically significant.\n\n#run t-test for body weight comparison\nt_test_weight &lt;- t.test(body_weight ~ diet, data = femalerat)\nprint(t_test_weight)\n\n\n    Welch Two Sample t-test\n\ndata:  body_weight by diet\nt = -6.3936, df = 347.4, p-value = 5.224e-10\nalternative hypothesis: true difference in means between group chow and group hf is not equal to 0\n95 percent confidence interval:\n -5.073552 -2.686420\nsample estimates:\nmean in group chow   mean in group hf \n          27.82860           31.70859 \n\n#run t-test for percent fat comparison\nt_test_fat &lt;- t.test(percent_fat ~ diet, data = femalerat)\nprint(t_test_fat)\n\n\n    Welch Two Sample t-test\n\ndata:  percent_fat by diet\nt = -11.316, df = 349.56, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group chow and group hf is not equal to 0\n95 percent confidence interval:\n -4.310618 -3.034073\nsample estimates:\nmean in group chow   mean in group hf \n          6.860235          10.532581"
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#report-the-results",
    "href": "coding-exercise/coding-exercise.html#report-the-results",
    "title": "R Coding Exercise",
    "section": "Report the results",
    "text": "Report the results\nFor the body weight comparison, the difference between high fat group and chow group is 3.88, p value is less than 0.05, so the mean body weight differs significantly between the high Fat and chow groups. For the percent fat comparison, the difference between high fat group and chow group is 3.67, p value is less than 0.05, so the mean percent fat differs significantly between the high Fat and chow groups."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "I chose to complete option 2: generating a synthetic dataset. Note: I will create data with a simple “rectangular structure.”"
  },
  {
    "objectID": "data-exercise/data-exercise.html#loading-packages",
    "href": "data-exercise/data-exercise.html#loading-packages",
    "title": "Data Exercise",
    "section": "Loading Packages",
    "text": "Loading Packages\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\nlibrary(ggplot2)\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(corrplot)\n\ncorrplot 0.95 loaded\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows"
  },
  {
    "objectID": "data-exercise/data-exercise.html#setting-seed",
    "href": "data-exercise/data-exercise.html#setting-seed",
    "title": "Data Exercise",
    "section": "Setting Seed",
    "text": "Setting Seed\nI will set a seed so that the synthetic dataset will be reproducible with the code I create in this activity.\n\n# Set seed \nset.seed(116)\n# Assign how many patients/observations I want to generate\nn_patients &lt;- 100 # I will generate only 100 patients for this exercise since this is the first time I am creating a synthetic dataset and I want to keep things simple!"
  },
  {
    "objectID": "data-exercise/data-exercise.html#generating-data",
    "href": "data-exercise/data-exercise.html#generating-data",
    "title": "Data Exercise",
    "section": "Generating Data",
    "text": "Generating Data\nNow, I will generate my synthetic data set. I will create a data frame with fake data with several measures and risk factors. I will add correlations between some of the variables.\n\n# Create an empty data frame with placeholders for variables\nsynthetic_data &lt;- data.frame(\n  Patient_ID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = integer(n_patients),\n  Enrollment_Date = lubridate::as_date(character(n_patients)),\n  Height = numeric(n_patients),\n  Weight = numeric(n_patients),\n  Blood_Pressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  Diabetes = integer(n_patients),\n  Smoking = integer(n_patients))\n\n# Variable 1: Patient ID\nsynthetic_data$Patient_ID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\nsynthetic_data$Age &lt;- round(runif(n_patients, min = 18, max = 90), 1)\n\n# Variable 3: Gender (binary variable; 0 = Male, 1 = Female)\nsynthetic_data$Gender &lt;- as.numeric(sample(c(0, 1), n_patients, replace = TRUE))\n\n# Variable 4: Date of Enrollment (date variable)\nsynthetic_data$Enrollment_Date &lt;- lubridate::as_date(sample(seq(from = lubridate::as_date(\"2022-01-01\"), to = lubridate::as_date(\"2022-12-31\"), by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 5: Height (numeric variable; in inches)\nsynthetic_data$Height &lt;- round(runif(n_patients, min = 57, max = 78), 1)\n\n# Variable 6: Weight (numeric variable; in lbs; dependent on Height) \nsynthetic_data$Weight &lt;- ifelse(\n  synthetic_data$Height &gt;= 57 & synthetic_data$Height &lt;= 62, \n  round(rnorm(sum(synthetic_data$Height &gt;= 57 & synthetic_data$Height &lt;= 62), mean = 120, sd = 10), 1),ifelse(synthetic_data$Height &gt; 62 & synthetic_data$Height &lt;= 70, round(rnorm(sum(synthetic_data$Height &gt; 62 & synthetic_data$Height &lt;= 70), mean = 160, sd = 15), 1),\n    round(rnorm(sum(synthetic_data$Height &gt; 70 & synthetic_data$Height &lt;= 78), mean = 190, sd = 20), 1)))\n\n# Variable 7: Blood Pressure (numeric variable)\nsynthetic_data$Blood_Pressure &lt;- round(runif(n_patients, min = 90, max = 160), 1)\n\n# Variable 8: Cholesterol Level (numeric variable; in mg/dL; dependent on Weight)\nsynthetic_data$Cholesterol &lt;- ifelse(synthetic_data$Weight &gt;= 70 & synthetic_data$Weight &lt;= 130, round(rnorm(sum(synthetic_data$Weight &gt;= 70 & synthetic_data$Weight &lt;= 130), mean = 160, sd = 10), 1), ifelse(synthetic_data$Weight &gt;= 131 & synthetic_data$Weight &lt;= 180, round(rnorm(sum(synthetic_data$Weight &gt;= 131 & synthetic_data$Weight &lt;= 180), mean = 185, sd = 10), 1), round(rnorm(sum(synthetic_data$Weight &gt;= 181 & synthetic_data$Weight &lt;= 200), mean = 210, sd = 10), 1)))\n\n# Variable 9: Diabetes (binary variable; 0 = Not Diabetic, 1 = Diabetic)\nsynthetic_data$Diabetes &lt;- as.numeric(sample(c(0, 1), n_patients, replace = TRUE))\n\n# Variable 10: Smoking (binary variable; 0 = Does Not Smoke, 1 = Smokes)\nsynthetic_data$Smoking &lt;- as.numeric(sample(c(0, 1), n_patients, replace = TRUE))\n\n# Print the first few rows of the generated data\nhead(synthetic_data)\n\n  Patient_ID  Age Gender Enrollment_Date Height Weight Blood_Pressure\n1          1 71.3      0      2022-11-12   68.6  170.6           96.0\n2          2 42.2      0      2022-12-16   69.0  168.9          125.8\n3          3 31.9      1      2022-11-20   65.2  171.2          156.6\n4          4 38.2      0      2022-10-05   78.0  155.7          109.0\n5          5 89.8      1      2022-03-31   77.1  183.8          100.8\n6          6 56.5      0      2022-06-23   62.1  174.1           95.2\n  Cholesterol Diabetes Smoking\n1       177.0        1       0\n2       184.7        1       0\n3       192.5        1       0\n4       194.5        0       0\n5       205.4        0       0\n6       182.9        0       0\n\n\nLuckily from the head() output, I can see that this worked!"
  },
  {
    "objectID": "data-exercise/data-exercise.html#exploring-data-structure",
    "href": "data-exercise/data-exercise.html#exploring-data-structure",
    "title": "Data Exercise",
    "section": "Exploring Data Structure",
    "text": "Exploring Data Structure\nNow, I will explore the synthetic dataset I just created. I will do this with the summary(), str(), and glimpse.\n\nsummary(synthetic_data)\n\n   Patient_ID          Age            Gender    Enrollment_Date     \n Min.   :  1.00   Min.   :18.20   Min.   :0.0   Min.   :2022-01-06  \n 1st Qu.: 25.75   1st Qu.:33.95   1st Qu.:0.0   1st Qu.:2022-04-18  \n Median : 50.50   Median :57.00   Median :0.5   Median :2022-07-18  \n Mean   : 50.50   Mean   :55.38   Mean   :0.5   Mean   :2022-07-18  \n 3rd Qu.: 75.25   3rd Qu.:71.38   3rd Qu.:1.0   3rd Qu.:2022-10-25  \n Max.   :100.00   Max.   :89.90   Max.   :1.0   Max.   :2022-12-27  \n     Height          Weight      Blood_Pressure   Cholesterol       Diabetes  \n Min.   :57.20   Min.   : 93.7   Min.   : 90.1   Min.   :129.3   Min.   :0.0  \n 1st Qu.:62.02   1st Qu.:132.0   1st Qu.:107.2   1st Qu.:170.2   1st Qu.:0.0  \n Median :68.00   Median :160.9   Median :120.0   Median :182.4   Median :0.5  \n Mean   :67.55   Mean   :159.1   Mean   :123.4   Mean   :183.3   Mean   :0.5  \n 3rd Qu.:72.97   3rd Qu.:179.2   3rd Qu.:139.9   3rd Qu.:195.1   3rd Qu.:1.0  \n Max.   :78.00   Max.   :228.6   Max.   :158.9   Max.   :217.9   Max.   :1.0  \n    Smoking    \n Min.   :0.00  \n 1st Qu.:0.00  \n Median :0.00  \n Mean   :0.48  \n 3rd Qu.:1.00  \n Max.   :1.00  \n\nstr(synthetic_data)\n\n'data.frame':   100 obs. of  10 variables:\n $ Patient_ID     : int  1 2 3 4 5 6 7 8 9 10 ...\n $ Age            : num  71.3 42.2 31.9 38.2 89.8 56.5 87 18.2 48.1 67.3 ...\n $ Gender         : num  0 0 1 0 1 0 0 1 1 1 ...\n $ Enrollment_Date: Date, format: \"2022-11-12\" \"2022-12-16\" ...\n $ Height         : num  68.6 69 65.2 78 77.1 62.1 62.1 71.5 69.5 68.4 ...\n $ Weight         : num  171 169 171 156 184 ...\n $ Blood_Pressure : num  96 126 157 109 101 ...\n $ Cholesterol    : num  177 185 192 194 205 ...\n $ Diabetes       : num  1 1 1 0 0 0 0 0 1 1 ...\n $ Smoking        : num  0 0 0 0 0 0 0 0 1 0 ...\n\nglimpse(synthetic_data)\n\nRows: 100\nColumns: 10\n$ Patient_ID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,…\n$ Age             &lt;dbl&gt; 71.3, 42.2, 31.9, 38.2, 89.8, 56.5, 87.0, 18.2, 48.1, …\n$ Gender          &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, …\n$ Enrollment_Date &lt;date&gt; 2022-11-12, 2022-12-16, 2022-11-20, 2022-10-05, 2022-…\n$ Height          &lt;dbl&gt; 68.6, 69.0, 65.2, 78.0, 77.1, 62.1, 62.1, 71.5, 69.5, …\n$ Weight          &lt;dbl&gt; 170.6, 168.9, 171.2, 155.7, 183.8, 174.1, 175.4, 182.6…\n$ Blood_Pressure  &lt;dbl&gt; 96.0, 125.8, 156.6, 109.0, 100.8, 95.2, 155.1, 99.9, 1…\n$ Cholesterol     &lt;dbl&gt; 177.0, 184.7, 192.5, 194.5, 205.4, 182.9, 163.2, 198.9…\n$ Diabetes        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, …\n$ Smoking         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, …\n\n\nThe summary output shows the minimum, 1st quartile, median, mean, 3rd quartile, and maximum values for each variable. This data is more useful for the continuous variables, such as Height, Weight, and Cholesterol. The str output shows the class of each variable. The glimpse output shows the first few values of each variable."
  },
  {
    "objectID": "data-exercise/data-exercise.html#plots-tables-correlations",
    "href": "data-exercise/data-exercise.html#plots-tables-correlations",
    "title": "Data Exercise",
    "section": "Plots, Tables, Correlations",
    "text": "Plots, Tables, Correlations\n\nScatterplot and Correlation of Height and Weight\nSince I created the weight variable to be depenent on height, I will create a scatterplot to examine the reationship between these two.\n\nggplot(synthetic_data, aes(x = Weight, y = Height)) +\n  geom_point(color = \"#3067c2\") +\n  labs(title = \"Scatterplot of Weight and Height\",\n       x = \"Weight (lbs)\",\n       y = \"Height (inches)\") + \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nThe scaterplot above depicts a positive relationship between weight and height. This means that we see an increase in weight as height increases.\nNow, I will create code to assess the strength of the correlation between weight and height using the cor() function.\n\ncor(synthetic_data$Weight, synthetic_data$Height)\n\n[1] 0.709389\n\n\nThe correlation between weight and height is 0.750217, which is pretty high. This indicates that there is a relatively strong relationship between weight and height.\n\n\nScatterplot and Correlation of Weight and Cholesterol Level\nSince I created the cholesterol variable to be depenent on weight, I will create a scatterplot to examine the reationship between these two.\n\nggplot(synthetic_data, aes(x = Weight, y = Cholesterol)) +\n  geom_point(color = \"#63c230\") +\n  labs(title = \"Scatterplot of Weight and Cholesterol Level\",\n       x = \"Weight (lbs)\",\n       y = \"Cholesterol Level (mg/dL)\") + \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nThe scaterplot above reveals that there is a positive relationship between weight and cholesterol level. This means that as weight increases, cholesterol level also increases.\nNow, I will create code to assess the strength of the correlation between weight and cholesterol level by using the cor() function.\n\ncor(synthetic_data$Weight, synthetic_data$Cholesterol)\n\n[1] 0.8155417\n\n\nThe correlation between weight and cholesterol level is 0.7709426, which is pretty high. This indicates that there is a relatively strong relationship between weight and cholesterol level.\n\n\nScatterplot and Correlation of Height and Cholesterol Level\nSince I created a relationship between weight and cholesterol as well as a relationship between height and weight, I am checking to see if height and cholesterol level have a relationship as a result. I will do this by creating a scatterplot.\n\nggplot(synthetic_data, aes(x = Height, y = Cholesterol)) +\n  geom_point(color = \"#e83d3d\") +\n  labs(title = \"Scatterplot of Weight and Height\",\n       x = \"Height (inches)\",\n       y = \"Cholesterol Level (mg/dL)\") + \n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nFrom the scatterplot above, it appears as though there is a relationship between cholesterol level and height due to the relationship I created between weight and cholesterol level. The points of the scatterplot appear to be a bit more spaced out than those of the previous scatterplots.\nNext, I will assess the strength of the correlation between height and cholesterol level by using the cor() function.\n\ncor(synthetic_data$Height, synthetic_data$Cholesterol)\n\n[1] 0.6777356\n\n\nThe correlation between height and cholesterol level is 0.5984226, which is moderate. This indicates there is a moderate relationship between height and cholesterol level. However, this relationship is not as strong as the relationship between weight and cholesterol level.\n\n\nCorrelation Matrix of All Variables\nI will create a correlation matrix to display the correlations between all of the variables within this synthetic dataset.\n\n# I will select only numeric variables (date wouldn't make sense to include here)\ncor_matrix &lt;- cor(synthetic_data %&gt;% select(where(is.numeric)))\n\n# Print out so we can see the correlation matrix!\nprint(cor_matrix)\n\n                Patient_ID          Age       Gender      Height       Weight\nPatient_ID      1.00000000  0.041808056 -0.090071146 -0.12744041 -0.171766405\nAge             0.04180806  1.000000000  0.009395789 -0.09858675  0.050323944\nGender         -0.09007115  0.009395789  1.000000000 -0.01433314  0.005319659\nHeight         -0.12744041 -0.098586750 -0.014333136  1.00000000  0.709388957\nWeight         -0.17176640  0.050323944  0.005319659  0.70938896  1.000000000\nBlood_Pressure  0.09974092  0.021928615  0.067946307 -0.03859859 -0.176520089\nCholesterol    -0.16167647  0.057061265  0.069728916  0.67773561  0.815541663\nDiabetes       -0.06997835  0.087519153  0.000000000 -0.03816801 -0.051150566\nSmoking         0.13590834  0.021204714 -0.040032038  0.05432258  0.051626270\n               Blood_Pressure Cholesterol    Diabetes     Smoking\nPatient_ID         0.09974092 -0.16167647 -0.06997835  0.13590834\nAge                0.02192862  0.05706127  0.08751915  0.02120471\nGender             0.06794631  0.06972892  0.00000000 -0.04003204\nHeight            -0.03859859  0.67773561 -0.03816801  0.05432258\nWeight            -0.17652009  0.81554166 -0.05115057  0.05162627\nBlood_Pressure     1.00000000 -0.23075388 -0.10877200  0.03482333\nCholesterol       -0.23075388  1.00000000 -0.04657569 -0.03045310\nDiabetes          -0.10877200 -0.04657569  1.00000000  0.04003204\nSmoking            0.03482333 -0.03045310  0.04003204  1.00000000\n\n\nI will now put this correlation matrix into a table and use the KableExtra package.\n\nkable(cor_matrix, caption = \"Correlation Matrix of Numeric Variables\") %&gt;%\n  kable_styling(\"striped\", full_width = F) %&gt;%\n  row_spec(0, background = \"#00509e\", color = \"white\") %&gt;%  # Blue header with white text\n  row_spec(1:nrow(cor_matrix), background = \"#d6eaf8\")  # Light blue for rows\n\n\nCorrelation Matrix of Numeric Variables\n\n\n\nPatient_ID\nAge\nGender\nHeight\nWeight\nBlood_Pressure\nCholesterol\nDiabetes\nSmoking\n\n\n\n\nPatient_ID\n1.0000000\n0.0418081\n-0.0900711\n-0.1274404\n-0.1717664\n0.0997409\n-0.1616765\n-0.0699784\n0.1359083\n\n\nAge\n0.0418081\n1.0000000\n0.0093958\n-0.0985867\n0.0503239\n0.0219286\n0.0570613\n0.0875192\n0.0212047\n\n\nGender\n-0.0900711\n0.0093958\n1.0000000\n-0.0143331\n0.0053197\n0.0679463\n0.0697289\n0.0000000\n-0.0400320\n\n\nHeight\n-0.1274404\n-0.0985867\n-0.0143331\n1.0000000\n0.7093890\n-0.0385986\n0.6777356\n-0.0381680\n0.0543226\n\n\nWeight\n-0.1717664\n0.0503239\n0.0053197\n0.7093890\n1.0000000\n-0.1765201\n0.8155417\n-0.0511506\n0.0516263\n\n\nBlood_Pressure\n0.0997409\n0.0219286\n0.0679463\n-0.0385986\n-0.1765201\n1.0000000\n-0.2307539\n-0.1087720\n0.0348233\n\n\nCholesterol\n-0.1616765\n0.0570613\n0.0697289\n0.6777356\n0.8155417\n-0.2307539\n1.0000000\n-0.0465757\n-0.0304531\n\n\nDiabetes\n-0.0699784\n0.0875192\n0.0000000\n-0.0381680\n-0.0511506\n-0.1087720\n-0.0465757\n1.0000000\n0.0400320\n\n\nSmoking\n0.1359083\n0.0212047\n-0.0400320\n0.0543226\n0.0516263\n0.0348233\n-0.0304531\n0.0400320\n1.0000000\n\n\n\n\n\n\n\nI will now create a graphical version of the correlation matrix that aids with visualization of the relationships between variables of this synthetic dataset. I will do this using the corrplot() function.\n\n# Define custom colors\ncorrplot_colors &lt;- colorRampPalette(c(\"#e83d3d\", \"white\", \"#63c230\"))(200)\n\n# Create the correlation plot with variable names on both sides\ncorrplot(cor_matrix, \n         method = \"circle\",       # Circle method\n         type = \"lower\",          # Lower half of the correlation matrix (since including the upper half would be repetitive)\n         order = \"hclust\",        # Order variables by hierarchical clustering\n         col = corrplot_colors,   # Custom color palette above!\n         tl.col = \"black\",        # Variable names in black\n         addCoef.col = \"black\",   # Correlation coefficients in black\n         number.cex = 0.5,        # Adjust coefficient size \n         number.digits = 2,       # Display two decimal places for coefficients\n         tl.srt = 45,             # Rotate labels on x-axis\n         mar = c(0, 0, 1, 0))     # Margins to adjust spacing\ntitle(\"Correlations from Synethic Dataset\", line = 2, cex.main = 1.5) # Adding a title!\n\n\n\n\n\n\n\n\nThe correlation matrix and plot above show the relationships between all of the variables in the synthetic dataset. The correlation matrix shows the correlation coefficients between each pair of variables. The correlation plot shows the same information in a visual format."
  },
  {
    "objectID": "data-exercise/data-exercise.html#simple-linear-models",
    "href": "data-exercise/data-exercise.html#simple-linear-models",
    "title": "Data Exercise",
    "section": "Simple Linear Models",
    "text": "Simple Linear Models\nI will now use the lm() function to fit a linear model with cholesterol as the outcome and weight as the predictor. Then, I will apply the summary() function.\n\nCholesterol_Weight &lt;- lm(Cholesterol ~ Weight, data = synthetic_data)\nsummary(Cholesterol_Weight)\n\n\nCall:\nlm(formula = Cholesterol ~ Weight, data = synthetic_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-33.070  -5.595   1.213   7.534  24.518 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 107.44682    5.54422   19.38   &lt;2e-16 ***\nWeight        0.47676    0.03417   13.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.86 on 98 degrees of freedom\nMultiple R-squared:  0.6651,    Adjusted R-squared:  0.6617 \nF-statistic: 194.6 on 1 and 98 DF,  p-value: &lt; 2.2e-16\n\n\nThe intercept (a) is 100.39517; this means that the average cholesterol level (in mg/dL) is 100.39517 when weight is 0 lbs. The slope (b) is 0.55317; this means that for every 1 unit increase in weight (in lbs), cholesterol level (in mg/dL) increases by 0.55317 mg/dL. The p-value is &lt; 2.2e-16.\nI will now use the lm() function to fit a linear model with cholesterol as the outcome and height as the predictor. Then, I will apply the summary() function.\n\nCholesterol_Height &lt;- lm(Cholesterol ~ Height, data = synthetic_data)\nsummary(Cholesterol_Height)\n\n\nCall:\nlm(formula = Cholesterol ~ Height, data = synthetic_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-41.326  -7.246  -0.234   8.266  34.098 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  46.3665    15.0703   3.077  0.00271 ** \nHeight        2.0271     0.2222   9.124 9.62e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.79 on 98 degrees of freedom\nMultiple R-squared:  0.4593,    Adjusted R-squared:  0.4538 \nF-statistic: 83.26 on 1 and 98 DF,  p-value: 9.617e-15\n\n\nThe intercept (a) is 40.8193; this means that the average cholesterol level (in mg/dL) is 40.8193 when height is 0 inches. The slope (b) is 2.1923; this means that for every 1 unit increase in height (in inches), cholesterol level (in mg/dL) increases by 2.1923 mg/dL. The p-value is 4.856e-11.\nAt an alpha (significance level) of 0.05, the p-values indicated above are both significant. Meaning, we can reject the null hypothesis that is no relationship between height and cholesterol as well as between weight and cholesterol. However, p-value for weight and cholesterol (&lt; 2.2e-16) is lower than that for height and cholesterol (4.856e-11). This indicates that while both height and weight are significant predictors of cholesterol level, weight is a stronger predictor of cholesterol level than height."
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "Introduction and Background\n\nMy name is Natalie and I am a first-year Epidemiology student. I’m from Radford, Virginia and graduated from Virginia Tech this past May with a degree in Microbiology.\nI participated in a few research projects during my time as an undergrad student. These were all wet lab focused (working with La Crosse virus and the fungus P. destructans). I am familiar with R since taking the EPID 7500 course last semester. However, I have not done any research involving programming and coding.\nA fun fact is that I have been to 46 out of the 50 states. The four I have not been to are: Alaska, Hawaii, New Mexico, and Vermont.\n\n\n\n\nGraduation Photo\n\n\n\n\nArticle and Brief Summary\nI chose to read a small article on the history of data analytics: A Brief History of Data Analytics\nThe article highlights how data analytics has been around for much longer than many of us realize. Data analytics looked much different than it does now, but it was still occurring in some form. For example, in 18,000 BCE bones and sticks were used to keep records of trading behaviors and the use of supplies. Statistics weren’t brought into the picture until 1663, when John Graunt kept mortality data in an effort to help track the plague. The article discusses how the internet had a great effect on the way data was collected and analyzed. The internet made communication between researchers much easier and for some data to be made public. The development of software also had a great impact on data analytics."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data used in this exercise will be using in this exercise is called “NCHS - Drug Poisoning Mortality by State: United States.” I downloaded this dataset from the CDC website linked here: https://data.cdc.gov/NCHS/NCHS-Drug-Poisoning-Mortality-by-State-United-Stat/jx6g-fdh6/about_data. This dataset contains entries on drug poisoning deaths in the U.S. and at the state-level. The data set’s time range is from 1999 to 2015."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#description-of-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#description-of-data",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "The data used in this exercise will be using in this exercise is called “NCHS - Drug Poisoning Mortality by State: United States.” I downloaded this dataset from the CDC website linked here: https://data.cdc.gov/NCHS/NCHS-Drug-Poisoning-Mortality-by-State-United-Stat/jx6g-fdh6/about_data. This dataset contains entries on drug poisoning deaths in the U.S. and at the state-level. The data set’s time range is from 1999 to 2015."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#loading-in-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#loading-in-the-data",
    "title": "CDC Data Exercise",
    "section": "Loading in the Data",
    "text": "Loading in the Data\nFirst, I will load any packages that may be of help in this exercise.\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ forcats   1.0.0     ✔ readr     2.1.5\n✔ ggplot2   3.5.1     ✔ stringr   1.5.1\n✔ lubridate 1.9.3     ✔ tibble    3.2.1\n✔ purrr     1.0.2     ✔ tidyr     1.3.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\n\nNow I will load in the CDC data for this exercise that was described in the section above. I will call it “data”.\n\ndata &lt;- read.csv(\"drug-data.csv\")\n\nI will use the summary(), dim(), and head() functions to get a better look at the data.\n\nsummary(data)\n\n      Year          Sex                Age            Race.and.Hispanic.Origin\n Min.   :1999   Length:2703        Length:2703        Length:2703             \n 1st Qu.:2003   Class :character   Class :character   Class :character        \n Median :2007   Mode  :character   Mode  :character   Mode  :character        \n Mean   :2007                                                                 \n 3rd Qu.:2011                                                                 \n Max.   :2015                                                                 \n                                                                              \n    State               Deaths        Population        Crude.Death.Rate\n Length:2703        Min.   :    1   Min.   :   238163   Min.   : 0.00   \n Class :character   1st Qu.:  124   1st Qu.:  2723372   1st Qu.: 3.90   \n Mode  :character   Median :  478   Median :  6403290   Median : 8.80   \n                    Mean   : 1873   Mean   : 16586603   Mean   :10.19   \n                    3rd Qu.: 1406   3rd Qu.: 18116192   3rd Qu.:14.30   \n                    Max.   :52404   Max.   :321418820   Max.   :53.50   \n                                                                        \n Standard.Error.for.Crude.Rate Low.Confidence.Limit.for.Crude.Rate\n Min.   :0.0000                Min.   : 0.000                     \n 1st Qu.:0.2000                1st Qu.: 3.300                     \n Median :0.4000                Median : 8.000                     \n Mean   :0.4271                Mean   : 9.364                     \n 3rd Qu.:0.6000                3rd Qu.:13.200                     \n Max.   :1.8000                Max.   :52.200                     \n                                                                  \n Upper.Confidence.Limit.for.Crude.Rate Age.adjusted.Rate\n Min.   : 0.10                         Min.   : 1.80    \n 1st Qu.: 4.60                         1st Qu.: 7.40    \n Median : 9.60                         Median :10.80    \n Mean   :11.07                         Mean   :11.24    \n 3rd Qu.:15.40                         3rd Qu.:14.00    \n Max.   :54.80                         Max.   :41.50    \n                                       NA's   :1632     \n Standard.Error.Age.adjusted.Rate Lower.Confidence.Limit.for.Age.adjusted.rate\n Min.   :0.000                    Min.   : 0.9                                \n 1st Qu.:0.300                    1st Qu.: 6.5                                \n Median :0.500                    Median : 9.8                                \n Mean   :0.543                    Mean   :10.2                                \n 3rd Qu.:0.700                    3rd Qu.:12.8                                \n Max.   :1.900                    Max.   :38.4                                \n NA's   :1632                     NA's   :1632                                \n Upper.Confidence.Limit.for.Age.adjusted.Rate State.Crude.Rate.in.Range\n Min.   : 2.20                                Length:2703              \n 1st Qu.: 8.20                                Class :character         \n Median :11.80                                Mode  :character         \n Mean   :12.34                                                         \n 3rd Qu.:15.30                                                         \n Max.   :44.60                                                         \n NA's   :1632                                                          \n US.Crude.Rate   US.Age.adjusted.Rate\n Min.   : 6.00   Min.   : 6.10       \n 1st Qu.: 8.90   1st Qu.: 8.90       \n Median :12.00   Median :11.90       \n Mean   :11.01   Mean   :10.96       \n 3rd Qu.:13.20   3rd Qu.:13.10       \n Max.   :16.30   Max.   :16.30       \n                                     \n\ndim(data)\n\n[1] 2703   18\n\nhead(data)\n\n  Year        Sex      Age Race.and.Hispanic.Origin         State Deaths\n1 2004 Both Sexes All Ages    All Races-All Origins         Texas   1813\n2 2012 Both Sexes All Ages    All Races-All Origins       Arizona   1134\n3 2008       Male All Ages                 Hispanic United States   1875\n4 2009 Both Sexes All Ages    All Races-All Origins    Washington    987\n5 2015     Female All Ages    All Races-All Origins United States  19447\n6 2008 Both Sexes All Ages    All Races-All Origins          Utah    468\n  Population Crude.Death.Rate Standard.Error.for.Crude.Rate\n1   22394023              8.1                           0.2\n2    6553255             17.3                           0.5\n3   24322906              7.7                           0.2\n4    6667426             14.8                           0.5\n5  163189523             11.9                           0.1\n6    2663029             17.6                           0.8\n  Low.Confidence.Limit.for.Crude.Rate Upper.Confidence.Limit.for.Crude.Rate\n1                                 7.7                                   8.5\n2                                16.3                                  18.3\n3                                 7.4                                   8.1\n4                                13.9                                  15.7\n5                                11.7                                  12.1\n6                                16.0                                  19.2\n  Age.adjusted.Rate Standard.Error.Age.adjusted.Rate\n1               8.2                              0.2\n2              17.7                              0.5\n3               8.4                              0.2\n4              14.3                              0.5\n5              11.8                              0.1\n6              19.0                              0.9\n  Lower.Confidence.Limit.for.Age.adjusted.rate\n1                                          7.8\n2                                         16.6\n3                                          8.0\n4                                         13.4\n5                                         11.7\n6                                         17.2\n  Upper.Confidence.Limit.for.Age.adjusted.Rate State.Crude.Rate.in.Range\n1                                          8.5                     7–9.8\n2                                         18.7                 15.2–41.5\n3                                          8.8                   8.3–8.4\n4                                         15.2                 12.3–15.2\n5                                         12.0                 10.6–11.8\n6                                         20.7                 15.2–41.5\n  US.Crude.Rate US.Age.adjusted.Rate\n1           9.4                  9.4\n2          13.2                 13.1\n3          12.0                 11.9\n4          12.1                 11.9\n5          16.3                 16.3\n6          12.0                 11.9\n\n\nAs seen by the output, this dataset contains 18 variables with 2,703 observations."
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#processing-the-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#processing-the-data",
    "title": "CDC Data Exercise",
    "section": "Processing the Data",
    "text": "Processing the Data\nBelow, I will select the variables that I will work with in this exercise. I will reassign this to a new dataset called “processed_data”.\n\nprocessed_data &lt;- data %&gt;% select(Year, Sex, Age, Race.and.Hispanic.Origin, State, Deaths, Population, Crude.Death.Rate)\n\nI will rename the variables to be easier to understand. I will use head() to ensure this worked.\n\nprocessed_data &lt;- processed_data %&gt;%\n  rename(Race_and_Hispanic_Origin = Race.and.Hispanic.Origin,\n         Crude_Death_Rate = Crude.Death.Rate)\nhead(processed_data)\n\n  Year        Sex      Age Race_and_Hispanic_Origin         State Deaths\n1 2004 Both Sexes All Ages    All Races-All Origins         Texas   1813\n2 2012 Both Sexes All Ages    All Races-All Origins       Arizona   1134\n3 2008       Male All Ages                 Hispanic United States   1875\n4 2009 Both Sexes All Ages    All Races-All Origins    Washington    987\n5 2015     Female All Ages    All Races-All Origins United States  19447\n6 2008 Both Sexes All Ages    All Races-All Origins          Utah    468\n  Population Crude_Death_Rate\n1   22394023              8.1\n2    6553255             17.3\n3   24322906              7.7\n4    6667426             14.8\n5  163189523             11.9\n6    2663029             17.6"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#exploratorydescriptive-analysis-and-frequency-table",
    "href": "cdcdata-exercise/cdcdata-exercise.html#exploratorydescriptive-analysis-and-frequency-table",
    "title": "CDC Data Exercise",
    "section": "Exploratory/Descriptive Analysis and Frequency Table",
    "text": "Exploratory/Descriptive Analysis and Frequency Table\nI will create a scatterplot to assess the relationship between deaths and population. Then, I will use the cor() function to obtain the correlation coefficient to determine the strength of this relationship.\n\nggplot(processed_data, aes(x = Deaths, y = Population)) +\n  geom_point(color = \"#429bf9\") + # giving the points of the scatterplot a blue color\n  labs(title = \"Scatterplot of Relationship between Number of Deaths \\n From Drug Poisoning and Population Size\",\n       x = \"Number of Deaths\",\n       y = \"Size of Population\") + # labeling axes and titles!\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # removing legend; making title and axes bold and giving them a larger font size\n\n\n\n\n\n\n\ncor(processed_data$Deaths, processed_data$Population) # correlation coefficient\n\n[1] 0.8997261\n\n\nThe scatterplot above, shows a positive linear relationship between the number of deaths from drug poisoning and the size of the population. The correlation coefficient is 0.8997261, which indicates a strong positive linear relationship.\nNow, I will assess the relationship between year and death. Then, I will use the cor() function to obtain the correlation coefficient to determine the strength of this relationship.\n\nggplot(processed_data, aes(x = Year, y = Deaths)) +\n  geom_point(color = \"#33d11a\") + # giving the points of the scatterplot a green color\n  labs(title = \"Scatterplot of Relationship between Number of Deaths \\n From Drug Poisoning and Year\",\n       x = \"Year\",\n       y = \"Number of Deaths\") + # labeling axes and titles!\n  theme(legend.position = \"none\", plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5), axis.title.x = element_text(size = 12, face = \"bold\"), axis.title.y = element_text(size = 12, face = \"bold\")) # removing legend; making title and axes bold and giving them a larger font size\n\n\n\n\n\n\n\ncor(processed_data$Deaths, processed_data$Year) # correlation coefficient\n\n[1] 0.1246065\n\n\nThe scatterplot above shows that there may be a slight relationship between year and number deaths due to drug poisoning. The correlation coefficient is 0.1246065; this indicates a VERY weak relationship (possibly no realtionship at all).\nI will create a correlation matrix. I did this in the “data-exercise” and found it to be helpful.\n\n# use only numberic variables!\ncor_matrix &lt;- cor(processed_data %&gt;% select(where(is.numeric)))\n\n# print out!\nprint(cor_matrix)\n\n                       Year    Deaths Population Crude_Death_Rate\nYear             1.00000000 0.1246065 0.01947317       0.32354160\nDeaths           0.12460651 1.0000000 0.89972612       0.31923101\nPopulation       0.01947317 0.8997261 1.00000000       0.06736837\nCrude_Death_Rate 0.32354160 0.3192310 0.06736837       1.00000000\n\n\nI will create a frequency table to display the total number of deaths from drug poisoning by year.\n\n# Create frequency table for the total number of deaths by year\ndeath_by_year &lt;- processed_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(Total_Deaths = sum(Deaths, na.rm = TRUE)) %&gt;%\n  arrange(Year)\n\n# View the frequency table\ndeath_by_year\n\n# A tibble: 17 × 2\n    Year Total_Deaths\n   &lt;int&gt;        &lt;int&gt;\n 1  1999       149899\n 2  2000       155057\n 3  2001       172732\n 4  2002       209642\n 5  2003       229715\n 6  2004       244414\n 7  2005       265629\n 8  2006       306901\n 9  2007       321148\n10  2008       324838\n11  2009       329252\n12  2010       341295\n13  2011       367830\n14  2012       368968\n15  2013       391308\n16  2014       418345\n17  2015       465632"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#assessing-distribution",
    "href": "cdcdata-exercise/cdcdata-exercise.html#assessing-distribution",
    "title": "CDC Data Exercise",
    "section": "Assessing Distribution",
    "text": "Assessing Distribution\nI will assess the distribution of both the number of deaths due to drug poisoning and the population size.\n\n# Deaths\nggplot(processed_data, aes(x = Deaths)) +\n  geom_histogram(bins = 30, fill = \"#ee5050\", color = \"#c41616\", alpha = 0.7) +\n  labs(title = \"Histogram of Deaths from Drug Poisoning\",\n       x = \"Number of Deaths\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n# Population Size\nggplot(processed_data, aes(x = Population)) +\n  geom_histogram(bins = 30, fill = \"#f9c363\", color = \"#ffa303\", alpha = 0.7) +\n  labs(title = \"Histogram of Population Size\",\n       x = \"Population Size\",\n       y = \"Frequency\") +\n  theme(plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n        axis.title.x = element_text(size = 12, face = \"bold\"),\n        axis.title.y = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\nBoth histograms for number of deaths due to drug poisoning and population size appeared to be skewed to the right.\nI will create a table to display the mean and standard deviations of the number of deaths due to drug poisoning from 1999 to 2015 as well as the population size from 1999 to 2015.\n\ndistribution_table &lt;- processed_data %&gt;%\n  group_by(Year) %&gt;%\n  summarise(\n    Mean_Deaths = mean(Deaths, na.rm = TRUE),\n    SD_Deaths = sd(Deaths, na.rm = TRUE),\n    Mean_Population = mean(Population, na.rm = TRUE),\n    SD_Population = sd(Population, na.rm = TRUE)\n  )\n\nprint(distribution_table) # print table to see it!\n\n# A tibble: 17 × 5\n    Year Mean_Deaths SD_Deaths Mean_Population SD_Population\n   &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;\n 1  1999        943.     2155.       15462000.     32759001.\n 2  2000        975.     2247.       15584898.     32954121.\n 3  2001       1086.     2507.       15769468.     33258376.\n 4  2002       1319.     3045.       15906943.     33474245.\n 5  2003       1445.     3339.       16035120.     33675188.\n 6  2004       1537.     3559.       16175519.     33902213.\n 7  2005       1671.     3839.       16316254.     34126543.\n 8  2006       1930.     4434.       16465436.     34368590.\n 9  2007       2020.     4659.       16614163.     34609414.\n10  2008       2043.     4734.       16763441.     34851622.\n11  2009       2071.     4789.       16902888.     35076777.\n12  2010       2147.     4963.       17005636.     35242661.\n13  2011       2313.     5337.       17154800.     35484630.\n14  2012       2321.     5335.       17270704.     35678128.\n15  2013       2461.     5628.       17382466.     35862756.\n16  2014       2631.     6014.       17517589.     36089522.\n17  2015       2929.     6683.       17644933.     36306209."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section. This data is example data created for the MADA course. The variables are: height, weight, gender, age, and hair color. The two variables I added are: age and hair color. Age is any number greater than 0 (years). Hair color fits into one of several categories: black, blonde, brunette, red, or other."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\nfactor\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\n165.66667\n15.97655\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\n70.11111\n21.24526\n45\n55\n70\n80\n110\n▇▂▃▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-boxplot",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-boxplot",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.1 Muhammad’s Boxplot",
    "text": "7.1 Muhammad’s Boxplot\n\n\n\n\n\nBoxplot created for Exercise 2.\n\n\n\n\nThe boxplot above shows the median height among the different genders within the dataset. As you can see, out of the three categories above, the median height of females within the dataset is the highest. The female category also contains the greatest amount of variation. The median heights of males and those selecting “other” are relatively similar. The male category contains the least amount of variation; it is also appears to contain an outlier.\nNote: The instructions specify for the boxplot to be between height and hair color. I will analyze the boxplot given above."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-scatterplot",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-scatterplot",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.2 Muhammad’s Scatterplot",
    "text": "7.2 Muhammad’s Scatterplot\n\n\n\n\n\nScatterplot created for Exercise 2.\n\n\n\n\nThe scatterplot above depicts the relationship between weight and height. There appears to be little to no correlation between the two variables as the points are relatively scattered throughout the plot.\nNote: The instructions specify for the scatterplot to be between weight and age. I will analyze the scatterplot given above."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-qq-plot",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#muhammads-qq-plot",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.3 Muhammad’s QQ-Plot",
    "text": "7.3 Muhammad’s QQ-Plot\n\n\n\n\n\nQQ-Plot created for Exercise 2.\n\n\n\n\nMuhammad created a QQ-Plot to analyze the data. The red line in this QQ-Plot represents a normal distribution. As you can see the points follow relatively close to this red line, indicating that the data is normally distributed. There appear to be slight deviations from the red line, however these do not appear to be significant. Therefore, any skewness is minimal."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#natalies-table-3",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#natalies-table-3",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "7.4 Natalie’s Table 3",
    "text": "7.4 Natalie’s Table 3\n\n\n\nLinear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n159.2356979\n27.543979\n5.7811436\n0.0102925\n\n\nHair_ColorBLO\n11.4477498\n31.161763\n0.3673653\n0.7377324\n\n\nHair_ColorBRU\n24.0175439\n29.399389\n0.8169402\n0.4738031\n\n\nHair_ColorO\n8.8718535\n37.668646\n0.2355236\n0.8289647\n\n\nHair_ColorRED\n29.6544622\n28.516514\n1.0399049\n0.3748287\n\n\nAge\n-0.3272311\n1.177337\n-0.2779418\n0.7991117\n\n\n\n\n\nTable 3 displays a summary of a linear model modeling Hair Color and Age as predictors of height. Neither characteristic was significantly associated with height as the p-values are all very high."
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\nlibrary(ggplot2)\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          3     \n_______________________          \nColumn type frequency:           \n  factor                   1     \n  numeric                  2     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean   sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0 133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2  45  55  70  80  110 ▇▂▃▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\nlibrary(dplyr) #for data processing/cleaning\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\nlibrary(skimr) #for nice visualization of data \nlibrary(here) #to set paths\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\") # To set up location using here() function \nrawdata &lt;- readxl::read_excel(data_location) # open dataset\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                         `Allowed Values`\n  &lt;chr&gt;           &lt;chr&gt;                                         &lt;chr&gt;           \n1 Height          height in centimeters                         numeric value &gt;…\n2 Weight          weight in kilograms                           numeric value &gt;…\n3 Gender          identified gender (male/female/other)         M/F/O/NA        \n4 Age             age in years                                  numeric value ≥…\n5 Hair Color      color of hair (black/blonde/brunette/red/oth… BLA/BLO/BRU/RED…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height       &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"…\n$ Weight       &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender       &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"…\n$ Age          &lt;dbl&gt; 29, 24, 43, 27, 52, 18, 37, 41, 20, 35, 62, 31, 43, 16\n$ `Hair Color` &lt;chr&gt; \"BLO\", \"BRU\", \"BLA\", \"BRU\", \"BLA\", \"RED\", \"O\", \"BLO\", \"BL…\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :16.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:24.75  \n Mode  :character   Median :  70.0   Mode  :character   Median :33.00  \n                    Mean   : 602.7                      Mean   :34.14  \n                    3rd Qu.:  90.0                      3rd Qu.:42.50  \n                    Max.   :7000.0                      Max.   :62.00  \n                    NA's   :1                                          \n  Hair Color       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age `Hair Color`\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;       \n1 180        80 M         29 BLO         \n2 175        70 O         24 BRU         \n3 sixty      60 F         43 BLA         \n4 178        76 F         27 BRU         \n5 192        90 NA        52 BLA         \n6 6          55 F         18 RED         \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70\n90.0\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n34.14\n13.27\n16\n24.75\n33\n42.5\n62\n▆▅▇▂▂\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% # delete observation with string \"sixty\"\n                  dplyr::mutate(Height = as.numeric(Height)) # convert the Height into numeric \nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n33.46\n13.55\n16\n24.00\n31\n41\n62\n▇▆▇▂▂\n\n\n\n\nhist(d1$Height)\n\n\n\n\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) ) # Convert 6\" into Centimeter \nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n33.46\n13.55\n16\n24.00\n31\n41\n62\n▇▆▇▂▂\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n30.73\n11.42\n16\n22.0\n29\n39\n52\n▇▇▅▅▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n30.73\n11.42\n16\n22.0\n29\n39\n52\n▇▇▅▅▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHair Color\n0\n1\n1\n3\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n29.56\n9.51\n16\n24\n29\n37\n43\n▅▂▇▂▅\n\n\n\n\n\nCreating a new variable for age category\n\nd4 &lt;- d4 %&gt;% \n  mutate(age_cat= ifelse (Age &lt;= 30, 0,\n                  ifelse (Age &gt; 30 & Age &lt;= 40, 1,\n                  ifelse (Age &gt; 40 & Age &lt;= 50, 2,\n                  ifelse (Age &gt; 50, 3, NA_real_)))))\n\nExtra Note from natalie: Hair Color needs to be changed into a factor.\n\nclass(d4$age_cat)\n\n[1] \"numeric\"\n\nclass(d4$`Hair Color`)\n\n[1] \"character\"\n\nd4$`Hair Color` &lt;- as.factor(d4$`Hair Color`)\nclass(d4$`Hair Color`)\n\n[1] \"factor\"\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata2 &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\nsaveRDS(processeddata2, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Natalie Cann Data Analysis Portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "I chose to recreate a graph that shows the distribution of movie ratings on the star scale from Fandango, IMDb, IMDb users, Metacritic, Metacritic users, and Rotten Tomatoes, and Rotten Tomatoes users.\nThe graph is depicted below.  Fandango Five Thirty Eight Article"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#load-packages",
    "href": "presentation-exercise/presentation-exercise.html#load-packages",
    "title": "Presentation Exercise",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(here)\n\nhere() starts at /Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\nlibrary(readr)"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#import-data",
    "href": "presentation-exercise/presentation-exercise.html#import-data",
    "title": "Presentation Exercise",
    "section": "Import Data",
    "text": "Import Data\n\nhere::here()\n\n[1] \"/Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio\"\n\ngetwd()\n\n[1] \"/Users/nataliecann/Desktop/MADA/nataliecann-MADA-portfolio/presentation-exercise\"\n\nsetwd(\"~/Desktop/MADA/nataliecann-MADA-portfolio/presentation-exercise\")\ndf &lt;- read_csv(here::here(\"presentation-exercise\", \"fandango\", \"fandango_score_comparison.csv\"))\n\nRows: 146 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): FILM\ndbl (21): RottenTomatoes, RottenTomatoes_User, Metacritic, Metacritic_User, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-one-creating-initial-graph",
    "href": "presentation-exercise/presentation-exercise.html#version-one-creating-initial-graph",
    "title": "Presentation Exercise",
    "section": "Version One: Creating Initial Graph",
    "text": "Version One: Creating Initial Graph\nI used Microsoft Copilot, and occasionally ChatGPT, for this assignment. First, I sent Microsoft Copilot the data nad the graph that I wanted to recreate. I asked it to provide me with code to do so.\nSent Data to Microsoft Copilot Here is the data fandango_score_comparison.csv\nSend Microsoft Copilot a .pdf of the graph that I was aiming to recreate fandangocomparison.pdf how do i recreate this graph (with the same shape, rather than a histogram with bars) using the data provided?\nAnswer: Install and load necessary packages install.packages(“ggplot2”) library(ggplot2)\nRead the CSV file df &lt;- read.csv(“path/to/your/fandango_score_comparison.csv”)\nNormalize the ratings df\\(Fandango_Stars_norm &lt;- df\\)Fandango_Stars / 5 df\\(Fandango_Ratingvalue_norm &lt;- df\\)Fandango_Ratingvalue / 5 df\\(IMDB_norm &lt;- df\\)IMDB / 10 df\\(Metacritic_norm &lt;- df\\)Metacritic / 100 df\\(Metacritic_User_norm &lt;- df\\)Metacritic_User / 10 df\\(RottenTomatoes_norm &lt;- df\\)RottenTomatoes / 100 df\\(RottenTomatoes_User_norm &lt;- df\\)RottenTomatoes_User / 100\nCreate the density plots ggplot() + geom_density(data = df, aes(x = Fandango_Stars_norm, fill = “Fandango”), alpha = 0.5) + geom_density(data = df, aes(x = Fandango_Ratingvalue_norm, fill = “Fandango Rating Value”), alpha = 0.5) + geom_density(data = df, aes(x = IMDB_norm, fill = “IMDb”), alpha = 0.5) + geom_density(data = df, aes(x = Metacritic_norm, fill = “Metacritic”), alpha = 0.5) + geom_density(data = df, aes(x = Metacritic_User_norm, fill = “Metacritic users”), alpha = 0.5) + geom_density(data = df, aes(x = RottenTomatoes_norm, fill = “Rotten Tomatoes”), alpha = 0.5) + geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = “Rotten Tomatoes users”), alpha = 0.5) + labs(title = “Normalized Ratings Distribution of 146 Films in Theaters in 2015”, x = “Normalized Rating”, y = “Density”) + scale_fill_manual(name = “Source”, values = c(“Fandango” = “blue”, “Fandango Rating Value” = “cyan”, “IMDb” = “red”, “Metacritic” = “green”, “Metacritic users” = “darkgreen”, “Rotten Tomatoes” = “purple”, “Rotten Tomatoes users” = “violet”)) + theme_minimal()\nSave the plot ggsave(“fandangocomparison_replicated.pdf”)\nI ran the code in the code chunk below.\n\n# Normalize the ratings\ndf$Fandango_Stars_norm &lt;- df$Fandango_Stars / 5\ndf$IMDB_norm &lt;- df$IMDB / 10\ndf$Metacritic_norm &lt;- df$Metacritic / 100\ndf$Metacritic_User_norm &lt;- df$Metacritic_User / 10\ndf$RottenTomatoes_norm &lt;- df$RottenTomatoes / 100\ndf$RottenTomatoes_User_norm &lt;- df$RottenTomatoes_User / 100\n\n# Create the density plots\nversion_one &lt;- ggplot() +\n  geom_density(data = df, aes(x = Fandango_Stars_norm, fill = \"Fandango\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = IMDB_norm, fill = \"IMDb\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_norm, fill = \"Metacritic\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), alpha = 0.5) +\n  labs(title = \"Normalized Ratings Distribution of 146 Films in Theaters in 2015\",\n       x = \"Normalized Rating\",\n       y = \"Density\") +\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"blue\", \"IMDb\" = \"red\", \"Metacritic\" = \"green\", \"Metacritic users\" = \"darkgreen\", \"Rotten Tomatoes\" = \"purple\", \"Rotten Tomatoes users\" = \"violet\")) +\n  theme_minimal()\n\nversion_one\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_one.png\") \n\nggsave(filename = figure_file, plot = version_one) # saving the plot as \"version_one\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-two-editing-graph-to-better-match-the-original-version",
    "href": "presentation-exercise/presentation-exercise.html#version-two-editing-graph-to-better-match-the-original-version",
    "title": "Presentation Exercise",
    "section": "Version Two: Editing Graph to Better Match the Original Version",
    "text": "Version Two: Editing Graph to Better Match the Original Version\nI manually edited the colors to fit the ones assigned to each source within the original graph. I simply did this by using hex codes online or typing in basic colors (such as “red”). Additionally, I changed the title under the labs() function.\n\nversion_two &lt;- ggplot() +\n  geom_density(data = df, aes(x = Fandango_Stars_norm, fill = \"Fandango\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = IMDB_norm, fill = \"IMDb\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_norm, fill = \"Metacritic\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), alpha = 0.5) +\n  geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), alpha = 0.5) +\n  labs(title = \"Fandango LOVES Movies\",\n       x = \"Normalized Rating\",\n       y = \"Density\") +\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"red\", \"IMDb\" = \"gold\", \"Metacritic\" = \"#8d31c1\", \"Metacritic users\" = \"#94e13d\", \"Rotten Tomatoes\" = \"#7b7b7c\", \"Rotten Tomatoes users\" = \"#309dd8\")) + # update colors to match those of the original graph \n  theme_minimal() \n\nversion_two\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_two.png\")\n\nggsave(filename = figure_file, plot = version_two) # saving the plot as \"version_two\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-three-editing-graph-more-to-better-match-the-original-version",
    "href": "presentation-exercise/presentation-exercise.html#version-three-editing-graph-more-to-better-match-the-original-version",
    "title": "Presentation Exercise",
    "section": "Version Three: Editing Graph MORE to Better Match the Original Version",
    "text": "Version Three: Editing Graph MORE to Better Match the Original Version\nI asked ChatGPT: how do I get rid of the legend and add the labels within the legend next to their corresponding areas within the graph?\nAnswer: theme(legend.position = “none”) + # Removes the legend geom_text(aes(x = 1, y = 0.15, label = “Fandango”, color = “red”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Fandango geom_text(aes(x = 1, y = 0.13, label = “IMDb”, color = “gold”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for IMDb geom_text(aes(x = 1, y = 0.11, label = “Metacritic”, color = “#8d31c1”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Metacritic geom_text(aes(x = 1, y = 0.09, label = “Metacritic users”, color = “#94e13d”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Metacritic users geom_text(aes(x = 1, y = 0.07, label = “Rotten Tomatoes”, color = “#7b7b7c”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Rotten Tomatoes geom_text(aes(x = 1, y = 0.05, label = “Rotten Tomatoes users”, color = “#309dd8”), size = 5, data = data.frame(), inherit.aes = FALSE) # Add label for Rotten Tomatoes users\nI manually changed the color of the IMDb users density/text to be a darker gold for better visibility. Additionally, I changed the name that ChatGPT gave me for the label from “IBDb” to “IMDb users” to match the original graph.\nAfter a few more intermediate steps: - I changed the theme of the graph to include a gray background and gray gridlines - I changed the positions of the labels to match the original graph - I fixed the x and y axes - I added a subtitle - I changed geom_density to stat_density and added their respective colors - I made the area under the densities gray with an outline of their respective color\n\nversion_three &lt;- ggplot(df) +\n\n   # Adding densities/areas\n  stat_density(aes(x = Fandango_Stars_norm, fill = \"Fandango\"), geom = \"area\", alpha = 0.5, color = \"red\") +\n  stat_density(aes(x = IMDB_norm, fill = \"IMDb\"), geom = \"area\", alpha = 0.5, color = \"#e6c531\") +\n  stat_density(aes(x = Metacritic_norm, fill = \"Metacritic\"), geom = \"area\", alpha = 0.5, color = \"#8d31c1\") +\n  stat_density(aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), geom = \"area\", alpha = 0.5, color = \"#94e13d\") +\n  stat_density(aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), geom = \"area\", alpha = 0.5, color = \"#7b7b7c\") +\n  stat_density(aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), geom = \"area\", alpha = 0.5, color = \"#309dd8\") +\n  \n  # Adding title and subtitle\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of 146 films in theaters in 2015 that \\nhad 30+ reviews on Fandango.com\", \n       x = NULL,  \n       y = NULL) +\n \n   # Adding fill colors for the densities/areas\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"#f95858\", \"IMDb\" = \"grey\", \"Metacritic\" = \"grey\", \"Metacritic users\" = \"grey\", \"Rotten Tomatoes\" = \"grey\", \"Rotten Tomatoes users\" = \"grey\")) +\n  \n  # Changing background to grey with grey and thin gridlines\n  theme_minimal() +\n  theme(legend.position = \"none\", \n        plot.background = element_rect(fill = \"#ececec\"),   \n        panel.grid.major = element_line(color = \"#a7a7a5\", size = 0.1), \n        panel.grid.minor = element_line(color = \"#a7a7a5\", size = 0.1), \n        panel.grid = element_line(color = \"transparent\"), \n        plot.title = element_text(face = \"bold\", size = 14), \n        plot.subtitle = element_text(size = 10)) + \n  \n  # Adding labels and placing them in the appropriate spot on the graph \n  geom_text(aes(x = 0.97, y = 3.75, label = \"Fandango\"), color = \"red\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = .65, y = 4.2, label = \"IMDb users\"), color = \"#e6c531\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.3, y = 1.5, label = \"Metacritic\"), color = \"#8d31c1\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.57, y = 2.5, label = \"Metacritic \\n users\"), color = \"#94e13d\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.1, y = 1.2, label = \"Rotten \\n Tomatoes\"), color = \"#7b7b7c\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.45, y = 1.9 , label = \"Rotten \\n Tomatoes \\n users\"), color = \"#309dd8\", size = 2.5, data = data.frame(), inherit.aes = FALSE) +\n  \n  # Changing the x and y axis labels to match the original graph\n  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), \n                     labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +  # the stars appear in the png version\n  scale_y_continuous(breaks = c(0, 1, 2, 3, 4), \n                     labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\"))\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\nversion_three\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_three.png\")\n\nggsave(filename = figure_file, plot = version_three) # saving the plot as \"version_three\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#version-four-creating-final-version",
    "href": "presentation-exercise/presentation-exercise.html#version-four-creating-final-version",
    "title": "Presentation Exercise",
    "section": "Version Four: Creating Final Version",
    "text": "Version Four: Creating Final Version\nFor the final version, I made the following changes: - I made the “fandango” label bold (just like it is in the original graph) - I made the font size of the title and subtitle larger - I updated the color of the green in “Metacritic users” to be a bit darker - I removed the “minor” gridlines\nAdditionally, I asked Microsoft Copilot to change the geom from stat_density to stat_bin. Stat_bin will better help me recreate the original graph than geom_density or stat_density.\nAnswer: stat_bin(aes(x = Fandango_Stars_norm, fill = “Fandango”), geom = “area”, alpha = 0.5, color = “red”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = IMDB_norm, fill = “IMDb”), geom = “area”, alpha = 0.5, color = “#f7ca62”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = Metacritic_norm, fill = “Metacritic”), geom = “area”, alpha = 0.5, color = “#8d31c1”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = Metacritic_User_norm, fill = “Metacritic users”), geom = “area”, alpha = 0.5, color = “#94e13d”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = RottenTomatoes_norm, fill = “Rotten Tomatoes”), geom = “area”, alpha = 0.5, color = “#7b7b7c”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = RottenTomatoes_User_norm, fill = “Rotten Tomatoes users”), geom = “area”, alpha = 0.5, color = “#309dd8”, binwidth = 0.1, position = “identity”) +\nAfter fixing the geom/stat, I realized the y-axis had been messed up. I used code to fix the y-axis and fix the source labels within the graph.\n\nversion_four &lt;- ggplot(df) +\n \n   # Adding densities/areas\n  stat_bin(aes(x = IMDB_norm, fill = \"IMDb\"), geom = \"area\", alpha = 0.5, color = \"#e6c531\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = Metacritic_norm, fill = \"Metacritic\"), geom = \"area\", alpha = 0.5, color = \"#8d31c1\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), geom = \"area\", alpha = 0.5, color = \"#87c866\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), geom = \"area\", alpha = 0.5, color = \"#7b7b7c\", binwidth = 0.1, position = \"identity\") +\n  stat_bin(aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), geom = \"area\", alpha = 0.5, color = \"#309dd8\", binwidth = 0.1, position = \"identity\") + stat_bin(aes(x = Fandango_Stars_norm, fill = \"Fandango\"), geom = \"area\", alpha = 0.5, color = \"red\", binwidth = 0.1, position = \"identity\") +\n  \n  # Adding title and subtitle\n  labs(title = \"Fandango LOVES Movies\",\n       subtitle = \"Normalized ratings distribution of 146 films in theaters in 2015 that \\nhad 30+ reviews on Fandango.com\", \n       x = NULL,  \n       y = NULL) +\n \n   # Adding fill colors for the densities/areas\n  scale_fill_manual(name = \"Source\", values = c(\"Fandango\" = \"#f95858\", \"IMDb\" = \"grey\", \"Metacritic\" = \"grey\", \"Metacritic users\" = \"grey\", \"Rotten Tomatoes\" = \"grey\", \"Rotten Tomatoes users\" = \"grey\")) +\n  \n  # Changing background to grey with grey and thin gridlines\ntheme_minimal() +\n  theme(legend.position = \"none\", \n        plot.background = element_rect(fill = \"#ececec\"),   \n        panel.grid.major = element_line(color = \"#a7a7a5\", size = 0.1), \n        panel.grid = element_line(color = \"transparent\"), \n        plot.title = element_text(face = \"bold\", size = 18),  # Bigger title\n        plot.subtitle = element_text(size = 14)) +  # Bigger subtitle\n  \n  # Adding labels and placing them in the appropriate spot on the graph \n  geom_text(aes(x = 0.97, y = 55, label = \"Fandango\"), color = \"red\", size = 3.2, fontface = \"bold\", data = data.frame(), inherit.aes = FALSE) + # only bold \"Fandango\"\n  geom_text(aes(x = .6, y = 60, label = \"IMDb users\"), color = \"#e6c531\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.3, y = 23, label = \"Metacritic\"), color = \"#8d31c1\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.55, y = 37, label = \"Metacritic \\n users\"), color = \"#87c866\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.1, y = 19, label = \"Rotten \\n Tomatoes\"), color = \"#7b7b7c\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  geom_text(aes(x = 0.45, y = 30 , label = \"Rotten \\n Tomatoes \\n users\"), color = \"#309dd8\", size = 3.2, data = data.frame(), inherit.aes = FALSE) +\n  \n  # Changing the x and y axis labels to match the original graph\n  scale_x_continuous(breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), \n                     labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")) +  # the stars appear in the png version\n  scale_y_continuous(breaks = c(0, 15, 30, 45, 60), \n                     labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\"))\n\nversion_four\n\n\n\n\n\n\n\nfigure_file = here(\"presentation-exercise\", \"graphs\", \"version_four.png\")\n\nggsave(filename = figure_file, plot = version_four) # saving the plot as \"version_four\"\n\nSaving 7 x 5 in image"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#final-version-of-graph-vs-original-graph",
    "href": "presentation-exercise/presentation-exercise.html#final-version-of-graph-vs-original-graph",
    "title": "Presentation Exercise",
    "section": "Final Version of Graph vs Original Graph",
    "text": "Final Version of Graph vs Original Graph\nThe stars used on the x-axis to represent the ratings did not appear in the output on my computer in R Studio. However, the stars appear in the .png file attached below.\n \nFor the most part, I believe that I was able to successfully recreate the original graph. It appears that the distriution of the ratings among the different sources is pretty similar to that of the original graph. There are miniscule differences. Another element that I had difficulty in recreating from the original graph was making the areas overlap in the same way that they do in the original graph."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#load-packages-1",
    "href": "presentation-exercise/presentation-exercise.html#load-packages-1",
    "title": "Presentation Exercise",
    "section": "Load Packages",
    "text": "Load Packages\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(gt)\nlibrary(stringr)\nlibrary(webshot2)\nlibrary(htmltools)\nlibrary(purrr)"
  },
  {
    "objectID": "presentation-exercise/visualization-exercise.html",
    "href": "presentation-exercise/visualization-exercise.html",
    "title": "Visualization Exercise",
    "section": "",
    "text": "This .qmd file contains the AI prompts, as well as the code that it returned, for the “presentation-exercise” file."
  },
  {
    "objectID": "presentation-exercise/visualization-exercise.html#version-one-creating-initial-graph",
    "href": "presentation-exercise/visualization-exercise.html#version-one-creating-initial-graph",
    "title": "Visualization Exercise",
    "section": "Version One: Creating Initial Graph",
    "text": "Version One: Creating Initial Graph\nI used Microsoft Copilot, and occasionally ChatGPT, for this assignment. First, I sent Microsoft Copilot the data nad the graph that I wanted to recreate. I asked it to provide me with code to do so.\nSent Data to Microsoft Copilot Here is the data fandango_score_comparison.csv\nSend Microsoft Copilot a .pdf of the graph that I was aiming to recreate fandangocomparison.pdf how do i recreate this graph (with the same shape, rather than a histogram with bars) using the data provided?\nAnswer: Install and load necessary packages install.packages(“ggplot2”) library(ggplot2)\nRead the CSV file df &lt;- read.csv(“path/to/your/fandango_score_comparison.csv”)\nNormalize the ratings df\\(Fandango_Stars_norm &lt;- df\\)Fandango_Stars / 5 df\\(Fandango_Ratingvalue_norm &lt;- df\\)Fandango_Ratingvalue / 5 df\\(IMDB_norm &lt;- df\\)IMDB / 10 df\\(Metacritic_norm &lt;- df\\)Metacritic / 100 df\\(Metacritic_User_norm &lt;- df\\)Metacritic_User / 10 df\\(RottenTomatoes_norm &lt;- df\\)RottenTomatoes / 100 df\\(RottenTomatoes_User_norm &lt;- df\\)RottenTomatoes_User / 100\nCreate the density plots ggplot() + geom_density(data = df, aes(x = Fandango_Stars_norm, fill = “Fandango”), alpha = 0.5) + geom_density(data = df, aes(x = Fandango_Ratingvalue_norm, fill = “Fandango Rating Value”), alpha = 0.5) + geom_density(data = df, aes(x = IMDB_norm, fill = “IMDb”), alpha = 0.5) + geom_density(data = df, aes(x = Metacritic_norm, fill = “Metacritic”), alpha = 0.5) + geom_density(data = df, aes(x = Metacritic_User_norm, fill = “Metacritic users”), alpha = 0.5) + geom_density(data = df, aes(x = RottenTomatoes_norm, fill = “Rotten Tomatoes”), alpha = 0.5) + geom_density(data = df, aes(x = RottenTomatoes_User_norm, fill = “Rotten Tomatoes users”), alpha = 0.5) + labs(title = “Normalized Ratings Distribution of 146 Films in Theaters in 2015”, x = “Normalized Rating”, y = “Density”) + scale_fill_manual(name = “Source”, values = c(“Fandango” = “blue”, “Fandango Rating Value” = “cyan”, “IMDb” = “red”, “Metacritic” = “green”, “Metacritic users” = “darkgreen”, “Rotten Tomatoes” = “purple”, “Rotten Tomatoes users” = “violet”)) + theme_minimal()\nSave the plot ggsave(“fandangocomparison_replicated.pdf”)"
  },
  {
    "objectID": "presentation-exercise/visualization-exercise.html#version-two-editing-graph-to-better-match-the-original-version",
    "href": "presentation-exercise/visualization-exercise.html#version-two-editing-graph-to-better-match-the-original-version",
    "title": "Visualization Exercise",
    "section": "Version Two: Editing Graph to Better Match the Original Version",
    "text": "Version Two: Editing Graph to Better Match the Original Version\nNo AI Prompts used in this step.\nI manually edited the colors to fit the ones assigned to each source within the original graph. I simply did this by using hex codes online or typing in basic colors (such as “red”). Additionally, I changed the title under the labs() function."
  },
  {
    "objectID": "presentation-exercise/visualization-exercise.html#version-three-editing-graph-more-to-better-match-the-original-version",
    "href": "presentation-exercise/visualization-exercise.html#version-three-editing-graph-more-to-better-match-the-original-version",
    "title": "Visualization Exercise",
    "section": "Version Three: Editing Graph MORE to Better Match the Original Version",
    "text": "Version Three: Editing Graph MORE to Better Match the Original Version\nI asked ChatGPT: how do I get rid of the legend and add the labels within the legend next to their corresponding areas within the graph?\nAnswer: theme(legend.position = “none”) + # Removes the legend geom_text(aes(x = 1, y = 0.15, label = “Fandango”, color = “red”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Fandango geom_text(aes(x = 1, y = 0.13, label = “IMDb”, color = “gold”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for IMDb geom_text(aes(x = 1, y = 0.11, label = “Metacritic”, color = “#8d31c1”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Metacritic geom_text(aes(x = 1, y = 0.09, label = “Metacritic users”, color = “#94e13d”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Metacritic users geom_text(aes(x = 1, y = 0.07, label = “Rotten Tomatoes”, color = “#7b7b7c”), size = 5, data = data.frame(), inherit.aes = FALSE) + # Add label for Rotten Tomatoes geom_text(aes(x = 1, y = 0.05, label = “Rotten Tomatoes users”, color = “#309dd8”), size = 5, data = data.frame(), inherit.aes = FALSE) # Add label for Rotten Tomatoes users\nI manually changed the color of the IMDb users density/text to be a darker gold for better visibility. Additionally, I changed the name that ChatGPT gave me for the label from “IBDb” to “IMDb users” to match the original graph.\nAfter a few more intermediate steps: - I changed the theme of the graph to include a gray background and gray gridlines - I changed the positions of the labels to match the original graph - I fixed the x and y axes - I added a subtitle - I changed geom_density to stat_density and added their respective colors - I made the area under the densities gray with an outline of their respective color"
  },
  {
    "objectID": "presentation-exercise/visualization-exercise.html#version-four-creating-final-version",
    "href": "presentation-exercise/visualization-exercise.html#version-four-creating-final-version",
    "title": "Visualization Exercise",
    "section": "Version Four: Creating Final Version",
    "text": "Version Four: Creating Final Version\nFor the final version, I made the following changes: - I made the “fandango” label bold (just like it is in the original graph) - I made the font size of the title and subtitle larger - I updated the color of the green in “Metacritic users” to be a bit darker - I removed the “minor” gridlines\nAdditionally, I asked Microsoft Copilot to change the geom from stat_density to stat_bin. Stat_bin will better help me recreate the original graph than geom_density or stat_density.\nAnswer: stat_bin(aes(x = Fandango_Stars_norm, fill = “Fandango”), geom = “area”, alpha = 0.5, color = “red”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = IMDB_norm, fill = “IMDb”), geom = “area”, alpha = 0.5, color = “#f7ca62”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = Metacritic_norm, fill = “Metacritic”), geom = “area”, alpha = 0.5, color = “#8d31c1”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = Metacritic_User_norm, fill = “Metacritic users”), geom = “area”, alpha = 0.5, color = “#94e13d”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = RottenTomatoes_norm, fill = “Rotten Tomatoes”), geom = “area”, alpha = 0.5, color = “#7b7b7c”, binwidth = 0.1, position = “identity”) + stat_bin(aes(x = RottenTomatoes_User_norm, fill = “Rotten Tomatoes users”), geom = “area”, alpha = 0.5, color = “#309dd8”, binwidth = 0.1, position = “identity”) +\nAfter fixing the geom/stat, I realized the y-axis had been messed up. I used code to fix the y-axis and fix the source labels within the graph."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#create-individual-graphs",
    "href": "presentation-exercise/presentation-exercise.html#create-individual-graphs",
    "title": "Presentation Exercise",
    "section": "Create Individual Graphs",
    "text": "Create Individual Graphs"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#create-table",
    "href": "presentation-exercise/presentation-exercise.html#create-table",
    "title": "Presentation Exercise",
    "section": "Create Table",
    "text": "Create Table\nI used Microsoft Copilot and ChatGPT. I asked it to create a graph in which the columns were: mean, median, minimum, maximum rating values, and distribution plot. I asked for the rows to be: Fandango, IMDb, IMDb users, Metacritic, Metacritic users, Rotten Tomatoes, and Rotten Tomatoes users. There were MANY small things that I had to fix.\n\n# Load required libraries\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(gt)\nlibrary(purrr)\n\n# Step 1: Summarize the data\ntable &lt;- df %&gt;%\n  select(Fandango_Stars_norm, IMDB_norm, Metacritic_norm, Metacritic_User_norm, RottenTomatoes_norm, RottenTomatoes_User_norm) %&gt;%\n  summarise(\n    Fandango_mean = mean(Fandango_Stars_norm, na.rm = TRUE),\n    Fandango_median = median(Fandango_Stars_norm, na.rm = TRUE),\n    Fandango_min = min(Fandango_Stars_norm, na.rm = TRUE),\n    Fandango_max = max(Fandango_Stars_norm, na.rm = TRUE),\n    \n    IMDB_mean = mean(IMDB_norm, na.rm = TRUE),\n    IMDB_median = median(IMDB_norm, na.rm = TRUE),\n    IMDB_min = min(IMDB_norm, na.rm = TRUE),\n    IMDB_max = max(IMDB_norm, na.rm = TRUE),\n    \n    Metacritic_mean = mean(Metacritic_norm, na.rm = TRUE),\n    Metacritic_median = median(Metacritic_norm, na.rm = TRUE),\n    Metacritic_min = min(Metacritic_norm, na.rm = TRUE),\n    Metacritic_max = max(Metacritic_norm, na.rm = TRUE),\n    \n    Metacritic_User_mean = mean(Metacritic_User_norm, na.rm = TRUE),\n    Metacritic_User_median = median(Metacritic_User_norm, na.rm = TRUE),\n    Metacritic_User_min = min(Metacritic_User_norm, na.rm = TRUE),\n    Metacritic_User_max = max(Metacritic_User_norm, na.rm = TRUE),\n    \n    RottenTomatoes_mean = mean(RottenTomatoes_norm, na.rm = TRUE),\n    RottenTomatoes_median = median(RottenTomatoes_norm, na.rm = TRUE),\n    RottenTomatoes_min = min(RottenTomatoes_norm, na.rm = TRUE),\n    RottenTomatoes_max = max(RottenTomatoes_norm, na.rm = TRUE),\n    \n    RottenTomatoes_User_mean = mean(RottenTomatoes_User_norm, na.rm = TRUE),\n    RottenTomatoes_User_median = median(RottenTomatoes_User_norm, na.rm = TRUE),\n    RottenTomatoes_User_min = min(RottenTomatoes_User_norm, na.rm = TRUE),\n    RottenTomatoes_User_max = max(RottenTomatoes_User_norm, na.rm = TRUE)\n  )\n\n# Step 2: Reshape the data\ntable_long &lt;- table %&gt;%\n  pivot_longer(cols = everything(), names_to = c(\"Rating\", \"Statistic\"), names_pattern = \"(.*)_(.*)\") %&gt;%\n  pivot_wider(names_from = \"Statistic\", values_from = \"value\") %&gt;%\n  select(Rating, mean, median, min, max)\n\n# Step 3: Add the distribution plot path and other steps\nget_image_path &lt;- function(rating) {\n  # Modify the function to return the correct image path or URL\n  paste0(\"path_to_images/\", rating, \"_plot.png\")  # Example path, adjust to your data\n}\n\ntable_with_plots &lt;- table_long %&gt;%\n  mutate(\n    distribution = map(Rating, get_image_path)\n  )\n\n# Compute the maximum values for highlighting\nmax_mean &lt;- max(table_long$mean, na.rm = TRUE)\nmax_median &lt;- max(table_long$median, na.rm = TRUE)\nmax_min &lt;- max(table_long$min, na.rm = TRUE)\nmax_max &lt;- max(table_long$max, na.rm = TRUE)\n\n\n# Step 4: Create and format the GT table\n# Create the presentation table with images using text_transform\npresentation_table &lt;- table_with_plots %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = md(\"**Summary of Ratings from Movie Review Sources**\")\n  ) %&gt;%\n  cols_label(\n    Rating = \"Rating Source\",\n    mean = \"Mean\",\n    median = \"Median\",\n    min = \"Minimum\",\n    max = \"Maximum\",\n    distribution = \"Density Plot\"\n  ) %&gt;%\n  tab_spanner(\n    label = \"Rating Statistics\",\n    columns = c(\"mean\", \"median\", \"min\", \"max\")\n  ) %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_column_labels(columns = c(\"mean\", \"median\", \"min\", \"max\"))\n  ) %&gt;%\n  tab_style(\n    style = list(cell_text(weight = \"bold\")),\n    locations = cells_column_labels(columns = everything())\n  ) %&gt;%\n  # Use text_transform to render HTML &lt;img&gt; tags for images\n  text_transform(\n    locations = cells_body(columns = \"distribution\"),\n    fn = function(x) {\n      map(x, ~ ifelse(!is.na(.x),\n                      paste0('&lt;img src=\"file://', file.path(\"~/Desktop/MADA/nataliecann-MADA-portfolio/presentation-exercise/graphs\", .x), '\" height=\"100\"&gt;'),\n                      NA_character_))\n    }\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"white\"),\n    locations = cells_body(rows = seq(1, nrow(table_with_plots), by = 2))\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#ececec\"),\n    locations = cells_body(rows = seq(2, nrow(table_with_plots), by = 2))\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#dbf2cf\"),\n    locations = cells_body(columns = \"mean\", rows = mean == max_mean)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#cfedf8\"),\n    locations = cells_body(columns = \"median\", rows = median == max_median)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#f5e4f3\"),\n    locations = cells_body(columns = \"min\", rows = min == max_min)\n  ) %&gt;%\n  tab_style(\n    style = cell_fill(color = \"#fee6cc\"),\n    locations = cells_body(columns = \"max\", rows = max == max_max)\n  ) %&gt;%\n  tab_source_note(\n    source_note = md(\"*Cells highlighted in different colors indicate the highest value in that column.*\")\n  )\n\n# Display the table\npresentation_table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary of Ratings from Movie Review Sources\n\n\nRating Source\n\nRating Statistics\n\nDensity Plot\n\n\nMean\nMedian\nMinimum\nMaximum\n\n\n\n\nFandango\n0.8178082\n0.800\n0.60\n1.00\n\n\n\nIMDB\n0.6736986\n0.690\n0.40\n0.86\n\n\n\nMetacritic\n0.5880822\n0.590\n0.13\n0.94\n\n\n\nMetacritic_User\n0.6519178\n0.685\n0.24\n0.96\n\n\n\nRottenTomatoes\n0.6084932\n0.635\n0.05\n1.00\n\n\n\nRottenTomatoes_User\n0.6387671\n0.665\n0.20\n0.94\n\n\n\n\nCells highlighted in different colors indicate the highest value in that column.\n\n\n\n\n\n\n\n# Save!\ngtsave(presentation_table, \"presentation_table.png\")\n\n\n# Define common scales\ncommon_x_scale &lt;- scale_x_continuous(\n  breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), \n  labels = c(\"☆\", \"★\", \"★★\", \"★★★\", \"★★★★\", \"★★★★★\")\n)\n\ncommon_y_scale &lt;- scale_y_continuous(\n  breaks = c(0, 15, 30, 45, 60), \n  labels = c(\"0\", \"10\", \"20\", \"30\", \"40%\")\n)\n\ncommon_theme &lt;- theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"#ececec\"),\n    panel.grid.major = element_line(color = \"#a7a7a5\", size = 0.1),\n    panel.grid = element_line(color = \"transparent\"),\n    plot.title = element_text(face = \"bold\", size = 18),\n    plot.subtitle = element_text(size = 14)\n  )\n\n# IMDb Plot\nIMDb &lt;- ggplot(df) + \n  stat_bin(aes(x = IMDB_norm, fill = \"IMDb\"), geom = \"area\", alpha = 0.5, color = \"#e6c531\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"IMDb\" = \"#e6c531\")) +  \n  common_x_scale + common_y_scale + common_theme\nIMDb\n\n\n\n\n\n\n\n# Metacritic Plot\nMetacritic &lt;- ggplot(df) + \n  stat_bin(aes(x = Metacritic_norm, fill = \"Metacritic\"), geom = \"area\", alpha = 0.5, color = \"#8d31c1\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Metacritic\" = \"#8d31c1\")) +  \n  common_x_scale + common_y_scale + common_theme\nMetacritic\n\n\n\n\n\n\n\n# Metacritic User Plot\nMetacritic_User &lt;- ggplot(df) + \n  stat_bin(aes(x = Metacritic_User_norm, fill = \"Metacritic users\"), geom = \"area\", alpha = 0.5, color = \"#87c866\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Metacritic users\" = \"#87c866\")) +  \n  common_x_scale + common_y_scale + common_theme\nMetacritic_User\n\n\n\n\n\n\n\n# Rotten Tomatoes Plot\nRottenTomatoes &lt;- ggplot(df) + \n  stat_bin(aes(x = RottenTomatoes_norm, fill = \"Rotten Tomatoes\"), geom = \"area\", alpha = 0.5, color = \"#7b7b7c\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Rotten Tomatoes\" = \"#7b7b7c\")) +  \n  common_x_scale + common_y_scale + common_theme\nRottenTomatoes\n\n\n\n\n\n\n\n# Rotten Tomatoes User Plot\nRottenTomatoes_User &lt;- ggplot(df) + \n  stat_bin(aes(x = RottenTomatoes_User_norm, fill = \"Rotten Tomatoes users\"), geom = \"area\", alpha = 0.5, color = \"#309dd8\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Rotten Tomatoes users\" = \"#309dd8\")) +  \n  common_x_scale + common_y_scale + common_theme\nRottenTomatoes_User\n\n\n\n\n\n\n\n# Fandango Plot\nFandango &lt;- ggplot(df) + \n  stat_bin(aes(x = Fandango_Stars_norm, fill = \"Fandango\"), geom = \"area\", alpha = 0.5, color = \"red\", binwidth = 0.1, position = \"identity\") +\n  scale_fill_manual(values = c(\"Fandango\" = \"red\")) +  \n  common_x_scale + common_y_scale + common_theme\nFandango\n\n\n\n\n\n\n\n\nI tried many different times to get the distributions to appear in the “density plot” column. I couldn’t get this to work. So, I took screenshots of the individual graphs created above and edited them into a screenshot of the table.\n\n\n\nEdited Version of Table"
  }
]